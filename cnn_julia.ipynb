{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9beed57c-7db7-4e59-9990-a470390d4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "#@code_warntype, @simd, @inbounds, statyczne typowenie (brak niestabilności typów, brak Any), @benchmark_tools, @btime, brak alokacji, \n",
    "#kolumny czy wiersze w for\n",
    "#test co szybsz, Matrix czy Array{Float64, 2}\n",
    "#softmax backward\n",
    "\n",
    "mutable struct Conv2DNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    W::Array{Float64, 4}\n",
    "    b::Vector{Float64}\n",
    "    batch_size::Int\n",
    "    kernel_size::Tuple{Int, Int}\n",
    "    filters::Int\n",
    "    output::Array{Float64, 4}\n",
    "    gradient_W::Array{Float64, 4}\n",
    "    gradient_b::Vector{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct MaxPoolNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    pool_size::Tuple{Int, Int}\n",
    "    output::Array{Float64, 4}\n",
    "end\n",
    "\n",
    "mutable struct DenseNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    W::Matrix{Float64} \n",
    "    b::Vector{Float64}\n",
    "    neurons::Int\n",
    "    output::Matrix{Float64}\n",
    "    gradient_W::Matrix{Float64}\n",
    "    gradient_b::Matrix{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct FlattenNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    output::Matrix{Float64}\n",
    "    input_shape::Tuple{Int, Int, Int}\n",
    "end\n",
    "\n",
    "mutable struct ReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct SoftmaxNode <: Node\n",
    "    x::Matrix{Float64}\n",
    "    output::Matrix{Float64}\n",
    "end\n",
    "\n",
    "mutable struct SigmoidNode <: Node\n",
    "    x::Any\n",
    "    output::Any\n",
    "end\n",
    "\n",
    "mutable struct TanhNode <: Node\n",
    "    x::Any\n",
    "    output::Any\n",
    "end\n",
    "\n",
    "mutable struct LeakyReLUNode <: Node\n",
    "    x::Any\n",
    "    output::Any\n",
    "    alpha::Float64\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8eb86b4-d598-4d00-ab75-f71b4ab219a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 9 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward!(node::Conv2DNode, x) \n",
    "    node.x = x\n",
    "    W = node.W\n",
    "    b = node.b\n",
    "\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width,  num_chanels, num_filters = size(W)\n",
    "\n",
    "    output_height = 1 + (input_height - filter_height)\n",
    "    output_width = 1 + (input_width - filter_width)\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, num_filters)\n",
    "    for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:input_width-filter_width+1, i in 1:input_height-filter_height+1\n",
    "                    window = view(x, n, i:i+filter_height-1, j:j+filter_width-1, :)\n",
    "                    output[n, i, j, f] = sum(window .* W[:,:,:,f]) + b[f]\n",
    "                    \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    node.output = output\n",
    "end\n",
    "\n",
    "function forward!(node::MaxPoolNode, x)\n",
    "    node.x = x\n",
    "    pool_size = node.pool_size\n",
    "    \n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "\n",
    "    output_height = 1 + (input_height - pool_size[1]) ÷ pool_size[1]\n",
    "    output_width = 1 + (input_width - pool_size[2]) ÷ pool_size[2]\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, input_channels)\n",
    "\n",
    "    for n in 1:batch_size\n",
    "        for c in 1:input_channels\n",
    "            for j in 1:pool_size[2]:input_width-pool_size[2]+1, i in 1:pool_size[1]:input_height-pool_size[1]+1\n",
    "                    window = view(x, n, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c)\n",
    "                    output[n, 1+div(i-1, pool_size[1]), 1+div(j-1, pool_size[2]), c] = maximum(window)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    node.output = output\n",
    "end\n",
    "\n",
    "\n",
    "function forward!(node::ReLUNode, x)\n",
    "    node.x = x\n",
    "    node.output = max.(0, x)\n",
    "end\n",
    "\n",
    "function forward!(node::DenseNode, x)\n",
    "    node.x = x\n",
    "    W = node.W\n",
    "    b = node.b\n",
    "    node.output = x * W .+ b'\n",
    "end\n",
    "\n",
    "function forward!(node::SoftmaxNode, x)\n",
    "    node.x = x\n",
    "    exp_x = exp.(x .- maximum(x, dims=2))\n",
    "    node.output = exp_x ./ sum(exp_x, dims=2)\n",
    "end\n",
    "\n",
    "function forward!(node::FlattenNode, x)\n",
    "    node.x = x\n",
    "    node.output = reshape(x, size(x, 1), size(x, 2) * size(x, 3) * size(x, 4))\n",
    "end\n",
    "\n",
    "function forward!(node::SigmoidNode, x)\n",
    "    node.x = x\n",
    "    node.output = 1.0 ./ (1.0 .+ exp.(-x))\n",
    "end\n",
    "\n",
    "function forward!(node::TanhNode, x)\n",
    "    node.x = x\n",
    "    node.output = tanh.(x)\n",
    "end\n",
    "\n",
    "function forward!(node::LeakyReLUNode, x)\n",
    "    node.x = x\n",
    "    node.output = ifelse.(x .> 0, x, node.alpha .* x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602fd274-f243-48fd-874c-9567ce11b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 8 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(node::Conv2DNode, dy, x)\n",
    "    W = node.W\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width, num_channels, num_filters = size(W)\n",
    "\n",
    "    dx = zeros(size(x))\n",
    "    dW = zeros(size(W))\n",
    "    db = zeros(num_filters)\n",
    "\n",
    "    for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:input_width - filter_width + 1, i in 1:input_height - filter_height + 1\n",
    "                    dx[n, i:i + filter_height - 1, j:j + filter_width - 1, :] .+= W[:, :, :, f] .* dy[n, i, j, f]\n",
    "                    dW[:, :, :, f] .+= view(x, n, i:i + filter_height - 1, j:j + filter_width - 1, :) .* dy[n, i, j, f]\n",
    "            end\n",
    "            db[f] += sum(dy[n, :, :, f])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::MaxPoolNode, dy, x)\n",
    "    pool_size = node.pool_size\n",
    "    new_size = size(node.output)\n",
    "    dy = reshape(dy, new_size)\n",
    "    \n",
    "    #println(\"back_max_pool \",size(dy),size(x), pool_size)\n",
    "    \n",
    "    batch_size, height, width, channels = size(x)\n",
    "    dx = zeros(size(x))\n",
    "    for b in 1:batch_size\n",
    "        for c in 1:channels\n",
    "            for j in 1:pool_size[2]:width-pool_size[2]+1, i in 1:pool_size[1]:height-pool_size[1]+1\n",
    "                    window = view(x, b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c)\n",
    "                    dx[b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c] .+= dy[b, i ÷ pool_size[1]+1, j ÷ pool_size[2]+1, c] .* (window .== maximum(window))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::DenseNode, dy, x)\n",
    "    W = node.W\n",
    "    #println(\"back_dense \",size(dy),size(x), size(W))\n",
    "    dx, dW, db = dy * W', x' * dy, reshape(sum(dy, dims=1), :, 1)\n",
    "    #println(\"back_dense123 \",size(dx),size(dW), size(db))\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::ReLUNode, dy, x)\n",
    "    #println(\"back_relu \", size(dy), size(x))\n",
    "    dx = dy .* (x .> 0)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::FlattenNode, dy, x)\n",
    "    input_shape = node.input_shape\n",
    "    dx = reshape(dy, size(x, 1), input_shape...)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::SigmoidNode, dy, x)\n",
    "    dx = dy .* (node.output .* (1.0 .- node.output))\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::TanhNode, dy, x)\n",
    "    dx = dy .* (1.0 .- node.output .^ 2)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::LeakyReLUNode, dy, x)\n",
    "    dx = dy .* ifelse.(x .> 0, 1, node.alpha)\n",
    "    return dx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d202a415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hasproperty (generic function with 4 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Conv2DNode(; batch_size::Int, filters::Int, kernel_size::Tuple{Int, Int}, input_shape::Tuple{Int, Int, Int}, activation::String)\n",
    "    W = randn(kernel_size[1], kernel_size[2], input_shape[3], filters) * sqrt(1 / (kernel_size[1] * kernel_size[2] * input_shape[3]))\n",
    "    b = zeros(filters)\n",
    "\n",
    "    return Conv2DNode(zeros((input_shape..., batch_size)), W, b, batch_size, kernel_size, filters, zeros((input_shape..., size(W)[4])), zeros((1, 1, 1, 1)), zeros(1), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function MaxPoolNode(; pool_size::Tuple{Int, Int})\n",
    "    return MaxPoolNode(zeros((1, 1, 1, 1)), pool_size, zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function DenseNode(; neurons::Int, activation::String, input_shape::Union{Tuple{Int, Int}, Tuple{Int, Int, Int}})\n",
    "    W = randn(prod(input_shape), neurons) * sqrt(1 / (prod(input_shape)))\n",
    "    b = zeros(neurons)\n",
    "\n",
    "    return DenseNode(zeros((input_shape..., 1, 1)), W, b, neurons, zeros((1,1)), zeros((1, 1)), zeros((1, 1)), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function FlattenNode(; input_shape::Tuple{Int, Int, Int})\n",
    "    return FlattenNode(zeros((input_shape..., 1)), zeros((1, 1)), input_shape)\n",
    "end\n",
    "\n",
    "function ReLUNode()\n",
    "    return ReLUNode(zeros((1, 1, 1, 1)), zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function SoftmaxNode()\n",
    "    return SoftmaxNode(zeros((1, 1)), zeros((1, 1)))\n",
    "end\n",
    "\n",
    "\n",
    "function forward_pass!(graph, x)\n",
    "    input = x\n",
    "    for node in graph\n",
    "        forward!(node, input)\n",
    "        input = node.output\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function backward_pass!(graph, y_true)\n",
    "    preds = graph[end].output\n",
    "    dout  = softmax_cross_entropy_backward(preds, y_true)\n",
    "    \n",
    "    for i in reverse(1:length(graph)-1)\n",
    "        layer = graph[i]\n",
    "        prev_layer_output = i > 1 ? graph[i - 1].output : layer.x\n",
    "        if hasproperty(layer, :gradient_W)\n",
    "            dout,dW,db = backward!(layer, dout, prev_layer_output)\n",
    "            layer.gradient_W = dW\n",
    "            layer.gradient_b = db\n",
    "\n",
    "        else \n",
    "            dout = backward!(layer, dout, prev_layer_output)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function onehotbatch(labels, classes)\n",
    "    Y = zeros(Int, length(labels), length(classes))\n",
    "    for (i, label) in enumerate(labels)\n",
    "        Y[i, label .== classes] .= 1\n",
    "    end\n",
    "    return Y\n",
    "end\n",
    "\n",
    "function graph_build(layers)\n",
    "    output = []\n",
    "    for layer in layers\n",
    "        push!(output, layer)\n",
    "        if hasproperty(layer, :activation)\n",
    "            push!(output, getActivation(layer.activation))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return output\n",
    "end\n",
    "\n",
    "function getActivation(name)\n",
    "    if name == \"relu\"\n",
    "        return ReLUNode()\n",
    "    elseif name == \"softmax\"\n",
    "        return SoftmaxNode()\n",
    "    elseif name == \"sigmoid\"\n",
    "        return SigmoidNode(nothing, nothing)\n",
    "    elseif name == \"tanh\"\n",
    "        return TanhNode(nothing, nothing)\n",
    "    elseif name == \"leakyrelu\"\n",
    "        return LeakyReLUNode(nothing, nothing, 0.01)\n",
    "    else \n",
    "        error(\"no such activation function\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function softmax_cross_entropy_backward(y_pred, y_true)\n",
    "    batch_size = size(y_pred, 1)\n",
    "    dloss = (y_pred .- onehotbatch(y_true, 0:9)) ./ size(y_true, 1)\n",
    "    return dloss\n",
    "end\n",
    "\n",
    "import Base.hasproperty\n",
    "hasproperty(x, s::Symbol) = s in fieldnames(typeof(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "433dfab2-7d04-4fef-a952-6546082e4fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_adam!(layer, learning_rate, t, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
    "    layer.m_W = beta1 * layer.m_W + (1 - beta1) * layer.gradient_W\n",
    "    layer.v_W = beta2 * layer.v_W + (1 - beta2) * layer.gradient_W .^ 2\n",
    "    mhat_W = layer.m_W / (1 - beta1 ^ t)\n",
    "    vhat_W = layer.v_W / (1 - beta2 ^ t)\n",
    "    layer.W .-= learning_rate .* mhat_W ./ (sqrt.(vhat_W) .+ epsilon)\n",
    "\n",
    "\n",
    "    layer.m_b = beta1 * layer.m_b + (1 - beta1) * layer.gradient_b\n",
    "    layer.v_b = beta2 * layer.v_b + (1 - beta2) * layer.gradient_b .^ 2\n",
    "    mhat_b = layer.m_b / (1 - beta1 ^ t)\n",
    "    vhat_b = layer.v_b / (1 - beta2 ^ t)\n",
    "    layer.b .-= learning_rate .* mhat_b ./ (sqrt.(vhat_b) .+ epsilon)\n",
    "end\n",
    "\n",
    "function update_sgd!(layer, learning_rate)\n",
    "    layer.W .-= learning_rate .* layer.gradient_W\n",
    "    layer.b .-= learning_rate .* layer.gradient_b\n",
    "end\n",
    "\n",
    "function train!(graph, train_x, train_y, epochs=5, batch_size=32, learning_rate=1e-3, optimizer=\"adam\")\n",
    "    total_batches = ceil(Int, size(train_x, 1) / batch_size)\n",
    "    each = ceil(Int, total_batches/10)\n",
    " \n",
    "    loss_history = zeros(ceil(Int, total_batches*epochs/each))\n",
    "    counter = 1\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        println(\"----EPOCH $epoch----\")\n",
    "\n",
    "        idx = randperm(size(train_x, 1))\n",
    "        train_x = train_x[idx, :, :, :]\n",
    "        train_y = train_y[idx]\n",
    "        loss = 0\n",
    "        current_epoch_loss = zeros(total_batches)\n",
    "        \n",
    "\n",
    "        for i in 1:batch_size:size(train_x, 1)\n",
    "            x_batch = train_x[i:min(i+batch_size-1, end), :, :, :]\n",
    "            y_batch = train_y[i:min(i+batch_size-1, end)]\n",
    "            tmp  = (i ÷ batch_size)\n",
    "\n",
    "            forward_pass!(graph, x_batch)\n",
    "            \n",
    "            loss = -sum(log.(graph[end].output) .* onehotbatch(y_batch, 0:9))#/size(y_batch, 1)\n",
    "            current_epoch_loss[tmp+1] = loss\n",
    "            \n",
    "            backward_pass!(graph, y_batch)\n",
    "\n",
    "            for layer in graph\n",
    "                if hasproperty(layer, :gradient_W)\n",
    "                    if optimizer == \"adam\"\n",
    "                        update_adam!(layer, learning_rate, i)\n",
    "                    elseif optimizer == \"sgd\"\n",
    "                        update_sgd!(layer, learning_rate)\n",
    "                    else\n",
    "                        error(\"Invalid optimizer selected.\")\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            \n",
    "            if i == 1  ||  tmp % each == 0\n",
    "                avg_loss = sum(current_epoch_loss)/(tmp+1)\n",
    "\n",
    "                loss_history[counter] = avg_loss\n",
    "                counter += 1\n",
    "                print(i ÷ batch_size, \"/\", total_batches)\n",
    "                println(\" avg epoch loss: \", round(avg_loss, digits=5))\n",
    "                \n",
    "            end\n",
    "        end\n",
    "        println()\n",
    "        \n",
    "    end\n",
    "    println(\"Training finished!\")\n",
    "\n",
    "    return loss_history\n",
    "end\n",
    "\n",
    "function evaluate(graph, x_data, y_data, batch_size, pool_size)\n",
    "    num_correct = 0\n",
    "    num_samples = size(x_data, 1)\n",
    "\n",
    "    for i in 1:batch_size:num_samples\n",
    "        x_batch = x_data[i:min(i+batch_size-1, end), :, :, :]\n",
    "        y_batch = y_data[i:min(i+batch_size-1, end)]\n",
    "        \n",
    "        forward_pass!(graph, x_batch)\n",
    "        pred = graph[end].output\n",
    "        \n",
    "        predictions = argmax.(eachrow(pred)) .- 1\n",
    "        num_correct += sum(predictions .== y_batch)\n",
    "    end\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "    return accuracy\n",
    "end\n",
    "\n",
    "function plot_loss(loss::Vector{Float64}, epochs::Int)\n",
    "    plot(1:size(loss,1), loss, label=\"loss_batches\")\n",
    "\n",
    "    each = Int64(size(loss,1)/epochs)\n",
    "    pts = loss[each:each:end]\n",
    "    plot!(each:each:each*size(pts,1), pts, seriestype=:scatter, label=\"epochs\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5e96f3-f2cb-47d1-a14d-8c9d84c3bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "mnist = pyimport(\"mnist\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_data = mnist.train_images()\n",
    "train_labels = Int64.(mnist.train_labels())\n",
    "train_x =  reshape(train_data, :, 28, 28, 1) / 255.0\n",
    "train_y = train_labels\n",
    "\n",
    "\n",
    "n = 1000\n",
    "idx = randperm(n)\n",
    "train_x = train_x[idx, :,:,:]\n",
    "train_y = train_y[idx]\n",
    "\n",
    "train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b287c46-20e9-435c-95e2-23d699c4eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 37.64258\n",
      "7/63 avg epoch loss: 36.61067\n",
      "14/63 avg epoch loss: 35.39136\n",
      "21/63 avg epoch loss: 33.40183\n",
      "28/63 avg epoch loss: 30.76219\n",
      "35/63 avg epoch loss: 28.43829\n",
      "42/63 avg epoch loss: 26.48047\n",
      "49/63 avg epoch loss: 24.59443\n",
      "56/63 avg epoch loss: 22.98704\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 2.72502\n",
      "7/63 avg epoch loss: 8.30483\n",
      "14/63 avg epoch loss: 8.08829\n",
      "21/63 avg epoch loss: 7.3643\n",
      "28/63 avg epoch loss: 7.77083\n",
      "35/63 avg epoch loss: 8.08102\n",
      "42/63 avg epoch loss: 8.09323\n",
      "49/63 avg epoch loss: 8.01583\n",
      "56/63 avg epoch loss: 7.90081\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 3.92959\n",
      "7/63 avg epoch loss: 7.03428\n",
      "14/63 avg epoch loss: 7.56002\n",
      "21/63 avg epoch loss: 6.18117\n",
      "28/63 avg epoch loss: 6.18037\n",
      "35/63 avg epoch loss: 5.99319\n",
      "42/63 avg epoch loss: 5.71351\n",
      "49/63 avg epoch loss: 5.53993\n",
      "56/63 avg epoch loss: 5.3604\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 5.89906\n",
      "7/63 avg epoch loss: 5.05196\n",
      "14/63 avg epoch loss: 4.19028\n",
      "21/63 avg epoch loss: 3.57787\n",
      "28/63 avg epoch loss: 3.05602\n",
      "35/63 avg epoch loss: 2.91194\n",
      "42/63 avg epoch loss: 3.18095\n",
      "49/63 avg epoch loss: 3.4067\n",
      "56/63 avg epoch loss: 3.63584\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 3.46356\n",
      "7/63 avg epoch loss: 2.48205\n",
      "14/63 avg epoch loss: 2.6941\n",
      "21/63 avg epoch loss: 2.34563\n",
      "28/63 avg epoch loss: 2.72221\n",
      "35/63 avg epoch loss: 2.70366\n",
      "42/63 avg epoch loss: 2.81509\n",
      "49/63 avg epoch loss: 2.85618\n",
      "56/63 avg epoch loss: 2.82653\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 4.38699\n",
      "7/63 avg epoch loss: 2.60716\n",
      "14/63 avg epoch loss: 2.59877\n",
      "21/63 avg epoch loss: 2.58178\n",
      "28/63 avg epoch loss: 2.42258\n",
      "35/63 avg epoch loss: 2.38473\n",
      "42/63 avg epoch loss: 2.26342\n",
      "49/63 avg epoch loss: 2.23725\n",
      "56/63 avg epoch loss: 2.28341\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 1.81875\n",
      "7/63 avg epoch loss: 1.36995\n",
      "14/63 avg epoch loss: 1.49478\n",
      "21/63 avg epoch loss: 1.82335\n",
      "28/63 avg epoch loss: 1.7047\n",
      "35/63 avg epoch loss: 1.67945\n",
      "42/63 avg epoch loss: 1.84741\n",
      "49/63 avg epoch loss: 1.89913\n",
      "56/63 avg epoch loss: 1.90079\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 2.15976\n",
      "7/63 avg epoch loss: 1.49669\n",
      "14/63 avg epoch loss: 1.60641\n",
      "21/63 avg epoch loss: 1.37176\n",
      "28/63 avg epoch loss: 1.34342\n",
      "35/63 avg epoch loss: 1.37175\n",
      "42/63 avg epoch loss: 1.44905\n",
      "49/63 avg epoch loss: 1.38573\n",
      "56/63 avg epoch loss: 1.42531\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 6.04129\n",
      "7/63 avg epoch loss: 1.68378\n",
      "14/63 avg epoch loss: 1.21136\n",
      "21/63 avg epoch loss: 1.30623\n",
      "28/63 avg epoch loss: 1.29161\n",
      "35/63 avg epoch loss: 1.22006\n",
      "42/63 avg epoch loss: 1.22946\n",
      "49/63 avg epoch loss: 1.23642\n",
      "56/63 avg epoch loss: 1.28538\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.32295\n",
      "7/63 avg epoch loss: 0.61567\n",
      "14/63 avg epoch loss: 0.62022\n",
      "21/63 avg epoch loss: 0.78577\n",
      "28/63 avg epoch loss: 0.83679\n",
      "35/63 avg epoch loss: 0.82479\n",
      "42/63 avg epoch loss: 1.00545\n",
      "49/63 avg epoch loss: 1.05381\n",
      "56/63 avg epoch loss: 1.0646\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 1.02088\n",
      "7/63 avg epoch loss: 0.57209\n",
      "14/63 avg epoch loss: 0.83883\n",
      "21/63 avg epoch loss: 0.89903\n",
      "28/63 avg epoch loss: 0.85693\n",
      "35/63 avg epoch loss: 0.83216\n",
      "42/63 avg epoch loss: 0.84674\n",
      "49/63 avg epoch loss: 0.81633\n",
      "56/63 avg epoch loss: 0.906\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.12971\n",
      "7/63 avg epoch loss: 0.89719\n",
      "14/63 avg epoch loss: 1.02091\n",
      "21/63 avg epoch loss: 0.91683\n",
      "28/63 avg epoch loss: 0.88492\n",
      "35/63 avg epoch loss: 0.92383\n",
      "42/63 avg epoch loss: 0.88749\n",
      "49/63 avg epoch loss: 0.8262\n",
      "56/63 avg epoch loss: 0.80239\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.98803\n",
      "7/63 avg epoch loss: 0.5506\n",
      "14/63 avg epoch loss: 0.61395\n",
      "21/63 avg epoch loss: 0.5815\n",
      "28/63 avg epoch loss: 0.53891\n",
      "35/63 avg epoch loss: 0.58969\n",
      "42/63 avg epoch loss: 0.54695\n",
      "49/63 avg epoch loss: 0.572\n",
      "56/63 avg epoch loss: 0.5481\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.02538\n",
      "7/63 avg epoch loss: 0.32781\n",
      "14/63 avg epoch loss: 0.47037\n",
      "21/63 avg epoch loss: 0.47562\n",
      "28/63 avg epoch loss: 0.43356\n",
      "35/63 avg epoch loss: 0.41788\n",
      "42/63 avg epoch loss: 0.42581\n",
      "49/63 avg epoch loss: 0.42617\n",
      "56/63 avg epoch loss: 0.45822\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.2101\n",
      "7/63 avg epoch loss: 0.68906\n",
      "14/63 avg epoch loss: 0.63751\n",
      "21/63 avg epoch loss: 0.53524\n",
      "28/63 avg epoch loss: 0.49144\n",
      "35/63 avg epoch loss: 0.45238\n",
      "42/63 avg epoch loss: 0.41834\n",
      "49/63 avg epoch loss: 0.4163\n",
      "56/63 avg epoch loss: 0.42716\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 1.32314\n",
      "7/63 avg epoch loss: 0.4089\n",
      "14/63 avg epoch loss: 0.31385\n",
      "21/63 avg epoch loss: 0.28733\n",
      "28/63 avg epoch loss: 0.30024\n",
      "35/63 avg epoch loss: 0.27877\n",
      "42/63 avg epoch loss: 0.27074\n",
      "49/63 avg epoch loss: 0.26845\n",
      "56/63 avg epoch loss: 0.26005\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.20065\n",
      "7/63 avg epoch loss: 0.26761\n",
      "14/63 avg epoch loss: 0.28246\n",
      "21/63 avg epoch loss: 0.25469\n",
      "28/63 avg epoch loss: 0.23823\n",
      "35/63 avg epoch loss: 0.22722\n",
      "42/63 avg epoch loss: 0.22172\n",
      "49/63 avg epoch loss: 0.22075\n",
      "56/63 avg epoch loss: 0.23243\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.09773\n",
      "7/63 avg epoch loss: 0.16691\n",
      "14/63 avg epoch loss: 0.14703\n",
      "21/63 avg epoch loss: 0.19318\n",
      "28/63 avg epoch loss: 0.18637\n",
      "35/63 avg epoch loss: 0.17538\n",
      "42/63 avg epoch loss: 0.17516\n",
      "49/63 avg epoch loss: 0.16924\n",
      "56/63 avg epoch loss: 0.18202\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.14384\n",
      "7/63 avg epoch loss: 0.16005\n",
      "14/63 avg epoch loss: 0.14549\n",
      "21/63 avg epoch loss: 0.14193\n",
      "28/63 avg epoch loss: 0.14966\n",
      "35/63 avg epoch loss: 0.1723\n",
      "42/63 avg epoch loss: 0.17389\n",
      "49/63 avg epoch loss: 0.17205\n",
      "56/63 avg epoch loss: 0.17922\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.0799\n",
      "7/63 avg epoch loss: 0.5212\n",
      "14/63 avg epoch loss: 0.34536\n",
      "21/63 avg epoch loss: 0.28251\n",
      "28/63 avg epoch loss: 0.2642\n",
      "35/63 avg epoch loss: 0.23938\n",
      "42/63 avg epoch loss: 0.22205\n",
      "49/63 avg epoch loss: 0.21408\n",
      "56/63 avg epoch loss: 0.22946\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.05369\n",
      "7/63 avg epoch loss: 0.13511\n",
      "14/63 avg epoch loss: 0.16693\n",
      "21/63 avg epoch loss: 0.156\n",
      "28/63 avg epoch loss: 0.13346\n",
      "35/63 avg epoch loss: 0.13097\n",
      "42/63 avg epoch loss: 0.14466\n",
      "49/63 avg epoch loss: 0.14064\n",
      "56/63 avg epoch loss: 0.15536\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.0267\n",
      "7/63 avg epoch loss: 0.17386\n",
      "14/63 avg epoch loss: 0.17709\n",
      "21/63 avg epoch loss: 0.20048\n",
      "28/63 avg epoch loss: 0.18272\n",
      "35/63 avg epoch loss: 0.16601\n",
      "42/63 avg epoch loss: 0.15817\n",
      "49/63 avg epoch loss: 0.14518\n",
      "56/63 avg epoch loss: 0.15126\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.18368\n",
      "7/63 avg epoch loss: 0.15101\n",
      "14/63 avg epoch loss: 0.13356\n",
      "21/63 avg epoch loss: 0.11539\n",
      "28/63 avg epoch loss: 0.10584\n",
      "35/63 avg epoch loss: 0.10902\n",
      "42/63 avg epoch loss: 0.10547\n",
      "49/63 avg epoch loss: 0.11494\n",
      "56/63 avg epoch loss: 0.11308\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.03064\n",
      "7/63 avg epoch loss: 0.08061\n",
      "14/63 avg epoch loss: 0.11571\n",
      "21/63 avg epoch loss: 0.12094\n",
      "28/63 avg epoch loss: 0.10406\n",
      "35/63 avg epoch loss: 0.09691\n",
      "42/63 avg epoch loss: 0.09694\n",
      "49/63 avg epoch loss: 0.09257\n",
      "56/63 avg epoch loss: 0.09311\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.06678\n",
      "7/63 avg epoch loss: 0.07412\n",
      "14/63 avg epoch loss: 0.06414\n",
      "21/63 avg epoch loss: 0.06658\n",
      "28/63 avg epoch loss: 0.06716\n",
      "35/63 avg epoch loss: 0.07494\n",
      "42/63 avg epoch loss: 0.07375\n",
      "49/63 avg epoch loss: 0.07375\n",
      "56/63 avg epoch loss: 0.07073\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.06542\n",
      "7/63 avg epoch loss: 0.04181\n",
      "14/63 avg epoch loss: 0.0538\n",
      "21/63 avg epoch loss: 0.06005\n",
      "28/63 avg epoch loss: 0.06514\n",
      "35/63 avg epoch loss: 0.06664\n",
      "42/63 avg epoch loss: 0.06819\n",
      "49/63 avg epoch loss: 0.06376\n",
      "56/63 avg epoch loss: 0.06521\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.01553\n",
      "7/63 avg epoch loss: 0.03478\n",
      "14/63 avg epoch loss: 0.04339\n",
      "21/63 avg epoch loss: 0.04528\n",
      "28/63 avg epoch loss: 0.055\n",
      "35/63 avg epoch loss: 0.05091\n",
      "42/63 avg epoch loss: 0.05173\n",
      "49/63 avg epoch loss: 0.05493\n",
      "56/63 avg epoch loss: 0.05823\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.04729\n",
      "7/63 avg epoch loss: 0.04771\n",
      "14/63 avg epoch loss: 0.03601\n",
      "21/63 avg epoch loss: 0.0464\n",
      "28/63 avg epoch loss: 0.04994\n",
      "35/63 avg epoch loss: 0.04739\n",
      "42/63 avg epoch loss: 0.05149\n",
      "49/63 avg epoch loss: 0.05724\n",
      "56/63 avg epoch loss: 0.05703\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.02532\n",
      "7/63 avg epoch loss: 0.05238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/63 avg epoch loss: 0.04864\n",
      "21/63 avg epoch loss: 0.04778\n",
      "28/63 avg epoch loss: 0.05224\n",
      "35/63 avg epoch loss: 0.04973\n",
      "42/63 avg epoch loss: 0.04855\n",
      "49/63 avg epoch loss: 0.04867\n",
      "56/63 avg epoch loss: 0.05002\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.01655\n",
      "7/63 avg epoch loss: 0.05509\n",
      "14/63 avg epoch loss: 0.05248\n",
      "21/63 avg epoch loss: 0.05145\n",
      "28/63 avg epoch loss: 0.04863\n",
      "35/63 avg epoch loss: 0.05124\n",
      "42/63 avg epoch loss: 0.04704\n",
      "49/63 avg epoch loss: 0.0454\n",
      "56/63 avg epoch loss: 0.04777\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.0316\n",
      "7/63 avg epoch loss: 0.03032\n",
      "14/63 avg epoch loss: 0.03634\n",
      "21/63 avg epoch loss: 0.03553\n",
      "28/63 avg epoch loss: 0.03493\n",
      "35/63 avg epoch loss: 0.03515\n",
      "42/63 avg epoch loss: 0.03783\n",
      "49/63 avg epoch loss: 0.04055\n",
      "56/63 avg epoch loss: 0.03996\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.03235\n",
      "7/63 avg epoch loss: 0.03762\n",
      "14/63 avg epoch loss: 0.03544\n",
      "21/63 avg epoch loss: 0.03626\n",
      "28/63 avg epoch loss: 0.03678\n",
      "35/63 avg epoch loss: 0.03848\n",
      "42/63 avg epoch loss: 0.03942\n",
      "49/63 avg epoch loss: 0.0368\n",
      "56/63 avg epoch loss: 0.0361\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.02894\n",
      "7/63 avg epoch loss: 0.0388\n",
      "14/63 avg epoch loss: 0.03812\n",
      "21/63 avg epoch loss: 0.03801\n",
      "28/63 avg epoch loss: 0.03807\n",
      "35/63 avg epoch loss: 0.03568\n",
      "42/63 avg epoch loss: 0.03763\n",
      "49/63 avg epoch loss: 0.03689\n",
      "56/63 avg epoch loss: 0.03731\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 0.04824\n",
      "7/63 avg epoch loss: 0.03336\n",
      "14/63 avg epoch loss: 0.03413\n",
      "21/63 avg epoch loss: 0.03701\n",
      "28/63 avg epoch loss: 0.03468\n",
      "35/63 avg epoch loss: 0.03277\n",
      "42/63 avg epoch loss: 0.0323\n",
      "49/63 avg epoch loss: 0.03417\n",
      "56/63 avg epoch loss: 0.03259\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 0.0563\n",
      "7/63 avg epoch loss: 0.03123\n",
      "14/63 avg epoch loss: 0.0308\n",
      "21/63 avg epoch loss: 0.02874\n",
      "28/63 avg epoch loss: 0.02703\n",
      "35/63 avg epoch loss: 0.03011\n",
      "42/63 avg epoch loss: 0.03098\n",
      "49/63 avg epoch loss: 0.03043\n",
      "56/63 avg epoch loss: 0.0292\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 0.00252\n",
      "7/63 avg epoch loss: 0.043\n",
      "14/63 avg epoch loss: 0.03303\n",
      "21/63 avg epoch loss: 0.03361\n",
      "28/63 avg epoch loss: 0.03434\n",
      "35/63 avg epoch loss: 0.03133\n",
      "42/63 avg epoch loss: 0.02874\n",
      "49/63 avg epoch loss: 0.02822\n",
      "56/63 avg epoch loss: 0.02689\n",
      "\n",
      "Training finished!\n",
      "  1.520 s (20587165 allocations: 2.71 GiB)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InexactError: Int64(26.666666666666668)",
     "output_type": "error",
     "traceback": [
      "InexactError: Int64(26.666666666666668)",
      "",
      "Stacktrace:",
      " [1] Int64",
      "   @ ./float.jl:788 [inlined]",
      " [2] plot_loss(loss::Vector{Float64}, epochs::Int64)",
      "   @ Main ./In[15]:103",
      " [3] top-level scope",
      "   @ In[21]:20"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "using BenchmarkTools: @btime\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "learning_rate = 0.0075\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=batch_size, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(prod((6,6,2)), 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model)\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2304da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "test_data = mnist.test_images()\n",
    "test_labels = Int64.(mnist.test_labels())\n",
    "\n",
    "test_x =  reshape(test_data, :, 28, 28, 1) / 255.0\n",
    "test_y = test_labels\n",
    "\n",
    "n = 5000\n",
    "idx = randperm(n)\n",
    "\n",
    "test_x = test_x[idx, :,:,:]\n",
    "test_y = test_y[idx]\n",
    "\n",
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "617527d7-dea6-41da-9ecd-f68b7687482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 167.38948\n",
      "2/16 avg epoch loss: 157.14773\n",
      "4/16 avg epoch loss: 154.58339\n",
      "6/16 avg epoch loss: 149.96334\n",
      "8/16 avg epoch loss: 145.67213\n",
      "10/16 avg epoch loss: 141.41149\n",
      "12/16 avg epoch loss: 137.43503\n",
      "14/16 avg epoch loss: 132.00282\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 83.3737\n",
      "2/16 avg epoch loss: 88.08394\n",
      "4/16 avg epoch loss: 83.72199\n",
      "6/16 avg epoch loss: 79.17688\n",
      "8/16 avg epoch loss: 76.06024\n",
      "10/16 avg epoch loss: 72.83292\n",
      "12/16 avg epoch loss: 71.4785\n",
      "14/16 avg epoch loss: 68.83259\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 57.0956\n",
      "2/16 avg epoch loss: 47.6644\n",
      "4/16 avg epoch loss: 47.85129\n",
      "6/16 avg epoch loss: 47.33534\n",
      "8/16 avg epoch loss: 45.51791\n",
      "10/16 avg epoch loss: 45.64605\n",
      "12/16 avg epoch loss: 45.08582\n",
      "14/16 avg epoch loss: 45.27988\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 27.86498\n",
      "2/16 avg epoch loss: 38.62776\n",
      "4/16 avg epoch loss: 36.1864\n",
      "6/16 avg epoch loss: 37.58715\n",
      "8/16 avg epoch loss: 36.28799\n",
      "10/16 avg epoch loss: 36.43501\n",
      "12/16 avg epoch loss: 37.24301\n",
      "14/16 avg epoch loss: 36.63557\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 20.46703\n",
      "2/16 avg epoch loss: 27.48728\n",
      "4/16 avg epoch loss: 30.52242\n",
      "6/16 avg epoch loss: 31.68713\n",
      "8/16 avg epoch loss: 32.07531\n",
      "10/16 avg epoch loss: 31.14398\n",
      "12/16 avg epoch loss: 31.49701\n",
      "14/16 avg epoch loss: 30.36705\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 22.39925\n",
      "2/16 avg epoch loss: 25.91528\n",
      "4/16 avg epoch loss: 25.80805\n",
      "6/16 avg epoch loss: 24.59463\n",
      "8/16 avg epoch loss: 25.83038\n",
      "10/16 avg epoch loss: 27.37195\n",
      "12/16 avg epoch loss: 27.55129\n",
      "14/16 avg epoch loss: 26.93998\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 33.81302\n",
      "2/16 avg epoch loss: 30.16226\n",
      "4/16 avg epoch loss: 28.83967\n",
      "6/16 avg epoch loss: 27.43028\n",
      "8/16 avg epoch loss: 26.53461\n",
      "10/16 avg epoch loss: 26.8593\n",
      "12/16 avg epoch loss: 26.48246\n",
      "14/16 avg epoch loss: 24.70067\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 18.2219\n",
      "2/16 avg epoch loss: 25.28664\n",
      "4/16 avg epoch loss: 24.70386\n",
      "6/16 avg epoch loss: 23.68532\n",
      "8/16 avg epoch loss: 23.45687\n",
      "10/16 avg epoch loss: 22.32628\n",
      "12/16 avg epoch loss: 22.41571\n",
      "14/16 avg epoch loss: 23.86668\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 25.29436\n",
      "2/16 avg epoch loss: 28.61672\n",
      "4/16 avg epoch loss: 26.50178\n",
      "6/16 avg epoch loss: 26.40555\n",
      "8/16 avg epoch loss: 24.43908\n",
      "10/16 avg epoch loss: 23.38896\n",
      "12/16 avg epoch loss: 22.91452\n",
      "14/16 avg epoch loss: 21.938\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 16.44113\n",
      "2/16 avg epoch loss: 19.32211\n",
      "4/16 avg epoch loss: 17.17489\n",
      "6/16 avg epoch loss: 19.61012\n",
      "8/16 avg epoch loss: 19.4023\n",
      "10/16 avg epoch loss: 19.93821\n",
      "12/16 avg epoch loss: 20.07682\n",
      "14/16 avg epoch loss: 20.35012\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 9.07562\n",
      "2/16 avg epoch loss: 16.51972\n",
      "4/16 avg epoch loss: 16.15856\n",
      "6/16 avg epoch loss: 16.33899\n",
      "8/16 avg epoch loss: 16.98021\n",
      "10/16 avg epoch loss: 18.62855\n",
      "12/16 avg epoch loss: 19.73636\n",
      "14/16 avg epoch loss: 19.25448\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 12.54359\n",
      "2/16 avg epoch loss: 18.38028\n",
      "4/16 avg epoch loss: 15.94946\n",
      "6/16 avg epoch loss: 15.37208\n",
      "8/16 avg epoch loss: 16.37421\n",
      "10/16 avg epoch loss: 17.10477\n",
      "12/16 avg epoch loss: 17.38184\n",
      "14/16 avg epoch loss: 17.56114\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 15.40526\n",
      "2/16 avg epoch loss: 15.13069\n",
      "4/16 avg epoch loss: 16.01868\n",
      "6/16 avg epoch loss: 16.01047\n",
      "8/16 avg epoch loss: 16.67363\n",
      "10/16 avg epoch loss: 17.11436\n",
      "12/16 avg epoch loss: 17.5635\n",
      "14/16 avg epoch loss: 17.00517\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 8.35729\n",
      "2/16 avg epoch loss: 12.36693\n",
      "4/16 avg epoch loss: 15.07071\n",
      "6/16 avg epoch loss: 14.92025\n",
      "8/16 avg epoch loss: 13.62095\n",
      "10/16 avg epoch loss: 14.75883\n",
      "12/16 avg epoch loss: 15.94395\n",
      "14/16 avg epoch loss: 15.84547\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 7.13714\n",
      "2/16 avg epoch loss: 12.25954\n",
      "4/16 avg epoch loss: 12.72582\n",
      "6/16 avg epoch loss: 13.66639\n",
      "8/16 avg epoch loss: 14.46434\n",
      "10/16 avg epoch loss: 14.44068\n",
      "12/16 avg epoch loss: 15.02807\n",
      "14/16 avg epoch loss: 15.04163\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 19.47628\n",
      "2/16 avg epoch loss: 16.24525\n",
      "4/16 avg epoch loss: 14.72496\n",
      "6/16 avg epoch loss: 13.58055\n",
      "8/16 avg epoch loss: 13.22106\n",
      "10/16 avg epoch loss: 13.89072\n",
      "12/16 avg epoch loss: 14.01322\n",
      "14/16 avg epoch loss: 13.80093\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 14.4423\n",
      "2/16 avg epoch loss: 17.58838\n",
      "4/16 avg epoch loss: 14.7754\n",
      "6/16 avg epoch loss: 15.16332\n",
      "8/16 avg epoch loss: 13.65342\n",
      "10/16 avg epoch loss: 14.53168\n",
      "12/16 avg epoch loss: 14.2072\n",
      "14/16 avg epoch loss: 14.21658\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 15.85032\n",
      "2/16 avg epoch loss: 15.24203\n",
      "4/16 avg epoch loss: 15.22387\n",
      "6/16 avg epoch loss: 14.08611\n",
      "8/16 avg epoch loss: 14.05149\n",
      "10/16 avg epoch loss: 13.68343\n",
      "12/16 avg epoch loss: 13.65128\n",
      "14/16 avg epoch loss: 13.75459\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 16.73546\n",
      "2/16 avg epoch loss: 14.97564\n",
      "4/16 avg epoch loss: 11.832\n",
      "6/16 avg epoch loss: 12.30011\n",
      "8/16 avg epoch loss: 12.86684\n",
      "10/16 avg epoch loss: 12.4819\n",
      "12/16 avg epoch loss: 12.99432\n",
      "14/16 avg epoch loss: 12.71067\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 9.20554\n",
      "2/16 avg epoch loss: 15.10756\n",
      "4/16 avg epoch loss: 15.08076\n",
      "6/16 avg epoch loss: 13.9614\n",
      "8/16 avg epoch loss: 13.10588\n",
      "10/16 avg epoch loss: 12.97406\n",
      "12/16 avg epoch loss: 13.52582\n",
      "14/16 avg epoch loss: 12.86778\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 10.41224\n",
      "2/16 avg epoch loss: 9.32632\n",
      "4/16 avg epoch loss: 9.48247\n",
      "6/16 avg epoch loss: 10.30669\n",
      "8/16 avg epoch loss: 10.71973\n",
      "10/16 avg epoch loss: 11.0647\n",
      "12/16 avg epoch loss: 10.74475\n",
      "14/16 avg epoch loss: 11.02086\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 9.04524\n",
      "2/16 avg epoch loss: 13.95863\n",
      "4/16 avg epoch loss: 13.17075\n",
      "6/16 avg epoch loss: 12.71024\n",
      "8/16 avg epoch loss: 12.46713\n",
      "10/16 avg epoch loss: 11.91025\n",
      "12/16 avg epoch loss: 12.13347\n",
      "14/16 avg epoch loss: 11.80061\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 16.4279\n",
      "2/16 avg epoch loss: 10.32909\n",
      "4/16 avg epoch loss: 10.52876\n",
      "6/16 avg epoch loss: 11.10883\n",
      "8/16 avg epoch loss: 10.12104\n",
      "10/16 avg epoch loss: 10.98062\n",
      "12/16 avg epoch loss: 10.48876\n",
      "14/16 avg epoch loss: 10.94078\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 9.37317\n",
      "2/16 avg epoch loss: 6.89514\n",
      "4/16 avg epoch loss: 7.34767\n",
      "6/16 avg epoch loss: 9.56025\n",
      "8/16 avg epoch loss: 9.52565\n",
      "10/16 avg epoch loss: 10.29143\n",
      "12/16 avg epoch loss: 10.05268\n",
      "14/16 avg epoch loss: 10.41319\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 7.29473\n",
      "2/16 avg epoch loss: 11.36573\n",
      "4/16 avg epoch loss: 10.58306\n",
      "6/16 avg epoch loss: 10.12668\n",
      "8/16 avg epoch loss: 9.37947\n",
      "10/16 avg epoch loss: 9.19581\n",
      "12/16 avg epoch loss: 9.72903\n",
      "14/16 avg epoch loss: 10.04675\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 14.45401\n",
      "2/16 avg epoch loss: 11.08875\n",
      "4/16 avg epoch loss: 9.87183\n",
      "6/16 avg epoch loss: 11.39335\n",
      "8/16 avg epoch loss: 10.56979\n",
      "10/16 avg epoch loss: 10.03099\n",
      "12/16 avg epoch loss: 9.74588\n",
      "14/16 avg epoch loss: 9.48486\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 10.43345\n",
      "2/16 avg epoch loss: 10.11803\n",
      "4/16 avg epoch loss: 9.00587\n",
      "6/16 avg epoch loss: 8.7018\n",
      "8/16 avg epoch loss: 9.51631\n",
      "10/16 avg epoch loss: 8.61972\n",
      "12/16 avg epoch loss: 8.94423\n",
      "14/16 avg epoch loss: 9.63781\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 11.19909\n",
      "2/16 avg epoch loss: 8.21421\n",
      "4/16 avg epoch loss: 9.72917\n",
      "6/16 avg epoch loss: 8.91405\n",
      "8/16 avg epoch loss: 9.23553\n",
      "10/16 avg epoch loss: 9.33279\n",
      "12/16 avg epoch loss: 8.69726\n",
      "14/16 avg epoch loss: 8.65978\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 5.4727\n",
      "2/16 avg epoch loss: 5.92904\n",
      "4/16 avg epoch loss: 8.37884\n",
      "6/16 avg epoch loss: 7.71875\n",
      "8/16 avg epoch loss: 7.71276\n",
      "10/16 avg epoch loss: 8.09623\n",
      "12/16 avg epoch loss: 8.24724\n",
      "14/16 avg epoch loss: 8.0525\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 9.62386\n",
      "2/16 avg epoch loss: 9.78158\n",
      "4/16 avg epoch loss: 8.71841\n",
      "6/16 avg epoch loss: 8.3624\n",
      "8/16 avg epoch loss: 7.8899\n",
      "10/16 avg epoch loss: 8.19811\n",
      "12/16 avg epoch loss: 8.55018\n",
      "14/16 avg epoch loss: 8.50332\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 8.88045\n",
      "2/16 avg epoch loss: 9.50829\n",
      "4/16 avg epoch loss: 8.96001\n",
      "6/16 avg epoch loss: 8.38288\n",
      "8/16 avg epoch loss: 8.57857\n",
      "10/16 avg epoch loss: 8.13708\n",
      "12/16 avg epoch loss: 8.2066\n",
      "14/16 avg epoch loss: 8.28876\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 10.28288\n",
      "2/16 avg epoch loss: 9.40062\n",
      "4/16 avg epoch loss: 8.50231\n",
      "6/16 avg epoch loss: 7.6368\n",
      "8/16 avg epoch loss: 7.35409\n",
      "10/16 avg epoch loss: 7.20781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16 avg epoch loss: 7.87093\n",
      "14/16 avg epoch loss: 7.76802\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 5.84873\n",
      "2/16 avg epoch loss: 6.94297\n",
      "4/16 avg epoch loss: 7.94108\n",
      "6/16 avg epoch loss: 7.37064\n",
      "8/16 avg epoch loss: 7.15515\n",
      "10/16 avg epoch loss: 7.65775\n",
      "12/16 avg epoch loss: 7.16575\n",
      "14/16 avg epoch loss: 7.06177\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 8.80341\n",
      "2/16 avg epoch loss: 7.11852\n",
      "4/16 avg epoch loss: 6.92754\n",
      "6/16 avg epoch loss: 6.60787\n",
      "8/16 avg epoch loss: 6.38252\n",
      "10/16 avg epoch loss: 6.07635\n",
      "12/16 avg epoch loss: 6.04186\n",
      "14/16 avg epoch loss: 6.75714\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 4.14553\n",
      "2/16 avg epoch loss: 3.97268\n",
      "4/16 avg epoch loss: 5.64073\n",
      "6/16 avg epoch loss: 6.90453\n",
      "8/16 avg epoch loss: 6.492\n",
      "10/16 avg epoch loss: 6.51576\n",
      "12/16 avg epoch loss: 6.82229\n",
      "14/16 avg epoch loss: 6.61012\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 4.76356\n",
      "2/16 avg epoch loss: 4.83716\n",
      "4/16 avg epoch loss: 5.54727\n",
      "6/16 avg epoch loss: 5.68131\n",
      "8/16 avg epoch loss: 5.69115\n",
      "10/16 avg epoch loss: 5.62812\n",
      "12/16 avg epoch loss: 6.17327\n",
      "14/16 avg epoch loss: 6.27351\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 6.92192\n",
      "2/16 avg epoch loss: 5.28559\n",
      "4/16 avg epoch loss: 4.87645\n",
      "6/16 avg epoch loss: 5.33476\n",
      "8/16 avg epoch loss: 5.3765\n",
      "10/16 avg epoch loss: 5.71244\n",
      "12/16 avg epoch loss: 6.03653\n",
      "14/16 avg epoch loss: 6.18872\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 5.67924\n",
      "2/16 avg epoch loss: 6.89039\n",
      "4/16 avg epoch loss: 6.32742\n",
      "6/16 avg epoch loss: 6.14152\n",
      "8/16 avg epoch loss: 5.84468\n",
      "10/16 avg epoch loss: 6.29268\n",
      "12/16 avg epoch loss: 6.39418\n",
      "14/16 avg epoch loss: 6.23032\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 5.23795\n",
      "2/16 avg epoch loss: 5.42676\n",
      "4/16 avg epoch loss: 6.11544\n",
      "6/16 avg epoch loss: 5.99324\n",
      "8/16 avg epoch loss: 5.33912\n",
      "10/16 avg epoch loss: 5.49482\n",
      "12/16 avg epoch loss: 5.94194\n",
      "14/16 avg epoch loss: 5.9447\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 5.11868\n",
      "2/16 avg epoch loss: 6.65287\n",
      "4/16 avg epoch loss: 6.15166\n",
      "6/16 avg epoch loss: 6.07506\n",
      "8/16 avg epoch loss: 5.72593\n",
      "10/16 avg epoch loss: 5.66093\n",
      "12/16 avg epoch loss: 5.63884\n",
      "14/16 avg epoch loss: 6.43308\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 6.11669\n",
      "2/16 avg epoch loss: 4.84505\n",
      "4/16 avg epoch loss: 5.94108\n",
      "6/16 avg epoch loss: 5.63693\n",
      "8/16 avg epoch loss: 5.77258\n",
      "10/16 avg epoch loss: 5.46026\n",
      "12/16 avg epoch loss: 5.88355\n",
      "14/16 avg epoch loss: 5.8673\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 6.44504\n",
      "2/16 avg epoch loss: 5.51863\n",
      "4/16 avg epoch loss: 5.31978\n",
      "6/16 avg epoch loss: 5.51044\n",
      "8/16 avg epoch loss: 5.47578\n",
      "10/16 avg epoch loss: 5.62826\n",
      "12/16 avg epoch loss: 5.48749\n",
      "14/16 avg epoch loss: 5.28232\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 2.79998\n",
      "2/16 avg epoch loss: 3.65512\n",
      "4/16 avg epoch loss: 4.40497\n",
      "6/16 avg epoch loss: 4.40814\n",
      "8/16 avg epoch loss: 4.5713\n",
      "10/16 avg epoch loss: 4.72344\n",
      "12/16 avg epoch loss: 5.09806\n",
      "14/16 avg epoch loss: 5.023\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 6.63577\n",
      "2/16 avg epoch loss: 4.61182\n",
      "4/16 avg epoch loss: 3.89358\n",
      "6/16 avg epoch loss: 3.78025\n",
      "8/16 avg epoch loss: 3.85178\n",
      "10/16 avg epoch loss: 4.0714\n",
      "12/16 avg epoch loss: 4.32891\n",
      "14/16 avg epoch loss: 4.59081\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 3.52183\n",
      "2/16 avg epoch loss: 4.21103\n",
      "4/16 avg epoch loss: 4.32455\n",
      "6/16 avg epoch loss: 4.02986\n",
      "8/16 avg epoch loss: 4.23417\n",
      "10/16 avg epoch loss: 4.3475\n",
      "12/16 avg epoch loss: 4.23808\n",
      "14/16 avg epoch loss: 4.31309\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 3.35062\n",
      "2/16 avg epoch loss: 5.12919\n",
      "4/16 avg epoch loss: 4.20763\n",
      "6/16 avg epoch loss: 4.27927\n",
      "8/16 avg epoch loss: 4.04631\n",
      "10/16 avg epoch loss: 4.21427\n",
      "12/16 avg epoch loss: 4.54311\n",
      "14/16 avg epoch loss: 4.34999\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 5.43314\n",
      "2/16 avg epoch loss: 4.89971\n",
      "4/16 avg epoch loss: 4.0603\n",
      "6/16 avg epoch loss: 3.87467\n",
      "8/16 avg epoch loss: 3.63729\n",
      "10/16 avg epoch loss: 3.45763\n",
      "12/16 avg epoch loss: 3.78454\n",
      "14/16 avg epoch loss: 3.98201\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 5.20543\n",
      "2/16 avg epoch loss: 3.4989\n",
      "4/16 avg epoch loss: 4.43094\n",
      "6/16 avg epoch loss: 3.99655\n",
      "8/16 avg epoch loss: 3.62186\n",
      "10/16 avg epoch loss: 3.4489\n",
      "12/16 avg epoch loss: 3.81709\n",
      "14/16 avg epoch loss: 4.01292\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 1.78703\n",
      "2/16 avg epoch loss: 2.4798\n",
      "4/16 avg epoch loss: 2.35034\n",
      "6/16 avg epoch loss: 2.8693\n",
      "8/16 avg epoch loss: 3.62273\n",
      "10/16 avg epoch loss: 3.77883\n",
      "12/16 avg epoch loss: 3.96323\n",
      "14/16 avg epoch loss: 4.04002\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 3.32394\n",
      "2/16 avg epoch loss: 3.86158\n",
      "4/16 avg epoch loss: 4.13936\n",
      "6/16 avg epoch loss: 4.37148\n",
      "8/16 avg epoch loss: 4.48169\n",
      "10/16 avg epoch loss: 4.32593\n",
      "12/16 avg epoch loss: 4.09592\n",
      "14/16 avg epoch loss: 4.44278\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 4.53467\n",
      "2/16 avg epoch loss: 4.15954\n",
      "4/16 avg epoch loss: 4.04076\n",
      "6/16 avg epoch loss: 3.54133\n",
      "8/16 avg epoch loss: 3.35601\n",
      "10/16 avg epoch loss: 3.72147\n",
      "12/16 avg epoch loss: 3.64908\n",
      "14/16 avg epoch loss: 3.7909\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 2.1385\n",
      "2/16 avg epoch loss: 3.74485\n",
      "4/16 avg epoch loss: 3.69309\n",
      "6/16 avg epoch loss: 3.37415\n",
      "8/16 avg epoch loss: 3.14308\n",
      "10/16 avg epoch loss: 3.23585\n",
      "12/16 avg epoch loss: 3.20293\n",
      "14/16 avg epoch loss: 3.57936\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 6.07914\n",
      "2/16 avg epoch loss: 3.50107\n",
      "4/16 avg epoch loss: 3.10063\n",
      "6/16 avg epoch loss: 2.84907\n",
      "8/16 avg epoch loss: 3.50019\n",
      "10/16 avg epoch loss: 3.45324\n",
      "12/16 avg epoch loss: 3.25438\n",
      "14/16 avg epoch loss: 3.39365\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 2.68671\n",
      "2/16 avg epoch loss: 3.18594\n",
      "4/16 avg epoch loss: 3.11149\n",
      "6/16 avg epoch loss: 3.28901\n",
      "8/16 avg epoch loss: 3.3112\n",
      "10/16 avg epoch loss: 3.40813\n",
      "12/16 avg epoch loss: 3.38332\n",
      "14/16 avg epoch loss: 3.36763\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 3.98031\n",
      "2/16 avg epoch loss: 2.92375\n",
      "4/16 avg epoch loss: 2.99137\n",
      "6/16 avg epoch loss: 2.90463\n",
      "8/16 avg epoch loss: 2.97737\n",
      "10/16 avg epoch loss: 3.30508\n",
      "12/16 avg epoch loss: 3.12211\n",
      "14/16 avg epoch loss: 3.10738\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 4.59008\n",
      "2/16 avg epoch loss: 4.06895\n",
      "4/16 avg epoch loss: 3.49725\n",
      "6/16 avg epoch loss: 3.30139\n",
      "8/16 avg epoch loss: 3.11281\n",
      "10/16 avg epoch loss: 2.85422\n",
      "12/16 avg epoch loss: 2.89108\n",
      "14/16 avg epoch loss: 3.06116\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 1.06072\n",
      "2/16 avg epoch loss: 1.86929\n",
      "4/16 avg epoch loss: 2.10276\n",
      "6/16 avg epoch loss: 2.5656\n",
      "8/16 avg epoch loss: 2.79213\n",
      "10/16 avg epoch loss: 3.02761\n",
      "12/16 avg epoch loss: 3.10946\n",
      "14/16 avg epoch loss: 2.99696\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 3.71321\n",
      "2/16 avg epoch loss: 3.60762\n",
      "4/16 avg epoch loss: 3.08502\n",
      "6/16 avg epoch loss: 2.89173\n",
      "8/16 avg epoch loss: 3.09348\n",
      "10/16 avg epoch loss: 2.82477\n",
      "12/16 avg epoch loss: 2.81045\n",
      "14/16 avg epoch loss: 2.8257\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 5.22567\n",
      "2/16 avg epoch loss: 3.05197\n",
      "4/16 avg epoch loss: 2.75927\n",
      "6/16 avg epoch loss: 3.00631\n",
      "8/16 avg epoch loss: 3.22566\n",
      "10/16 avg epoch loss: 3.05176\n",
      "12/16 avg epoch loss: 2.9754\n",
      "14/16 avg epoch loss: 3.13118\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 4.01904\n",
      "2/16 avg epoch loss: 3.45725\n",
      "4/16 avg epoch loss: 3.10977\n",
      "6/16 avg epoch loss: 2.94046\n",
      "8/16 avg epoch loss: 2.88717\n",
      "10/16 avg epoch loss: 2.77855\n",
      "12/16 avg epoch loss: 2.75768\n",
      "14/16 avg epoch loss: 2.7423\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 5.3105\n",
      "2/16 avg epoch loss: 2.75318\n",
      "4/16 avg epoch loss: 3.00086\n",
      "6/16 avg epoch loss: 2.69388\n",
      "8/16 avg epoch loss: 2.62528\n",
      "10/16 avg epoch loss: 2.73176\n",
      "12/16 avg epoch loss: 2.57553\n",
      "14/16 avg epoch loss: 2.59863\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 1.70812\n",
      "2/16 avg epoch loss: 2.23604\n",
      "4/16 avg epoch loss: 2.92469\n",
      "6/16 avg epoch loss: 2.65254\n",
      "8/16 avg epoch loss: 2.73388\n",
      "10/16 avg epoch loss: 2.63539\n",
      "12/16 avg epoch loss: 2.50176\n",
      "14/16 avg epoch loss: 2.48825\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 3.00052\n",
      "2/16 avg epoch loss: 2.29664\n",
      "4/16 avg epoch loss: 2.44318\n",
      "6/16 avg epoch loss: 2.70034\n",
      "8/16 avg epoch loss: 2.47384\n",
      "10/16 avg epoch loss: 2.56764\n",
      "12/16 avg epoch loss: 2.66207\n",
      "14/16 avg epoch loss: 2.51946\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 1.79616\n",
      "2/16 avg epoch loss: 2.10513\n",
      "4/16 avg epoch loss: 2.25618\n",
      "6/16 avg epoch loss: 2.22939\n",
      "8/16 avg epoch loss: 2.21828\n",
      "10/16 avg epoch loss: 2.2233\n",
      "12/16 avg epoch loss: 2.32299\n",
      "14/16 avg epoch loss: 2.35688\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 2.99351\n",
      "2/16 avg epoch loss: 2.84554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/16 avg epoch loss: 2.69665\n",
      "6/16 avg epoch loss: 2.39245\n",
      "8/16 avg epoch loss: 2.39937\n",
      "10/16 avg epoch loss: 2.33444\n",
      "12/16 avg epoch loss: 2.19163\n",
      "14/16 avg epoch loss: 2.20405\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 1.41819\n",
      "2/16 avg epoch loss: 1.90404\n",
      "4/16 avg epoch loss: 2.53215\n",
      "6/16 avg epoch loss: 2.44395\n",
      "8/16 avg epoch loss: 2.42938\n",
      "10/16 avg epoch loss: 2.22314\n",
      "12/16 avg epoch loss: 2.17911\n",
      "14/16 avg epoch loss: 2.22157\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 1.69329\n",
      "2/16 avg epoch loss: 2.47988\n",
      "4/16 avg epoch loss: 2.19031\n",
      "6/16 avg epoch loss: 2.22817\n",
      "8/16 avg epoch loss: 2.28681\n",
      "10/16 avg epoch loss: 2.10902\n",
      "12/16 avg epoch loss: 1.96417\n",
      "14/16 avg epoch loss: 2.02813\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 0.91606\n",
      "2/16 avg epoch loss: 1.17986\n",
      "4/16 avg epoch loss: 1.67277\n",
      "6/16 avg epoch loss: 2.05598\n",
      "8/16 avg epoch loss: 1.8561\n",
      "10/16 avg epoch loss: 1.87498\n",
      "12/16 avg epoch loss: 1.94502\n",
      "14/16 avg epoch loss: 1.91709\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 2.54403\n",
      "2/16 avg epoch loss: 2.30399\n",
      "4/16 avg epoch loss: 1.75495\n",
      "6/16 avg epoch loss: 1.91124\n",
      "8/16 avg epoch loss: 1.67243\n",
      "10/16 avg epoch loss: 1.66549\n",
      "12/16 avg epoch loss: 1.84966\n",
      "14/16 avg epoch loss: 1.95957\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 1.67855\n",
      "2/16 avg epoch loss: 1.34589\n",
      "4/16 avg epoch loss: 1.43425\n",
      "6/16 avg epoch loss: 1.52944\n",
      "8/16 avg epoch loss: 1.45959\n",
      "10/16 avg epoch loss: 1.66658\n",
      "12/16 avg epoch loss: 1.71706\n",
      "14/16 avg epoch loss: 1.94038\n",
      "\n",
      "Training finished!\n",
      "  4.890 s (68464929 allocations: 8.94 GiB)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip320\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip321\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M169.121 1486.45 L2352.76 1486.45 L2352.76 47.2441 L169.121 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip322\">\n",
       "    <rect x=\"169\" y=\"47\" width=\"2185\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"204.846,1486.45 204.846,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"726.373,1486.45 726.373,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1247.9,1486.45 1247.9,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1769.43,1486.45 1769.43,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2290.95,1486.45 2290.95,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"204.846,1486.45 204.846,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"726.373,1486.45 726.373,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1247.9,1486.45 1247.9,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1769.43,1486.45 1769.43,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2290.95,1486.45 2290.95,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M204.846 1517.37 Q201.235 1517.37 199.406 1520.93 Q197.601 1524.47 197.601 1531.6 Q197.601 1538.71 199.406 1542.27 Q201.235 1545.82 204.846 1545.82 Q208.48 1545.82 210.286 1542.27 Q212.114 1538.71 212.114 1531.6 Q212.114 1524.47 210.286 1520.93 Q208.48 1517.37 204.846 1517.37 M204.846 1513.66 Q210.656 1513.66 213.712 1518.27 Q216.79 1522.85 216.79 1531.6 Q216.79 1540.33 213.712 1544.94 Q210.656 1549.52 204.846 1549.52 Q199.036 1549.52 195.957 1544.94 Q192.902 1540.33 192.902 1531.6 Q192.902 1522.85 195.957 1518.27 Q199.036 1513.66 204.846 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M705.146 1544.91 L721.466 1544.91 L721.466 1548.85 L699.521 1548.85 L699.521 1544.91 Q702.183 1542.16 706.767 1537.53 Q711.373 1532.88 712.554 1531.53 Q714.799 1529.01 715.679 1527.27 Q716.582 1525.51 716.582 1523.82 Q716.582 1521.07 714.637 1519.33 Q712.716 1517.6 709.614 1517.6 Q707.415 1517.6 704.961 1518.36 Q702.531 1519.13 699.753 1520.68 L699.753 1515.95 Q702.577 1514.82 705.031 1514.24 Q707.484 1513.66 709.521 1513.66 Q714.892 1513.66 718.086 1516.35 Q721.281 1519.03 721.281 1523.52 Q721.281 1525.65 720.47 1527.57 Q719.683 1529.47 717.577 1532.07 Q716.998 1532.74 713.896 1535.95 Q710.795 1539.15 705.146 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M741.281 1517.37 Q737.669 1517.37 735.841 1520.93 Q734.035 1524.47 734.035 1531.6 Q734.035 1538.71 735.841 1542.27 Q737.669 1545.82 741.281 1545.82 Q744.915 1545.82 746.72 1542.27 Q748.549 1538.71 748.549 1531.6 Q748.549 1524.47 746.72 1520.93 Q744.915 1517.37 741.281 1517.37 M741.281 1513.66 Q747.091 1513.66 750.146 1518.27 Q753.225 1522.85 753.225 1531.6 Q753.225 1540.33 750.146 1544.94 Q747.091 1549.52 741.281 1549.52 Q735.47 1549.52 732.392 1544.94 Q729.336 1540.33 729.336 1531.6 Q729.336 1522.85 732.392 1518.27 Q735.47 1513.66 741.281 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1236.07 1518.36 L1224.27 1536.81 L1236.07 1536.81 L1236.07 1518.36 M1234.84 1514.29 L1240.72 1514.29 L1240.72 1536.81 L1245.66 1536.81 L1245.66 1540.7 L1240.72 1540.7 L1240.72 1548.85 L1236.07 1548.85 L1236.07 1540.7 L1220.47 1540.7 L1220.47 1536.19 L1234.84 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1263.39 1517.37 Q1259.78 1517.37 1257.95 1520.93 Q1256.14 1524.47 1256.14 1531.6 Q1256.14 1538.71 1257.95 1542.27 Q1259.78 1545.82 1263.39 1545.82 Q1267.02 1545.82 1268.83 1542.27 Q1270.65 1538.71 1270.65 1531.6 Q1270.65 1524.47 1268.83 1520.93 Q1267.02 1517.37 1263.39 1517.37 M1263.39 1513.66 Q1269.2 1513.66 1272.25 1518.27 Q1275.33 1522.85 1275.33 1531.6 Q1275.33 1540.33 1272.25 1544.94 Q1269.2 1549.52 1263.39 1549.52 Q1257.58 1549.52 1254.5 1544.94 Q1251.44 1540.33 1251.44 1531.6 Q1251.44 1522.85 1254.5 1518.27 Q1257.58 1513.66 1263.39 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1754.83 1529.7 Q1751.68 1529.7 1749.83 1531.86 Q1748 1534.01 1748 1537.76 Q1748 1541.49 1749.83 1543.66 Q1751.68 1545.82 1754.83 1545.82 Q1757.98 1545.82 1759.81 1543.66 Q1761.66 1541.49 1761.66 1537.76 Q1761.66 1534.01 1759.81 1531.86 Q1757.98 1529.7 1754.83 1529.7 M1764.12 1515.05 L1764.12 1519.31 Q1762.36 1518.48 1760.55 1518.04 Q1758.77 1517.6 1757.01 1517.6 Q1752.38 1517.6 1749.93 1520.72 Q1747.49 1523.85 1747.15 1530.17 Q1748.51 1528.15 1750.57 1527.09 Q1752.63 1526 1755.11 1526 Q1760.32 1526 1763.33 1529.17 Q1766.36 1532.32 1766.36 1537.76 Q1766.36 1543.08 1763.21 1546.3 Q1760.06 1549.52 1754.83 1549.52 Q1748.84 1549.52 1745.67 1544.94 Q1742.49 1540.33 1742.49 1531.6 Q1742.49 1523.41 1746.38 1518.55 Q1750.27 1513.66 1756.82 1513.66 Q1758.58 1513.66 1760.37 1514.01 Q1762.17 1514.36 1764.12 1515.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1784.42 1517.37 Q1780.8 1517.37 1778.98 1520.93 Q1777.17 1524.47 1777.17 1531.6 Q1777.17 1538.71 1778.98 1542.27 Q1780.8 1545.82 1784.42 1545.82 Q1788.05 1545.82 1789.86 1542.27 Q1791.68 1538.71 1791.68 1531.6 Q1791.68 1524.47 1789.86 1520.93 Q1788.05 1517.37 1784.42 1517.37 M1784.42 1513.66 Q1790.23 1513.66 1793.28 1518.27 Q1796.36 1522.85 1796.36 1531.6 Q1796.36 1540.33 1793.28 1544.94 Q1790.23 1549.52 1784.42 1549.52 Q1778.61 1549.52 1775.53 1544.94 Q1772.47 1540.33 1772.47 1531.6 Q1772.47 1522.85 1775.53 1518.27 Q1778.61 1513.66 1784.42 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2275.83 1532.44 Q2272.49 1532.44 2270.57 1534.22 Q2268.67 1536 2268.67 1539.13 Q2268.67 1542.25 2270.57 1544.03 Q2272.49 1545.82 2275.83 1545.82 Q2279.16 1545.82 2281.08 1544.03 Q2283 1542.23 2283 1539.13 Q2283 1536 2281.08 1534.22 Q2279.18 1532.44 2275.83 1532.44 M2271.15 1530.45 Q2268.14 1529.7 2266.45 1527.64 Q2264.79 1525.58 2264.79 1522.62 Q2264.79 1518.48 2267.73 1516.07 Q2270.69 1513.66 2275.83 1513.66 Q2280.99 1513.66 2283.93 1516.07 Q2286.87 1518.48 2286.87 1522.62 Q2286.87 1525.58 2285.18 1527.64 Q2283.51 1529.7 2280.53 1530.45 Q2283.91 1531.23 2285.78 1533.52 Q2287.68 1535.82 2287.68 1539.13 Q2287.68 1544.15 2284.6 1546.83 Q2281.55 1549.52 2275.83 1549.52 Q2270.11 1549.52 2267.03 1546.83 Q2263.98 1544.15 2263.98 1539.13 Q2263.98 1535.82 2265.87 1533.52 Q2267.77 1531.23 2271.15 1530.45 M2269.44 1523.06 Q2269.44 1525.75 2271.11 1527.25 Q2272.8 1528.76 2275.83 1528.76 Q2278.84 1528.76 2280.53 1527.25 Q2282.24 1525.75 2282.24 1523.06 Q2282.24 1520.38 2280.53 1518.87 Q2278.84 1517.37 2275.83 1517.37 Q2272.8 1517.37 2271.11 1518.87 Q2269.44 1520.38 2269.44 1523.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2305.99 1517.37 Q2302.38 1517.37 2300.55 1520.93 Q2298.74 1524.47 2298.74 1531.6 Q2298.74 1538.71 2300.55 1542.27 Q2302.38 1545.82 2305.99 1545.82 Q2309.62 1545.82 2311.43 1542.27 Q2313.26 1538.71 2313.26 1531.6 Q2313.26 1524.47 2311.43 1520.93 Q2309.62 1517.37 2305.99 1517.37 M2305.99 1513.66 Q2311.8 1513.66 2314.86 1518.27 Q2317.93 1522.85 2317.93 1531.6 Q2317.93 1540.33 2314.86 1544.94 Q2311.8 1549.52 2305.99 1549.52 Q2300.18 1549.52 2297.1 1544.94 Q2294.05 1540.33 2294.05 1531.6 Q2294.05 1522.85 2297.1 1518.27 Q2300.18 1513.66 2305.99 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,1292.99 2352.76,1292.99 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,1010.56 2352.76,1010.56 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,728.13 2352.76,728.13 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,445.701 2352.76,445.701 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,163.271 2352.76,163.271 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1486.45 169.121,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1292.99 188.019,1292.99 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1010.56 188.019,1010.56 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,728.13 188.019,728.13 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,445.701 188.019,445.701 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,163.271 188.019,163.271 \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M95.1817 1291.63 Q98.5382 1292.35 100.413 1294.62 Q102.311 1296.89 102.311 1300.22 Q102.311 1305.34 98.7928 1308.14 Q95.2743 1310.94 88.7928 1310.94 Q86.6169 1310.94 84.3021 1310.5 Q82.0105 1310.08 79.5568 1309.23 L79.5568 1304.71 Q81.5012 1305.85 83.816 1306.43 Q86.1308 1307 88.654 1307 Q93.0521 1307 95.3437 1305.27 Q97.6585 1303.53 97.6585 1300.22 Q97.6585 1297.17 95.5058 1295.45 Q93.3762 1293.72 89.5567 1293.72 L85.529 1293.72 L85.529 1289.87 L89.7419 1289.87 Q93.191 1289.87 95.0197 1288.51 Q96.8484 1287.12 96.8484 1284.53 Q96.8484 1281.87 94.9502 1280.45 Q93.0752 1279.02 89.5567 1279.02 Q87.6354 1279.02 85.4364 1279.43 Q83.2373 1279.85 80.5984 1280.73 L80.5984 1276.56 Q83.2605 1275.82 85.5753 1275.45 Q87.9132 1275.08 89.9734 1275.08 Q95.2974 1275.08 98.3993 1277.51 Q101.501 1279.92 101.501 1284.04 Q101.501 1286.91 99.8576 1288.9 Q98.2141 1290.87 95.1817 1291.63 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 1278.79 Q117.566 1278.79 115.737 1282.35 Q113.932 1285.89 113.932 1293.02 Q113.932 1300.13 115.737 1303.69 Q117.566 1307.24 121.177 1307.24 Q124.811 1307.24 126.617 1303.69 Q128.445 1300.13 128.445 1293.02 Q128.445 1285.89 126.617 1282.35 Q124.811 1278.79 121.177 1278.79 M121.177 1275.08 Q126.987 1275.08 130.043 1279.69 Q133.121 1284.27 133.121 1293.02 Q133.121 1301.75 130.043 1306.36 Q126.987 1310.94 121.177 1310.94 Q115.367 1310.94 112.288 1306.36 Q109.233 1301.75 109.233 1293.02 Q109.233 1284.27 112.288 1279.69 Q115.367 1275.08 121.177 1275.08 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M91.5938 1008.7 Q88.4456 1008.7 86.5938 1010.85 Q84.7651 1013 84.7651 1016.75 Q84.7651 1020.48 86.5938 1022.65 Q88.4456 1024.81 91.5938 1024.81 Q94.7419 1024.81 96.5706 1022.65 Q98.4224 1020.48 98.4224 1016.75 Q98.4224 1013 96.5706 1010.85 Q94.7419 1008.7 91.5938 1008.7 M100.876 994.043 L100.876 998.302 Q99.1169 997.469 97.3113 997.029 Q95.5289 996.589 93.7697 996.589 Q89.1401 996.589 86.6864 999.714 Q84.2558 1002.84 83.9086 1009.16 Q85.2743 1007.14 87.3345 1006.08 Q89.3947 1004.99 91.8715 1004.99 Q97.0798 1004.99 100.089 1008.16 Q103.121 1011.31 103.121 1016.75 Q103.121 1022.07 99.9733 1025.29 Q96.8252 1028.51 91.5938 1028.51 Q85.5984 1028.51 82.4271 1023.93 Q79.2559 1019.32 79.2559 1010.59 Q79.2559 1002.4 83.1447 997.538 Q87.0336 992.654 93.5845 992.654 Q95.3437 992.654 97.1261 993.001 Q98.9317 993.348 100.876 994.043 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 996.357 Q117.566 996.357 115.737 999.922 Q113.932 1003.46 113.932 1010.59 Q113.932 1017.7 115.737 1021.26 Q117.566 1024.81 121.177 1024.81 Q124.811 1024.81 126.617 1021.26 Q128.445 1017.7 128.445 1010.59 Q128.445 1003.46 126.617 999.922 Q124.811 996.357 121.177 996.357 M121.177 992.654 Q126.987 992.654 130.043 997.26 Q133.121 1001.84 133.121 1010.59 Q133.121 1019.32 130.043 1023.93 Q126.987 1028.51 121.177 1028.51 Q115.367 1028.51 112.288 1023.93 Q109.233 1019.32 109.233 1010.59 Q109.233 1001.84 112.288 997.26 Q115.367 992.654 121.177 992.654 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M81.154 744.692 L81.154 740.433 Q82.9133 741.266 84.7188 741.706 Q86.5243 742.146 88.2604 742.146 Q92.89 742.146 95.3206 739.044 Q97.7743 735.919 98.1215 729.576 Q96.7789 731.567 94.7187 732.632 Q92.6586 733.697 90.1586 733.697 Q84.9734 733.697 81.941 730.572 Q78.9318 727.424 78.9318 721.984 Q78.9318 716.66 82.0799 713.442 Q85.2281 710.225 90.4595 710.225 Q96.4548 710.225 99.603 714.831 Q102.774 719.414 102.774 728.164 Q102.774 736.336 98.8854 741.22 Q95.0197 746.081 88.4688 746.081 Q86.7095 746.081 84.904 745.734 Q83.0984 745.386 81.154 744.692 M90.4595 730.039 Q93.6076 730.039 95.4363 727.887 Q97.2882 725.734 97.2882 721.984 Q97.2882 718.257 95.4363 716.104 Q93.6076 713.928 90.4595 713.928 Q87.3114 713.928 85.4595 716.104 Q83.6308 718.257 83.6308 721.984 Q83.6308 725.734 85.4595 727.887 Q87.3114 730.039 90.4595 730.039 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 713.928 Q117.566 713.928 115.737 717.493 Q113.932 721.035 113.932 728.164 Q113.932 735.271 115.737 738.836 Q117.566 742.377 121.177 742.377 Q124.811 742.377 126.617 738.836 Q128.445 735.271 128.445 728.164 Q128.445 721.035 126.617 717.493 Q124.811 713.928 121.177 713.928 M121.177 710.225 Q126.987 710.225 130.043 714.831 Q133.121 719.414 133.121 728.164 Q133.121 736.891 130.043 741.498 Q126.987 746.081 121.177 746.081 Q115.367 746.081 112.288 741.498 Q109.233 736.891 109.233 728.164 Q109.233 719.414 112.288 714.831 Q115.367 710.225 121.177 710.225 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M51.6634 459.045 L59.3023 459.045 L59.3023 432.68 L50.9921 434.346 L50.9921 430.087 L59.256 428.421 L63.9319 428.421 L63.9319 459.045 L71.5707 459.045 L71.5707 462.981 L51.6634 462.981 L51.6634 459.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M85.0429 459.045 L101.362 459.045 L101.362 462.981 L79.4179 462.981 L79.4179 459.045 Q82.0799 456.291 86.6632 451.661 Q91.2697 447.008 92.4502 445.666 Q94.6956 443.143 95.5752 441.407 Q96.478 439.647 96.478 437.957 Q96.478 435.203 94.5336 433.467 Q92.6123 431.731 89.5104 431.731 Q87.3114 431.731 84.8577 432.495 Q82.4271 433.258 79.6494 434.809 L79.6494 430.087 Q82.4734 428.953 84.9271 428.374 Q87.3808 427.796 89.4178 427.796 Q94.7882 427.796 97.9826 430.481 Q101.177 433.166 101.177 437.657 Q101.177 439.786 100.367 441.707 Q99.5798 443.606 97.4734 446.198 Q96.8947 446.869 93.7928 450.087 Q90.691 453.281 85.0429 459.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 431.499 Q117.566 431.499 115.737 435.064 Q113.932 438.606 113.932 445.735 Q113.932 452.842 115.737 456.406 Q117.566 459.948 121.177 459.948 Q124.811 459.948 126.617 456.406 Q128.445 452.842 128.445 445.735 Q128.445 438.606 126.617 435.064 Q124.811 431.499 121.177 431.499 M121.177 427.796 Q126.987 427.796 130.043 432.402 Q133.121 436.985 133.121 445.735 Q133.121 454.462 130.043 459.068 Q126.987 463.652 121.177 463.652 Q115.367 463.652 112.288 459.068 Q109.233 454.462 109.233 445.735 Q109.233 436.985 112.288 432.402 Q115.367 427.796 121.177 427.796 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M51.6634 176.616 L59.3023 176.616 L59.3023 150.251 L50.9921 151.917 L50.9921 147.658 L59.256 145.991 L63.9319 145.991 L63.9319 176.616 L71.5707 176.616 L71.5707 180.551 L51.6634 180.551 L51.6634 176.616 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M81.0614 145.991 L99.4178 145.991 L99.4178 149.927 L85.3438 149.927 L85.3438 158.399 Q86.3623 158.052 87.3808 157.889 Q88.3993 157.704 89.4178 157.704 Q95.2049 157.704 98.5845 160.876 Q101.964 164.047 101.964 169.463 Q101.964 175.042 98.4919 178.144 Q95.0197 181.223 88.7003 181.223 Q86.5243 181.223 84.2558 180.852 Q82.0105 180.482 79.6031 179.741 L79.6031 175.042 Q81.6864 176.176 83.9086 176.732 Q86.1308 177.288 88.6077 177.288 Q92.6123 177.288 94.9502 175.181 Q97.2882 173.075 97.2882 169.463 Q97.2882 165.852 94.9502 163.746 Q92.6123 161.639 88.6077 161.639 Q86.7327 161.639 84.8577 162.056 Q83.0058 162.473 81.0614 163.352 L81.0614 145.991 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 149.07 Q117.566 149.07 115.737 152.635 Q113.932 156.177 113.932 163.306 Q113.932 170.413 115.737 173.977 Q117.566 177.519 121.177 177.519 Q124.811 177.519 126.617 173.977 Q128.445 170.413 128.445 163.306 Q128.445 156.177 126.617 152.635 Q124.811 149.07 121.177 149.07 M121.177 145.366 Q126.987 145.366 130.043 149.973 Q133.121 154.556 133.121 163.306 Q133.121 172.033 130.043 176.639 Q126.987 181.223 121.177 181.223 Q115.367 181.223 112.288 176.639 Q109.233 172.033 109.233 163.306 Q109.233 154.556 112.288 149.973 Q115.367 145.366 121.177 145.366 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip322)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"230.922,87.9763 256.999,107.006 283.075,147.7 309.151,184.098 335.228,227.837 361.304,268.984 387.38,317.203 413.457,380.234 439.533,859.216 465.61,896.404 491.686,915.151 517.762,951.486 543.839,969.882 569.915,995.593 595.991,1017.45 622.068,1036.29 648.144,1160.46 674.22,1216.6 700.297,1206.26 726.373,1239.98 752.45,1237.97 778.526,1217.18 804.602,1221.16 830.679,1214.64 856.755,1124.57 882.831,1219.07 908.908,1241.39 934.984,1269.36 961.06,1281.73 987.137,1277.49 1013.21,1279.49 1039.29,1278.16 1065.37,1325.29 1091.44,1327.52 1117.52,1341.94 1143.59,1339.91 1169.67,1332.26 1195.75,1324.24 1221.82,1317.05 1247.9,1323.98 1273.98,1387.35 1300.05,1377.73 1326.13,1397.01 1352.21,1351.24 1378.28,1353.82 1404.36,1361.94 1430.43,1362 1456.51,1350.3 1482.59,1374.53 1508.66,1385.21 1534.74,1386.24 1560.82,1379.5 1586.89,1380.39 1612.97,1384.75 1639.05,1387.68 1665.12,1385.05 1691.2,1403.53 1717.27,1368.46 1743.35,1384.79 1769.43,1383.18 1795.5,1384.05 1821.58,1382.74 1847.66,1393.46 1873.73,1389.89 1899.81,1445.72 1925.89,1399.94 1951.96,1375.46 1978.04,1384.12 2004.11,1390.62 2030.19,1391.66 2056.27,1402.4 2082.34,1407.59 2108.42,1371.16 2134.5,1437.36 2160.57,1435.97 2186.65,1417.23 2212.73,1411.78 2238.8,1417.98 2264.88,1420.87 2290.95,1422.6 \"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"413.457\" cy=\"380.234\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"622.068\" cy=\"1036.29\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"830.679\" cy=\"1214.64\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1039.29\" cy=\"1278.16\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1247.9\" cy=\"1323.98\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1456.51\" cy=\"1350.3\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1665.12\" cy=\"1385.05\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1873.73\" cy=\"1389.89\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"2082.34\" cy=\"1407.59\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"2290.95\" cy=\"1422.6\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1765.4 250.738 L2279.97 250.738 L2279.97 95.2176 L1765.4 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1765.4,250.738 2279.97,250.738 2279.97,95.2176 1765.4,95.2176 1765.4,250.738 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.67,147.058 1935.24,147.058 \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1959.5 128.319 L1963.76 128.319 L1963.76 164.338 L1959.5 164.338 L1959.5 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1982.72 141.398 Q1979.29 141.398 1977.3 144.083 Q1975.31 146.745 1975.31 151.398 Q1975.31 156.051 1977.28 158.736 Q1979.27 161.398 1982.72 161.398 Q1986.12 161.398 1988.11 158.713 Q1990.11 156.027 1990.11 151.398 Q1990.11 146.791 1988.11 144.106 Q1986.12 141.398 1982.72 141.398 M1982.72 137.787 Q1988.28 137.787 1991.45 141.398 Q1994.62 145.009 1994.62 151.398 Q1994.62 157.764 1991.45 161.398 Q1988.28 165.009 1982.72 165.009 Q1977.14 165.009 1973.97 161.398 Q1970.82 157.764 1970.82 151.398 Q1970.82 145.009 1973.97 141.398 Q1977.14 137.787 1982.72 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2018.21 139.176 L2018.21 143.203 Q2016.4 142.277 2014.46 141.815 Q2012.51 141.352 2010.43 141.352 Q2007.26 141.352 2005.66 142.324 Q2004.09 143.296 2004.09 145.24 Q2004.09 146.722 2005.22 147.578 Q2006.35 148.412 2009.78 149.176 L2011.24 149.5 Q2015.78 150.472 2017.67 152.254 Q2019.6 154.014 2019.6 157.185 Q2019.6 160.796 2016.73 162.902 Q2013.88 165.009 2008.88 165.009 Q2006.79 165.009 2004.53 164.592 Q2002.28 164.199 1999.78 163.388 L1999.78 158.99 Q2002.14 160.217 2004.43 160.842 Q2006.73 161.444 2008.97 161.444 Q2011.98 161.444 2013.6 160.426 Q2015.22 159.384 2015.22 157.509 Q2015.22 155.773 2014.04 154.847 Q2012.88 153.921 2008.92 153.064 L2007.44 152.717 Q2003.48 151.884 2001.73 150.171 Q1999.97 148.435 1999.97 145.426 Q1999.97 141.768 2002.56 139.778 Q2005.15 137.787 2009.92 137.787 Q2012.28 137.787 2014.36 138.134 Q2016.45 138.481 2018.21 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2042.91 139.176 L2042.91 143.203 Q2041.1 142.277 2039.16 141.815 Q2037.21 141.352 2035.13 141.352 Q2031.96 141.352 2030.36 142.324 Q2028.79 143.296 2028.79 145.24 Q2028.79 146.722 2029.92 147.578 Q2031.05 148.412 2034.48 149.176 L2035.94 149.5 Q2040.48 150.472 2042.37 152.254 Q2044.29 154.014 2044.29 157.185 Q2044.29 160.796 2041.42 162.902 Q2038.58 165.009 2033.58 165.009 Q2031.49 165.009 2029.23 164.592 Q2026.98 164.199 2024.48 163.388 L2024.48 158.99 Q2026.84 160.217 2029.13 160.842 Q2031.42 161.444 2033.67 161.444 Q2036.68 161.444 2038.3 160.426 Q2039.92 159.384 2039.92 157.509 Q2039.92 155.773 2038.74 154.847 Q2037.58 153.921 2033.62 153.064 L2032.14 152.717 Q2028.18 151.884 2026.42 150.171 Q2024.67 148.435 2024.67 145.426 Q2024.67 141.768 2027.26 139.778 Q2029.85 137.787 2034.62 137.787 Q2036.98 137.787 2039.06 138.134 Q2041.15 138.481 2042.91 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2070.78 172.208 L2070.78 175.518 L2046.15 175.518 L2046.15 172.208 L2070.78 172.208 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2093.39 151.398 Q2093.39 146.699 2091.45 144.037 Q2089.53 141.352 2086.15 141.352 Q2082.77 141.352 2080.82 144.037 Q2078.9 146.699 2078.9 151.398 Q2078.9 156.097 2080.82 158.782 Q2082.77 161.444 2086.15 161.444 Q2089.53 161.444 2091.45 158.782 Q2093.39 156.097 2093.39 151.398 M2078.9 142.347 Q2080.24 140.032 2082.28 138.921 Q2084.34 137.787 2087.19 137.787 Q2091.91 137.787 2094.85 141.537 Q2097.81 145.287 2097.81 151.398 Q2097.81 157.509 2094.85 161.259 Q2091.91 165.009 2087.19 165.009 Q2084.34 165.009 2082.28 163.898 Q2080.24 162.763 2078.9 160.449 L2078.9 164.338 L2074.62 164.338 L2074.62 128.319 L2078.9 128.319 L2078.9 142.347 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2116.66 151.305 Q2111.49 151.305 2109.5 152.486 Q2107.51 153.666 2107.51 156.514 Q2107.51 158.782 2108.99 160.125 Q2110.5 161.444 2113.07 161.444 Q2116.61 161.444 2118.74 158.944 Q2120.89 156.421 2120.89 152.254 L2120.89 151.305 L2116.66 151.305 M2125.15 149.546 L2125.15 164.338 L2120.89 164.338 L2120.89 160.402 Q2119.43 162.763 2117.26 163.898 Q2115.08 165.009 2111.93 165.009 Q2107.95 165.009 2105.59 162.787 Q2103.25 160.541 2103.25 156.791 Q2103.25 152.416 2106.17 150.194 Q2109.11 147.972 2114.92 147.972 L2120.89 147.972 L2120.89 147.555 Q2120.89 144.615 2118.95 143.018 Q2117.03 141.398 2113.53 141.398 Q2111.31 141.398 2109.2 141.93 Q2107.1 142.463 2105.15 143.527 L2105.15 139.592 Q2107.49 138.69 2109.69 138.25 Q2111.89 137.787 2113.97 137.787 Q2119.6 137.787 2122.37 140.703 Q2125.15 143.62 2125.15 149.546 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2138.14 131.051 L2138.14 138.412 L2146.91 138.412 L2146.91 141.722 L2138.14 141.722 L2138.14 155.796 Q2138.14 158.967 2138.99 159.87 Q2139.87 160.773 2142.53 160.773 L2146.91 160.773 L2146.91 164.338 L2142.53 164.338 Q2137.6 164.338 2135.73 162.509 Q2133.85 160.657 2133.85 155.796 L2133.85 141.722 L2130.73 141.722 L2130.73 138.412 L2133.85 138.412 L2133.85 131.051 L2138.14 131.051 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2171.17 139.407 L2171.17 143.389 Q2169.36 142.393 2167.53 141.907 Q2165.73 141.398 2163.88 141.398 Q2159.73 141.398 2157.44 144.037 Q2155.15 146.652 2155.15 151.398 Q2155.15 156.143 2157.44 158.782 Q2159.73 161.398 2163.88 161.398 Q2165.73 161.398 2167.53 160.912 Q2169.36 160.402 2171.17 159.407 L2171.17 163.342 Q2169.39 164.176 2167.47 164.592 Q2165.57 165.009 2163.41 165.009 Q2157.56 165.009 2154.11 161.328 Q2150.66 157.648 2150.66 151.398 Q2150.66 145.055 2154.13 141.421 Q2157.63 137.787 2163.69 137.787 Q2165.66 137.787 2167.53 138.203 Q2169.41 138.597 2171.17 139.407 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2200.13 148.689 L2200.13 164.338 L2195.87 164.338 L2195.87 148.828 Q2195.87 145.148 2194.43 143.319 Q2193 141.49 2190.13 141.49 Q2186.68 141.49 2184.69 143.69 Q2182.7 145.889 2182.7 149.685 L2182.7 164.338 L2178.41 164.338 L2178.41 128.319 L2182.7 128.319 L2182.7 142.44 Q2184.22 140.102 2186.28 138.944 Q2188.37 137.787 2191.08 137.787 Q2195.54 137.787 2197.84 140.565 Q2200.13 143.319 2200.13 148.689 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2230.8 150.31 L2230.8 152.393 L2211.21 152.393 Q2211.49 156.791 2213.85 159.106 Q2216.24 161.398 2220.47 161.398 Q2222.93 161.398 2225.22 160.796 Q2227.53 160.194 2229.8 158.99 L2229.8 163.018 Q2227.51 163.99 2225.1 164.5 Q2222.7 165.009 2220.22 165.009 Q2214.02 165.009 2210.38 161.398 Q2206.77 157.787 2206.77 151.629 Q2206.77 145.264 2210.2 141.537 Q2213.65 137.787 2219.48 137.787 Q2224.71 137.787 2227.74 141.166 Q2230.8 144.523 2230.8 150.31 M2226.54 149.06 Q2226.49 145.565 2224.57 143.481 Q2222.67 141.398 2219.53 141.398 Q2215.96 141.398 2213.81 143.412 Q2211.68 145.426 2211.35 149.083 L2226.54 149.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2254.32 139.176 L2254.32 143.203 Q2252.51 142.277 2250.57 141.815 Q2248.62 141.352 2246.54 141.352 Q2243.37 141.352 2241.77 142.324 Q2240.2 143.296 2240.2 145.24 Q2240.2 146.722 2241.33 147.578 Q2242.46 148.412 2245.89 149.176 L2247.35 149.5 Q2251.89 150.472 2253.78 152.254 Q2255.71 154.014 2255.71 157.185 Q2255.71 160.796 2252.84 162.902 Q2249.99 165.009 2244.99 165.009 Q2242.9 165.009 2240.64 164.592 Q2238.39 164.199 2235.89 163.388 L2235.89 158.99 Q2238.25 160.217 2240.54 160.842 Q2242.84 161.444 2245.08 161.444 Q2248.09 161.444 2249.71 160.426 Q2251.33 159.384 2251.33 157.509 Q2251.33 155.773 2250.15 154.847 Q2248.99 153.921 2245.03 153.064 L2243.55 152.717 Q2239.59 151.884 2237.84 150.171 Q2236.08 148.435 2236.08 145.426 Q2236.08 141.768 2238.67 139.778 Q2241.26 137.787 2246.03 137.787 Q2248.39 137.787 2250.47 138.134 Q2252.56 138.481 2254.32 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip320)\" cx=\"1862.45\" cy=\"198.898\" r=\"20.48\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"4.55111\"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1983.53 202.15 L1983.53 204.233 L1963.95 204.233 Q1964.23 208.631 1966.59 210.946 Q1968.97 213.238 1973.21 213.238 Q1975.66 213.238 1977.95 212.636 Q1980.27 212.034 1982.54 210.83 L1982.54 214.858 Q1980.24 215.83 1977.84 216.34 Q1975.43 216.849 1972.95 216.849 Q1966.75 216.849 1963.11 213.238 Q1959.5 209.627 1959.5 203.469 Q1959.5 197.104 1962.93 193.377 Q1966.38 189.627 1972.21 189.627 Q1977.44 189.627 1980.48 193.006 Q1983.53 196.363 1983.53 202.15 M1979.27 200.9 Q1979.23 197.405 1977.3 195.321 Q1975.41 193.238 1972.26 193.238 Q1968.69 193.238 1966.54 195.252 Q1964.41 197.266 1964.09 200.923 L1979.27 200.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1994.64 212.289 L1994.64 226.039 L1990.36 226.039 L1990.36 190.252 L1994.64 190.252 L1994.64 194.187 Q1995.98 191.872 1998.02 190.761 Q2000.08 189.627 2002.93 189.627 Q2007.65 189.627 2010.59 193.377 Q2013.55 197.127 2013.55 203.238 Q2013.55 209.349 2010.59 213.099 Q2007.65 216.849 2002.93 216.849 Q2000.08 216.849 1998.02 215.738 Q1995.98 214.603 1994.64 212.289 M2009.13 203.238 Q2009.13 198.539 2007.19 195.877 Q2005.27 193.192 2001.89 193.192 Q1998.51 193.192 1996.56 195.877 Q1994.64 198.539 1994.64 203.238 Q1994.64 207.937 1996.56 210.622 Q1998.51 213.284 2001.89 213.284 Q2005.27 213.284 2007.19 210.622 Q2009.13 207.937 2009.13 203.238 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2030.66 193.238 Q2027.23 193.238 2025.24 195.923 Q2023.25 198.585 2023.25 203.238 Q2023.25 207.891 2025.22 210.576 Q2027.21 213.238 2030.66 213.238 Q2034.06 213.238 2036.05 210.553 Q2038.04 207.867 2038.04 203.238 Q2038.04 198.631 2036.05 195.946 Q2034.06 193.238 2030.66 193.238 M2030.66 189.627 Q2036.22 189.627 2039.39 193.238 Q2042.56 196.849 2042.56 203.238 Q2042.56 209.604 2039.39 213.238 Q2036.22 216.849 2030.66 216.849 Q2025.08 216.849 2021.91 213.238 Q2018.76 209.604 2018.76 203.238 Q2018.76 196.849 2021.91 193.238 Q2025.08 189.627 2030.66 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2068.28 191.247 L2068.28 195.229 Q2066.47 194.233 2064.64 193.747 Q2062.84 193.238 2060.98 193.238 Q2056.84 193.238 2054.55 195.877 Q2052.26 198.492 2052.26 203.238 Q2052.26 207.983 2054.55 210.622 Q2056.84 213.238 2060.98 213.238 Q2062.84 213.238 2064.64 212.752 Q2066.47 212.242 2068.28 211.247 L2068.28 215.182 Q2066.49 216.016 2064.57 216.432 Q2062.67 216.849 2060.52 216.849 Q2054.66 216.849 2051.22 213.168 Q2047.77 209.488 2047.77 203.238 Q2047.77 196.895 2051.24 193.261 Q2054.73 189.627 2060.8 189.627 Q2062.77 189.627 2064.64 190.043 Q2066.52 190.437 2068.28 191.247 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2097.23 200.529 L2097.23 216.178 L2092.97 216.178 L2092.97 200.668 Q2092.97 196.988 2091.54 195.159 Q2090.1 193.33 2087.23 193.33 Q2083.79 193.33 2081.79 195.53 Q2079.8 197.729 2079.8 201.525 L2079.8 216.178 L2075.52 216.178 L2075.52 180.159 L2079.8 180.159 L2079.8 194.28 Q2081.33 191.942 2083.39 190.784 Q2085.47 189.627 2088.18 189.627 Q2092.65 189.627 2094.94 192.405 Q2097.23 195.159 2097.23 200.529 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2122.26 191.016 L2122.26 195.043 Q2120.45 194.117 2118.51 193.655 Q2116.56 193.192 2114.48 193.192 Q2111.31 193.192 2109.71 194.164 Q2108.14 195.136 2108.14 197.08 Q2108.14 198.562 2109.27 199.418 Q2110.41 200.252 2113.83 201.016 L2115.29 201.34 Q2119.83 202.312 2121.72 204.094 Q2123.65 205.854 2123.65 209.025 Q2123.65 212.636 2120.78 214.742 Q2117.93 216.849 2112.93 216.849 Q2110.85 216.849 2108.58 216.432 Q2106.33 216.039 2103.83 215.228 L2103.83 210.83 Q2106.19 212.057 2108.48 212.682 Q2110.78 213.284 2113.02 213.284 Q2116.03 213.284 2117.65 212.266 Q2119.27 211.224 2119.27 209.349 Q2119.27 207.613 2118.09 206.687 Q2116.93 205.761 2112.97 204.904 L2111.49 204.557 Q2107.53 203.724 2105.78 202.011 Q2104.02 200.275 2104.02 197.266 Q2104.02 193.608 2106.61 191.618 Q2109.2 189.627 2113.97 189.627 Q2116.33 189.627 2118.41 189.974 Q2120.5 190.321 2122.26 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.0025\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=32, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(72, 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model)\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2522c34c-babf-4c8b-85b1-24a2268f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
