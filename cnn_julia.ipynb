{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9beed57c-7db7-4e59-9990-a470390d4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "#@code_warntype, @simd, @inbounds, statyczne typowenie (brak niestabilności typów, brak Any), @benchmark_tools, @btime, brak alokacji, \n",
    "#kolumny czy wiersze w for\n",
    "#test co szybsz, Matrix czy Array{Float64, 2}\n",
    "#softmax backward\n",
    "\n",
    "mutable struct Conv2DNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    W::Array{Float64, 4}\n",
    "    b::Vector{Float64}\n",
    "    batch_size::Int\n",
    "    kernel_size::Tuple{Int, Int}\n",
    "    filters::Int\n",
    "    output::Array{Float64, 4}\n",
    "    gradient_W::Array{Float64, 4}\n",
    "    gradient_b::Vector{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct MaxPoolNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    pool_size::Tuple{Int, Int}\n",
    "    output::Array{Float64, 4}\n",
    "end\n",
    "\n",
    "mutable struct DenseNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    W::Matrix{Float64} \n",
    "    b::Vector{Float64}\n",
    "    neurons::Int\n",
    "    output::Matrix{Float64}\n",
    "    gradient_W::Matrix{Float64}\n",
    "    gradient_b::Matrix{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct FlattenNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    output::Matrix{Float64}\n",
    "    input_shape::Tuple{Int, Int, Int}\n",
    "end\n",
    "\n",
    "mutable struct ReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct SoftmaxNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct SigmoidNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct TanhNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct LeakyReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    alpha::Float64\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8eb86b4-d598-4d00-ab75-f71b4ab219a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 9 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward!(node::Conv2DNode, x) \n",
    "    node.x = x\n",
    "    W = node.W\n",
    "    b = node.b\n",
    "\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width,  num_chanels, num_filters = size(W)\n",
    "\n",
    "    output_height = 1 + (input_height - filter_height)\n",
    "    output_width = 1 + (input_width - filter_width)\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, num_filters)\n",
    "    for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:input_width-filter_width+1, i in 1:input_height-filter_height+1\n",
    "                    window = view(x, n, i:i+filter_height-1, j:j+filter_width-1, :)\n",
    "                    output[n, i, j, f] = sum(window .* W[:,:,:,f]) + b[f]\n",
    "                    \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    node.output = output\n",
    "end\n",
    "\n",
    "function forward!(node::MaxPoolNode, x)\n",
    "    node.x = x\n",
    "    pool_size = node.pool_size\n",
    "    \n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "\n",
    "    output_height = 1 + (input_height - pool_size[1]) ÷ pool_size[1]\n",
    "    output_width = 1 + (input_width - pool_size[2]) ÷ pool_size[2]\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, input_channels)\n",
    "\n",
    "    for n in 1:batch_size\n",
    "        for c in 1:input_channels\n",
    "            for j in 1:pool_size[2]:input_width-pool_size[2]+1, i in 1:pool_size[1]:input_height-pool_size[1]+1\n",
    "                    window = view(x, n, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c)\n",
    "                    output[n, 1+div(i-1, pool_size[1]), 1+div(j-1, pool_size[2]), c] = maximum(window)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    node.output = output\n",
    "end\n",
    "\n",
    "\n",
    "function forward!(node::ReLUNode, x)\n",
    "    node.x = x\n",
    "    node.output = max.(0, x)\n",
    "end\n",
    "\n",
    "function forward!(node::DenseNode, x)\n",
    "    node.x = x\n",
    "    W = node.W\n",
    "    b = node.b\n",
    "    node.output = x * W .+ b'\n",
    "end\n",
    "\n",
    "function forward!(node::SoftmaxNode, x)\n",
    "    node.x = x\n",
    "    exp_x = exp.(x .- maximum(x, dims=2))\n",
    "    node.output = exp_x ./ sum(exp_x, dims=2)\n",
    "end\n",
    "\n",
    "function forward!(node::FlattenNode, x)\n",
    "    node.x = x\n",
    "    node.output = reshape(x, size(x, 1), size(x, 2) * size(x, 3) * size(x, 4))\n",
    "end\n",
    "\n",
    "function forward!(node::SigmoidNode, x)\n",
    "    node.x = x\n",
    "    node.output = 1.0 ./ (1.0 .+ exp.(-x))\n",
    "end\n",
    "\n",
    "function forward!(node::TanhNode, x)\n",
    "    node.x = x\n",
    "    node.output = tanh.(x)\n",
    "end\n",
    "\n",
    "function forward!(node::LeakyReLUNode, x)\n",
    "    node.x = x\n",
    "    node.output = ifelse.(x .> 0, x, node.alpha .* x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602fd274-f243-48fd-874c-9567ce11b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 8 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(node::Conv2DNode, dy, x)\n",
    "    W = node.W\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width, num_channels, num_filters = size(W)\n",
    "\n",
    "    dx = zeros(size(x))\n",
    "    dW = zeros(size(W))\n",
    "    db = zeros(num_filters)\n",
    "\n",
    "    for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:input_width - filter_width + 1, i in 1:input_height - filter_height + 1\n",
    "                    dx[n, i:i + filter_height - 1, j:j + filter_width - 1, :] .+= W[:, :, :, f] .* dy[n, i, j, f]\n",
    "                    dW[:, :, :, f] .+= view(x, n, i:i + filter_height - 1, j:j + filter_width - 1, :) .* dy[n, i, j, f]\n",
    "            end\n",
    "            db[f] += sum(dy[n, :, :, f])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::MaxPoolNode, dy, x)\n",
    "    pool_size = node.pool_size\n",
    "    new_size = size(node.output)\n",
    "    dy = reshape(dy, new_size)\n",
    "    \n",
    "    #println(\"back_max_pool \",size(dy),size(x), pool_size)\n",
    "    \n",
    "    batch_size, height, width, channels = size(x)\n",
    "    dx = zeros(size(x))\n",
    "    for b in 1:batch_size\n",
    "        for c in 1:channels\n",
    "            for j in 1:pool_size[2]:width-pool_size[2]+1, i in 1:pool_size[1]:height-pool_size[1]+1\n",
    "                    window = view(x, b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c)\n",
    "                    dx[b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c] .+= dy[b, i ÷ pool_size[1]+1, j ÷ pool_size[2]+1, c] .* (window .== maximum(window))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::DenseNode, dy, x)\n",
    "    W = node.W\n",
    "    #println(\"back_dense \",size(dy),size(x), size(W))\n",
    "    dx, dW, db = dy * W', x' * dy, reshape(sum(dy, dims=1), :, 1)\n",
    "    #println(\"back_dense123 \",size(dx),size(dW), size(db))\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::ReLUNode, dy, x)\n",
    "    #println(\"back_relu \", size(dy), size(x))\n",
    "    dx = dy .* (x .> 0)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::FlattenNode, dy, x)\n",
    "    input_shape = node.input_shape\n",
    "    dx = reshape(dy, size(x, 1), input_shape...)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::SigmoidNode, dy, x)\n",
    "    dx = dy .* (node.output .* (1.0 .- node.output))\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::TanhNode, dy, x)\n",
    "    dx = dy .* (1.0 .- node.output .^ 2)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::LeakyReLUNode, dy, x)\n",
    "    dx = dy .* ifelse.(x .> 0, 1, node.alpha)\n",
    "    return dx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d202a415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hasproperty (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Conv2DNode(; batch_size::Int, filters::Int, kernel_size::Tuple{Int, Int}, input_shape::Tuple{Int, Int, Int}, activation::String)\n",
    "    W = randn(kernel_size[1], kernel_size[2], input_shape[3], filters) * sqrt(1 / (kernel_size[1] * kernel_size[2] * input_shape[3]))\n",
    "    b = zeros(filters)\n",
    "\n",
    "    return Conv2DNode(zeros((input_shape..., batch_size)), W, b, batch_size, kernel_size, filters, zeros((input_shape..., size(W)[4])), zeros((1, 1, 1, 1)), zeros(1), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function MaxPoolNode(; pool_size::Tuple{Int, Int})\n",
    "    return MaxPoolNode(zeros((1, 1, 1, 1)), pool_size, zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function DenseNode(; neurons::Int, activation::String, input_shape::Union{Tuple{Int, Int}, Tuple{Int, Int, Int}})\n",
    "    W = randn(prod(input_shape), neurons) * sqrt(1 / (prod(input_shape)))\n",
    "    b = zeros(neurons)\n",
    "\n",
    "    return DenseNode(zeros((input_shape..., 1, 1)), W, b, neurons, zeros((1,1)), zeros((1, 1)), zeros((1, 1)), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function FlattenNode(; input_shape::Tuple{Int, Int, Int})\n",
    "    return FlattenNode(zeros((input_shape..., 1)), zeros((1, 1)), input_shape)\n",
    "end\n",
    "\n",
    "function ReLUNode()\n",
    "    return ReLUNode(zeros((1, 1, 1, 1)), zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function SoftmaxNode()\n",
    "    return SoftmaxNode(zeros((1, 1)), zeros((1, 1)))\n",
    "end\n",
    "\n",
    "function TanhNode()\n",
    "    return TanhNode(zeros((1, 1)), zeros((1, 1)))\n",
    "end\n",
    "\n",
    "function SigmoidNode()\n",
    "    return SigmoidNode(zeros((1, 1)), zeros((1, 1)))\n",
    "end\n",
    "\n",
    "function LeakyReLUNode(alpha::Float64)\n",
    "    return LeakyReLUNode(zeros((1, 1)), zeros((1, 1)), alpha)\n",
    "end\n",
    "\n",
    "\n",
    "function forward_pass!(graph, x)\n",
    "    input = x\n",
    "    for node in graph\n",
    "        forward!(node, input)\n",
    "        input = node.output\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function backward_pass!(graph, y_true)\n",
    "    preds = graph[end].output\n",
    "    dout  = softmax_cross_entropy_backward(preds, y_true)\n",
    "    \n",
    "    for i in reverse(1:length(graph)-1)\n",
    "        layer = graph[i]\n",
    "        prev_layer_output = i > 1 ? graph[i - 1].output : layer.x\n",
    "        if hasproperty(layer, :gradient_W)\n",
    "            dout,dW,db = backward!(layer, dout, prev_layer_output)\n",
    "            layer.gradient_W = dW\n",
    "            layer.gradient_b = db\n",
    "\n",
    "        else \n",
    "            dout = backward!(layer, dout, prev_layer_output)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function onehotbatch(labels, classes)\n",
    "    Y = zeros(Int, length(labels), length(classes))\n",
    "    for (i, label) in enumerate(labels)\n",
    "        Y[i, label .== classes] .= 1\n",
    "    end\n",
    "    return Y\n",
    "end\n",
    "\n",
    "function graph_build(layers)\n",
    "    output = []\n",
    "    for layer in layers\n",
    "        push!(output, layer)\n",
    "        if hasproperty(layer, :activation)\n",
    "            push!(output, getActivation(layer.activation))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return output\n",
    "end\n",
    "\n",
    "function getActivation(name)\n",
    "    if name == \"relu\"\n",
    "        return ReLUNode()\n",
    "    elseif name == \"softmax\"\n",
    "        return SoftmaxNode()\n",
    "    elseif name == \"sigmoid\"\n",
    "        return SigmoidNode()\n",
    "    elseif name == \"tanh\"\n",
    "        return TanhNode()\n",
    "    elseif name == \"leakyrelu\"\n",
    "        return LeakyReLUNode(0.01)\n",
    "    else \n",
    "        error(\"no such activation function\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function softmax_cross_entropy_backward(y_pred, y_true)\n",
    "    batch_size = size(y_pred, 1)\n",
    "    dloss = (y_pred .- onehotbatch(y_true, 0:9)) ./ size(y_true, 1)\n",
    "    return dloss\n",
    "end\n",
    "\n",
    "import Base.hasproperty\n",
    "hasproperty(x, s::Symbol) = s in fieldnames(typeof(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433dfab2-7d04-4fef-a952-6546082e4fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_adam!(layer, learning_rate, t, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
    "    layer.m_W = beta1 * layer.m_W + (1 - beta1) * layer.gradient_W\n",
    "    layer.v_W = beta2 * layer.v_W + (1 - beta2) * layer.gradient_W .^ 2\n",
    "    mhat_W = layer.m_W / (1 - beta1 ^ t)\n",
    "    vhat_W = layer.v_W / (1 - beta2 ^ t)\n",
    "    layer.W .-= learning_rate .* mhat_W ./ (sqrt.(vhat_W) .+ epsilon)\n",
    "\n",
    "\n",
    "    layer.m_b = beta1 * layer.m_b + (1 - beta1) * layer.gradient_b\n",
    "    layer.v_b = beta2 * layer.v_b + (1 - beta2) * layer.gradient_b .^ 2\n",
    "    mhat_b = layer.m_b / (1 - beta1 ^ t)\n",
    "    vhat_b = layer.v_b / (1 - beta2 ^ t)\n",
    "    layer.b .-= learning_rate .* mhat_b ./ (sqrt.(vhat_b) .+ epsilon)\n",
    "end\n",
    "\n",
    "function update_sgd!(layer, learning_rate)\n",
    "    layer.W .-= learning_rate .* layer.gradient_W\n",
    "    layer.b .-= learning_rate .* layer.gradient_b\n",
    "end\n",
    "\n",
    "function train!(graph, train_x, train_y, epochs=5, batch_size=32, learning_rate=1e-3, optimizer=\"adam\")\n",
    "    total_batches = ceil(Int, size(train_x, 1) / batch_size)\n",
    "    each = ceil(Int, total_batches/10)\n",
    " \n",
    "    loss_history = zeros(ceil(Int, total_batches*epochs/each))\n",
    "    counter = 1\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        println(\"----EPOCH $epoch----\")\n",
    "\n",
    "        idx = randperm(size(train_x, 1))\n",
    "        train_x = train_x[idx, :, :, :]\n",
    "        train_y = train_y[idx]\n",
    "        loss = 0\n",
    "        current_epoch_loss = zeros(total_batches)\n",
    "        \n",
    "\n",
    "        for i in 1:batch_size:size(train_x, 1)\n",
    "            x_batch = train_x[i:min(i+batch_size-1, end), :, :, :]\n",
    "            y_batch = train_y[i:min(i+batch_size-1, end)]\n",
    "            tmp  = (i ÷ batch_size)\n",
    "\n",
    "            forward_pass!(graph, x_batch)\n",
    "            \n",
    "            loss = -sum(log.(graph[end].output) .* onehotbatch(y_batch, 0:9))#/size(y_batch, 1)\n",
    "            current_epoch_loss[tmp+1] = loss\n",
    "            \n",
    "            backward_pass!(graph, y_batch)\n",
    "\n",
    "            for layer in graph\n",
    "                if hasproperty(layer, :gradient_W)\n",
    "                    if optimizer == \"adam\"\n",
    "                        update_adam!(layer, learning_rate, i)\n",
    "                    elseif optimizer == \"sgd\"\n",
    "                        update_sgd!(layer, learning_rate)\n",
    "                    else\n",
    "                        error(\"Invalid optimizer selected.\")\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            \n",
    "            if i == 1  ||  tmp % each == 0\n",
    "                avg_loss = sum(current_epoch_loss)/(tmp+1)\n",
    "\n",
    "                loss_history[counter] = avg_loss\n",
    "                counter += 1\n",
    "                print(i ÷ batch_size, \"/\", total_batches)\n",
    "                println(\" avg epoch loss: \", round(avg_loss, digits=5))\n",
    "                \n",
    "            end\n",
    "        end\n",
    "        println()\n",
    "        \n",
    "    end\n",
    "    println(\"Training finished!\")\n",
    "\n",
    "    return loss_history\n",
    "end\n",
    "\n",
    "function evaluate(graph, x_data, y_data, batch_size, pool_size)\n",
    "    num_correct = 0\n",
    "    num_samples = size(x_data, 1)\n",
    "\n",
    "    for i in 1:batch_size:num_samples\n",
    "        x_batch = x_data[i:min(i+batch_size-1, end), :, :, :]\n",
    "        y_batch = y_data[i:min(i+batch_size-1, end)]\n",
    "        \n",
    "        forward_pass!(graph, x_batch)\n",
    "        pred = graph[end].output\n",
    "        \n",
    "        predictions = argmax.(eachrow(pred)) .- 1\n",
    "        num_correct += sum(predictions .== y_batch)\n",
    "    end\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "    return accuracy\n",
    "end\n",
    "\n",
    "function plot_loss(loss::Vector{Float64}, epochs::Int)\n",
    "    plot(1:size(loss,1), loss, label=\"loss_batches\")\n",
    "\n",
    "    each = Int64(size(loss,1)/epochs)\n",
    "    pts = loss[each:each:end]\n",
    "    plot!(each:each:each*size(pts,1), pts, seriestype=:scatter, label=\"epochs\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5e96f3-f2cb-47d1-a14d-8c9d84c3bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "mnist = pyimport(\"mnist\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_data = mnist.train_images()\n",
    "train_labels = Int64.(mnist.train_labels())\n",
    "train_x =  reshape(train_data, :, 28, 28, 1) / 255.0\n",
    "train_y = train_labels\n",
    "\n",
    "\n",
    "n = 1000\n",
    "idx = randperm(n)\n",
    "train_x = train_x[idx, :,:,:]\n",
    "train_y = train_y[idx]\n",
    "\n",
    "train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b287c46-20e9-435c-95e2-23d699c4eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "0/63 avg epoch loss: 39.68099\n",
      "7/63 avg epoch loss: 36.05574\n",
      "14/63 avg epoch loss: 33.90774\n",
      "21/63 avg epoch loss: 30.69501\n",
      "28/63 avg epoch loss: 29.18815\n",
      "35/63 avg epoch loss: 26.08478\n",
      "42/63 avg epoch loss: 24.10439\n",
      "49/63 avg epoch loss: 22.69327\n",
      "56/63 avg epoch loss: 21.87251\n",
      "\n",
      "----EPOCH 2----\n",
      "0/63 avg epoch loss: 16.5116\n",
      "7/63 avg epoch loss: 10.4552\n",
      "14/63 avg epoch loss: 9.57447\n",
      "21/63 avg epoch loss: 9.50623\n",
      "28/63 avg epoch loss: 9.76479\n",
      "35/63 avg epoch loss: 9.65156\n",
      "42/63 avg epoch loss: 9.57589\n",
      "49/63 avg epoch loss: 9.72558\n",
      "56/63 avg epoch loss: 9.40044\n",
      "\n",
      "----EPOCH 3----\n",
      "0/63 avg epoch loss: 4.47811\n",
      "7/63 avg epoch loss: 7.0741\n",
      "14/63 avg epoch loss: 5.96594\n",
      "21/63 avg epoch loss: 6.15974\n",
      "28/63 avg epoch loss: 6.3407\n",
      "35/63 avg epoch loss: 6.33522\n",
      "42/63 avg epoch loss: 6.30013\n",
      "49/63 avg epoch loss: 6.24522\n",
      "56/63 avg epoch loss: 6.11284\n",
      "\n",
      "Training finished!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip780\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip781\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M141.853 1486.45 L2352.76 1486.45 L2352.76 47.2441 L141.853 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip782\">\n",
       "    <rect x=\"141\" y=\"47\" width=\"2212\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"525.311,1486.45 525.311,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"926.419,1486.45 926.419,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1327.53,1486.45 1327.53,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1728.63,1486.45 1728.63,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2129.74,1486.45 2129.74,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"141.853,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"525.311,1486.45 525.311,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"926.419,1486.45 926.419,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1327.53,1486.45 1327.53,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1728.63,1486.45 1728.63,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2129.74,1486.45 2129.74,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M515.589 1514.29 L533.946 1514.29 L533.946 1518.22 L519.872 1518.22 L519.872 1526.7 Q520.89 1526.35 521.909 1526.19 Q522.927 1526 523.946 1526 Q529.733 1526 533.112 1529.17 Q536.492 1532.34 536.492 1537.76 Q536.492 1543.34 533.02 1546.44 Q529.548 1549.52 523.228 1549.52 Q521.052 1549.52 518.784 1549.15 Q516.538 1548.78 514.131 1548.04 L514.131 1543.34 Q516.214 1544.47 518.436 1545.03 Q520.659 1545.58 523.136 1545.58 Q527.14 1545.58 529.478 1543.48 Q531.816 1541.37 531.816 1537.76 Q531.816 1534.15 529.478 1532.04 Q527.14 1529.94 523.136 1529.94 Q521.261 1529.94 519.386 1530.35 Q517.534 1530.77 515.589 1531.65 L515.589 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M901.106 1544.91 L908.745 1544.91 L908.745 1518.55 L900.435 1520.21 L900.435 1515.95 L908.699 1514.29 L913.375 1514.29 L913.375 1544.91 L921.014 1544.91 L921.014 1548.85 L901.106 1548.85 L901.106 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M940.458 1517.37 Q936.847 1517.37 935.018 1520.93 Q933.213 1524.47 933.213 1531.6 Q933.213 1538.71 935.018 1542.27 Q936.847 1545.82 940.458 1545.82 Q944.092 1545.82 945.898 1542.27 Q947.726 1538.71 947.726 1531.6 Q947.726 1524.47 945.898 1520.93 Q944.092 1517.37 940.458 1517.37 M940.458 1513.66 Q946.268 1513.66 949.324 1518.27 Q952.402 1522.85 952.402 1531.6 Q952.402 1540.33 949.324 1544.94 Q946.268 1549.52 940.458 1549.52 Q934.648 1549.52 931.569 1544.94 Q928.514 1540.33 928.514 1531.6 Q928.514 1522.85 931.569 1518.27 Q934.648 1513.66 940.458 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1302.71 1544.91 L1310.35 1544.91 L1310.35 1518.55 L1302.04 1520.21 L1302.04 1515.95 L1310.3 1514.29 L1314.98 1514.29 L1314.98 1544.91 L1322.62 1544.91 L1322.62 1548.85 L1302.71 1548.85 L1302.71 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1332.11 1514.29 L1350.47 1514.29 L1350.47 1518.22 L1336.39 1518.22 L1336.39 1526.7 Q1337.41 1526.35 1338.43 1526.19 Q1339.45 1526 1340.47 1526 Q1346.25 1526 1349.63 1529.17 Q1353.01 1532.34 1353.01 1537.76 Q1353.01 1543.34 1349.54 1546.44 Q1346.07 1549.52 1339.75 1549.52 Q1337.57 1549.52 1335.3 1549.15 Q1333.06 1548.78 1330.65 1548.04 L1330.65 1543.34 Q1332.73 1544.47 1334.96 1545.03 Q1337.18 1545.58 1339.66 1545.58 Q1343.66 1545.58 1346 1543.48 Q1348.34 1541.37 1348.34 1537.76 Q1348.34 1534.15 1346 1532.04 Q1343.66 1529.94 1339.66 1529.94 Q1337.78 1529.94 1335.91 1530.35 Q1334.05 1530.77 1332.11 1531.65 L1332.11 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1707.41 1544.91 L1723.73 1544.91 L1723.73 1548.85 L1701.78 1548.85 L1701.78 1544.91 Q1704.44 1542.16 1709.03 1537.53 Q1713.63 1532.88 1714.81 1531.53 Q1717.06 1529.01 1717.94 1527.27 Q1718.84 1525.51 1718.84 1523.82 Q1718.84 1521.07 1716.9 1519.33 Q1714.98 1517.6 1711.87 1517.6 Q1709.67 1517.6 1707.22 1518.36 Q1704.79 1519.13 1702.01 1520.68 L1702.01 1515.95 Q1704.84 1514.82 1707.29 1514.24 Q1709.74 1513.66 1711.78 1513.66 Q1717.15 1513.66 1720.35 1516.35 Q1723.54 1519.03 1723.54 1523.52 Q1723.54 1525.65 1722.73 1527.57 Q1721.94 1529.47 1719.84 1532.07 Q1719.26 1532.74 1716.16 1535.95 Q1713.05 1539.15 1707.41 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1743.54 1517.37 Q1739.93 1517.37 1738.1 1520.93 Q1736.3 1524.47 1736.3 1531.6 Q1736.3 1538.71 1738.1 1542.27 Q1739.93 1545.82 1743.54 1545.82 Q1747.17 1545.82 1748.98 1542.27 Q1750.81 1538.71 1750.81 1531.6 Q1750.81 1524.47 1748.98 1520.93 Q1747.17 1517.37 1743.54 1517.37 M1743.54 1513.66 Q1749.35 1513.66 1752.41 1518.27 Q1755.48 1522.85 1755.48 1531.6 Q1755.48 1540.33 1752.41 1544.94 Q1749.35 1549.52 1743.54 1549.52 Q1737.73 1549.52 1734.65 1544.94 Q1731.6 1540.33 1731.6 1531.6 Q1731.6 1522.85 1734.65 1518.27 Q1737.73 1513.66 1743.54 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2109.01 1544.91 L2125.33 1544.91 L2125.33 1548.85 L2103.39 1548.85 L2103.39 1544.91 Q2106.05 1542.16 2110.63 1537.53 Q2115.24 1532.88 2116.42 1531.53 Q2118.66 1529.01 2119.54 1527.27 Q2120.45 1525.51 2120.45 1523.82 Q2120.45 1521.07 2118.5 1519.33 Q2116.58 1517.6 2113.48 1517.6 Q2111.28 1517.6 2108.83 1518.36 Q2106.4 1519.13 2103.62 1520.68 L2103.62 1515.95 Q2106.44 1514.82 2108.9 1514.24 Q2111.35 1513.66 2113.39 1513.66 Q2118.76 1513.66 2121.95 1516.35 Q2125.15 1519.03 2125.15 1523.52 Q2125.15 1525.65 2124.34 1527.57 Q2123.55 1529.47 2121.44 1532.07 Q2120.86 1532.74 2117.76 1535.95 Q2114.66 1539.15 2109.01 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2135.19 1514.29 L2153.55 1514.29 L2153.55 1518.22 L2139.47 1518.22 L2139.47 1526.7 Q2140.49 1526.35 2141.51 1526.19 Q2142.53 1526 2143.55 1526 Q2149.34 1526 2152.71 1529.17 Q2156.09 1532.34 2156.09 1537.76 Q2156.09 1543.34 2152.62 1546.44 Q2149.15 1549.52 2142.83 1549.52 Q2140.65 1549.52 2138.39 1549.15 Q2136.14 1548.78 2133.73 1548.04 L2133.73 1543.34 Q2135.82 1544.47 2138.04 1545.03 Q2140.26 1545.58 2142.74 1545.58 Q2146.74 1545.58 2149.08 1543.48 Q2151.42 1541.37 2151.42 1537.76 Q2151.42 1534.15 2149.08 1532.04 Q2146.74 1529.94 2142.74 1529.94 Q2140.86 1529.94 2138.99 1530.35 Q2137.14 1530.77 2135.19 1531.65 L2135.19 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"141.853,1232.74 2352.76,1232.74 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"141.853,847.052 2352.76,847.052 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"141.853,461.362 2352.76,461.362 \"/>\n",
       "<polyline clip-path=\"url(#clip782)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"141.853,75.6723 2352.76,75.6723 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"141.853,1486.45 141.853,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"141.853,1232.74 160.751,1232.74 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"141.853,847.052 160.751,847.052 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"141.853,461.362 160.751,461.362 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"141.853,75.6723 160.751,75.6723 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M54.5569 1246.09 L62.1958 1246.09 L62.1958 1219.72 L53.8856 1221.39 L53.8856 1217.13 L62.1495 1215.46 L66.8254 1215.46 L66.8254 1246.09 L74.4642 1246.09 L74.4642 1250.02 L54.5569 1250.02 L54.5569 1246.09 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M93.9086 1218.54 Q90.2975 1218.54 88.4688 1222.11 Q86.6632 1225.65 86.6632 1232.78 Q86.6632 1239.88 88.4688 1243.45 Q90.2975 1246.99 93.9086 1246.99 Q97.5428 1246.99 99.3483 1243.45 Q101.177 1239.88 101.177 1232.78 Q101.177 1225.65 99.3483 1222.11 Q97.5428 1218.54 93.9086 1218.54 M93.9086 1214.84 Q99.7187 1214.84 102.774 1219.44 Q105.853 1224.03 105.853 1232.78 Q105.853 1241.5 102.774 1246.11 Q99.7187 1250.69 93.9086 1250.69 Q88.0984 1250.69 85.0197 1246.11 Q81.9642 1241.5 81.9642 1232.78 Q81.9642 1224.03 85.0197 1219.44 Q88.0984 1214.84 93.9086 1214.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M57.7745 860.397 L74.0939 860.397 L74.0939 864.332 L52.1495 864.332 L52.1495 860.397 Q54.8115 857.642 59.3949 853.013 Q64.0013 848.36 65.1819 847.017 Q67.4272 844.494 68.3068 842.758 Q69.2096 840.999 69.2096 839.309 Q69.2096 836.554 67.2652 834.818 Q65.3439 833.082 62.2421 833.082 Q60.043 833.082 57.5893 833.846 Q55.1588 834.61 52.381 836.161 L52.381 831.439 Q55.2051 830.304 57.6588 829.726 Q60.1124 829.147 62.1495 829.147 Q67.5198 829.147 70.7142 831.832 Q73.9087 834.517 73.9087 839.008 Q73.9087 841.138 73.0985 843.059 Q72.3115 844.957 70.205 847.55 Q69.6263 848.221 66.5245 851.439 Q63.4226 854.633 57.7745 860.397 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M93.9086 832.851 Q90.2975 832.851 88.4688 836.416 Q86.6632 839.957 86.6632 847.087 Q86.6632 854.193 88.4688 857.758 Q90.2975 861.3 93.9086 861.3 Q97.5428 861.3 99.3483 857.758 Q101.177 854.193 101.177 847.087 Q101.177 839.957 99.3483 836.416 Q97.5428 832.851 93.9086 832.851 M93.9086 829.147 Q99.7187 829.147 102.774 833.754 Q105.853 838.337 105.853 847.087 Q105.853 855.814 102.774 860.42 Q99.7187 865.003 93.9086 865.003 Q88.0984 865.003 85.0197 860.42 Q81.9642 855.814 81.9642 847.087 Q81.9642 838.337 85.0197 833.754 Q88.0984 829.147 93.9086 829.147 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M67.9133 460.008 Q71.2698 460.726 73.1448 462.994 Q75.0429 465.263 75.0429 468.596 Q75.0429 473.712 71.5244 476.513 Q68.0059 479.313 61.5245 479.313 Q59.3486 479.313 57.0338 478.874 Q54.7421 478.457 52.2884 477.601 L52.2884 473.087 Q54.2328 474.221 56.5477 474.8 Q58.8625 475.378 61.3856 475.378 Q65.7837 475.378 68.0754 473.642 Q70.3902 471.906 70.3902 468.596 Q70.3902 465.54 68.2374 463.827 Q66.1078 462.091 62.2884 462.091 L58.2606 462.091 L58.2606 458.249 L62.4735 458.249 Q65.9226 458.249 67.7513 456.883 Q69.58 455.494 69.58 452.902 Q69.58 450.24 67.6819 448.828 Q65.8069 447.392 62.2884 447.392 Q60.3671 447.392 58.168 447.809 Q55.969 448.226 53.3301 449.105 L53.3301 444.939 Q55.9921 444.198 58.3069 443.828 Q60.6449 443.457 62.705 443.457 Q68.0291 443.457 71.1309 445.888 Q74.2327 448.295 74.2327 452.415 Q74.2327 455.286 72.5892 457.277 Q70.9457 459.244 67.9133 460.008 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M93.9086 447.161 Q90.2975 447.161 88.4688 450.726 Q86.6632 454.267 86.6632 461.397 Q86.6632 468.503 88.4688 472.068 Q90.2975 475.61 93.9086 475.61 Q97.5428 475.61 99.3483 472.068 Q101.177 468.503 101.177 461.397 Q101.177 454.267 99.3483 450.726 Q97.5428 447.161 93.9086 447.161 M93.9086 443.457 Q99.7187 443.457 102.774 448.064 Q105.853 452.647 105.853 461.397 Q105.853 470.124 102.774 474.73 Q99.7187 479.313 93.9086 479.313 Q88.0984 479.313 85.0197 474.73 Q81.9642 470.124 81.9642 461.397 Q81.9642 452.647 85.0197 448.064 Q88.0984 443.457 93.9086 443.457 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M66.5939 62.4663 L54.7884 80.9153 L66.5939 80.9153 L66.5939 62.4663 M65.367 58.3923 L71.2466 58.3923 L71.2466 80.9153 L76.1772 80.9153 L76.1772 84.8042 L71.2466 84.8042 L71.2466 92.9523 L66.5939 92.9523 L66.5939 84.8042 L50.9921 84.8042 L50.9921 80.2903 L65.367 58.3923 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M93.9086 61.471 Q90.2975 61.471 88.4688 65.0358 Q86.6632 68.5774 86.6632 75.707 Q86.6632 82.8135 88.4688 86.3783 Q90.2975 89.9199 93.9086 89.9199 Q97.5428 89.9199 99.3483 86.3783 Q101.177 82.8135 101.177 75.707 Q101.177 68.5774 99.3483 65.0358 Q97.5428 61.471 93.9086 61.471 M93.9086 57.7673 Q99.7187 57.7673 102.774 62.3738 Q105.853 66.9571 105.853 75.707 Q105.853 84.4338 102.774 89.0403 Q99.7187 93.6236 93.9086 93.6236 Q88.0984 93.6236 85.0197 89.0403 Q81.9642 84.4338 81.9642 75.707 Q81.9642 66.9571 85.0197 62.3738 Q88.0984 57.7673 93.9086 57.7673 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip782)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"204.426,87.9763 284.647,227.799 364.869,310.645 445.09,434.556 525.311,492.674 605.533,612.368 685.754,688.75 765.976,743.175 846.197,774.831 926.419,981.596 1006.64,1215.19 1086.86,1249.15 1167.08,1251.79 1247.3,1241.81 1327.53,1246.18 1407.75,1249.1 1487.97,1243.33 1568.19,1255.87 1648.41,1445.72 1728.63,1345.59 1808.85,1388.33 1889.08,1380.86 1969.3,1373.88 2049.52,1374.09 2129.74,1375.44 2209.96,1377.56 2290.18,1382.67 \"/>\n",
       "<circle clip-path=\"url(#clip782)\" cx=\"846.197\" cy=\"774.831\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip782)\" cx=\"1568.19\" cy=\"1255.87\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip782)\" cx=\"2290.18\" cy=\"1382.67\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M1761.77 250.738 L2279.06 250.738 L2279.06 95.2176 L1761.77 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1761.77,250.738 2279.06,250.738 2279.06,95.2176 1761.77,95.2176 1761.77,250.738 \"/>\n",
       "<polyline clip-path=\"url(#clip780)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1786.33,147.058 1933.73,147.058 \"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M1958.29 128.319 L1962.55 128.319 L1962.55 164.338 L1958.29 164.338 L1958.29 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1981.51 141.398 Q1978.08 141.398 1976.09 144.083 Q1974.1 146.745 1974.1 151.398 Q1974.1 156.051 1976.07 158.736 Q1978.06 161.398 1981.51 161.398 Q1984.91 161.398 1986.9 158.713 Q1988.89 156.027 1988.89 151.398 Q1988.89 146.791 1986.9 144.106 Q1984.91 141.398 1981.51 141.398 M1981.51 137.787 Q1987.06 137.787 1990.24 141.398 Q1993.41 145.009 1993.41 151.398 Q1993.41 157.764 1990.24 161.398 Q1987.06 165.009 1981.51 165.009 Q1975.93 165.009 1972.76 161.398 Q1969.61 157.764 1969.61 151.398 Q1969.61 145.009 1972.76 141.398 Q1975.93 137.787 1981.51 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2016.99 139.176 L2016.99 143.203 Q2015.19 142.277 2013.24 141.815 Q2011.3 141.352 2009.22 141.352 Q2006.05 141.352 2004.45 142.324 Q2002.87 143.296 2002.87 145.24 Q2002.87 146.722 2004.01 147.578 Q2005.14 148.412 2008.57 149.176 L2010.03 149.5 Q2014.56 150.472 2016.46 152.254 Q2018.38 154.014 2018.38 157.185 Q2018.38 160.796 2015.51 162.902 Q2012.67 165.009 2007.67 165.009 Q2005.58 165.009 2003.31 164.592 Q2001.07 164.199 1998.57 163.388 L1998.57 158.99 Q2000.93 160.217 2003.22 160.842 Q2005.51 161.444 2007.76 161.444 Q2010.77 161.444 2012.39 160.426 Q2014.01 159.384 2014.01 157.509 Q2014.01 155.773 2012.83 154.847 Q2011.67 153.921 2007.71 153.064 L2006.23 152.717 Q2002.27 151.884 2000.51 150.171 Q1998.75 148.435 1998.75 145.426 Q1998.75 141.768 2001.35 139.778 Q2003.94 137.787 2008.71 137.787 Q2011.07 137.787 2013.15 138.134 Q2015.24 138.481 2016.99 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2041.69 139.176 L2041.69 143.203 Q2039.89 142.277 2037.94 141.815 Q2036 141.352 2033.92 141.352 Q2030.74 141.352 2029.15 142.324 Q2027.57 143.296 2027.57 145.24 Q2027.57 146.722 2028.71 147.578 Q2029.84 148.412 2033.27 149.176 L2034.73 149.5 Q2039.26 150.472 2041.16 152.254 Q2043.08 154.014 2043.08 157.185 Q2043.08 160.796 2040.21 162.902 Q2037.37 165.009 2032.37 165.009 Q2030.28 165.009 2028.01 164.592 Q2025.77 164.199 2023.27 163.388 L2023.27 158.99 Q2025.63 160.217 2027.92 160.842 Q2030.21 161.444 2032.46 161.444 Q2035.47 161.444 2037.09 160.426 Q2038.71 159.384 2038.71 157.509 Q2038.71 155.773 2037.53 154.847 Q2036.37 153.921 2032.41 153.064 L2030.93 152.717 Q2026.97 151.884 2025.21 150.171 Q2023.45 148.435 2023.45 145.426 Q2023.45 141.768 2026.05 139.778 Q2028.64 137.787 2033.41 137.787 Q2035.77 137.787 2037.85 138.134 Q2039.93 138.481 2041.69 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2069.56 172.208 L2069.56 175.518 L2044.93 175.518 L2044.93 172.208 L2069.56 172.208 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2092.18 151.398 Q2092.18 146.699 2090.24 144.037 Q2088.31 141.352 2084.93 141.352 Q2081.55 141.352 2079.61 144.037 Q2077.69 146.699 2077.69 151.398 Q2077.69 156.097 2079.61 158.782 Q2081.55 161.444 2084.93 161.444 Q2088.31 161.444 2090.24 158.782 Q2092.18 156.097 2092.18 151.398 M2077.69 142.347 Q2079.03 140.032 2081.07 138.921 Q2083.13 137.787 2085.98 137.787 Q2090.7 137.787 2093.64 141.537 Q2096.6 145.287 2096.6 151.398 Q2096.6 157.509 2093.64 161.259 Q2090.7 165.009 2085.98 165.009 Q2083.13 165.009 2081.07 163.898 Q2079.03 162.763 2077.69 160.449 L2077.69 164.338 L2073.41 164.338 L2073.41 128.319 L2077.69 128.319 L2077.69 142.347 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2115.44 151.305 Q2110.28 151.305 2108.29 152.486 Q2106.3 153.666 2106.3 156.514 Q2106.3 158.782 2107.78 160.125 Q2109.29 161.444 2111.86 161.444 Q2115.4 161.444 2117.53 158.944 Q2119.68 156.421 2119.68 152.254 L2119.68 151.305 L2115.44 151.305 M2123.94 149.546 L2123.94 164.338 L2119.68 164.338 L2119.68 160.402 Q2118.22 162.763 2116.05 163.898 Q2113.87 165.009 2110.72 165.009 Q2106.74 165.009 2104.38 162.787 Q2102.04 160.541 2102.04 156.791 Q2102.04 152.416 2104.96 150.194 Q2107.9 147.972 2113.71 147.972 L2119.68 147.972 L2119.68 147.555 Q2119.68 144.615 2117.74 143.018 Q2115.81 141.398 2112.32 141.398 Q2110.1 141.398 2107.99 141.93 Q2105.88 142.463 2103.94 143.527 L2103.94 139.592 Q2106.28 138.69 2108.48 138.25 Q2110.67 137.787 2112.76 137.787 Q2118.38 137.787 2121.16 140.703 Q2123.94 143.62 2123.94 149.546 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2136.92 131.051 L2136.92 138.412 L2145.7 138.412 L2145.7 141.722 L2136.92 141.722 L2136.92 155.796 Q2136.92 158.967 2137.78 159.87 Q2138.66 160.773 2141.32 160.773 L2145.7 160.773 L2145.7 164.338 L2141.32 164.338 Q2136.39 164.338 2134.52 162.509 Q2132.64 160.657 2132.64 155.796 L2132.64 141.722 L2129.52 141.722 L2129.52 138.412 L2132.64 138.412 L2132.64 131.051 L2136.92 131.051 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2169.96 139.407 L2169.96 143.389 Q2168.15 142.393 2166.32 141.907 Q2164.52 141.398 2162.67 141.398 Q2158.52 141.398 2156.23 144.037 Q2153.94 146.652 2153.94 151.398 Q2153.94 156.143 2156.23 158.782 Q2158.52 161.398 2162.67 161.398 Q2164.52 161.398 2166.32 160.912 Q2168.15 160.402 2169.96 159.407 L2169.96 163.342 Q2168.17 164.176 2166.25 164.592 Q2164.36 165.009 2162.2 165.009 Q2156.35 165.009 2152.9 161.328 Q2149.45 157.648 2149.45 151.398 Q2149.45 145.055 2152.92 141.421 Q2156.42 137.787 2162.48 137.787 Q2164.45 137.787 2166.32 138.203 Q2168.2 138.597 2169.96 139.407 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2198.92 148.689 L2198.92 164.338 L2194.66 164.338 L2194.66 148.828 Q2194.66 145.148 2193.22 143.319 Q2191.79 141.49 2188.92 141.49 Q2185.47 141.49 2183.48 143.69 Q2181.48 145.889 2181.48 149.685 L2181.48 164.338 L2177.2 164.338 L2177.2 128.319 L2181.48 128.319 L2181.48 142.44 Q2183.01 140.102 2185.07 138.944 Q2187.16 137.787 2189.86 137.787 Q2194.33 137.787 2196.62 140.565 Q2198.92 143.319 2198.92 148.689 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2229.59 150.31 L2229.59 152.393 L2210 152.393 Q2210.28 156.791 2212.64 159.106 Q2215.03 161.398 2219.26 161.398 Q2221.72 161.398 2224.01 160.796 Q2226.32 160.194 2228.59 158.99 L2228.59 163.018 Q2226.3 163.99 2223.89 164.5 Q2221.48 165.009 2219.01 165.009 Q2212.8 165.009 2209.17 161.398 Q2205.56 157.787 2205.56 151.629 Q2205.56 145.264 2208.98 141.537 Q2212.43 137.787 2218.27 137.787 Q2223.5 137.787 2226.53 141.166 Q2229.59 144.523 2229.59 150.31 M2225.33 149.06 Q2225.28 145.565 2223.36 143.481 Q2221.46 141.398 2218.31 141.398 Q2214.75 141.398 2212.6 143.412 Q2210.47 145.426 2210.14 149.083 L2225.33 149.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2253.1 139.176 L2253.1 143.203 Q2251.3 142.277 2249.35 141.815 Q2247.41 141.352 2245.33 141.352 Q2242.16 141.352 2240.56 142.324 Q2238.98 143.296 2238.98 145.24 Q2238.98 146.722 2240.12 147.578 Q2241.25 148.412 2244.68 149.176 L2246.14 149.5 Q2250.67 150.472 2252.57 152.254 Q2254.49 154.014 2254.49 157.185 Q2254.49 160.796 2251.62 162.902 Q2248.78 165.009 2243.78 165.009 Q2241.69 165.009 2239.42 164.592 Q2237.18 164.199 2234.68 163.388 L2234.68 158.99 Q2237.04 160.217 2239.33 160.842 Q2241.62 161.444 2243.87 161.444 Q2246.88 161.444 2248.5 160.426 Q2250.12 159.384 2250.12 157.509 Q2250.12 155.773 2248.94 154.847 Q2247.78 153.921 2243.82 153.064 L2242.34 152.717 Q2238.38 151.884 2236.62 150.171 Q2234.86 148.435 2234.86 145.426 Q2234.86 141.768 2237.46 139.778 Q2240.05 137.787 2244.82 137.787 Q2247.18 137.787 2249.26 138.134 Q2251.35 138.481 2253.1 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip780)\" cx=\"1860.03\" cy=\"198.898\" r=\"20.48\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"4.55111\"/>\n",
       "<path clip-path=\"url(#clip780)\" d=\"M1982.32 202.15 L1982.32 204.233 L1962.74 204.233 Q1963.01 208.631 1965.37 210.946 Q1967.76 213.238 1972 213.238 Q1974.45 213.238 1976.74 212.636 Q1979.06 212.034 1981.32 210.83 L1981.32 214.858 Q1979.03 215.83 1976.62 216.34 Q1974.22 216.849 1971.74 216.849 Q1965.54 216.849 1961.9 213.238 Q1958.29 209.627 1958.29 203.469 Q1958.29 197.104 1961.72 193.377 Q1965.17 189.627 1971 189.627 Q1976.23 189.627 1979.26 193.006 Q1982.32 196.363 1982.32 202.15 M1978.06 200.9 Q1978.01 197.405 1976.09 195.321 Q1974.19 193.238 1971.05 193.238 Q1967.48 193.238 1965.33 195.252 Q1963.2 197.266 1962.87 200.923 L1978.06 200.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M1993.43 212.289 L1993.43 226.039 L1989.15 226.039 L1989.15 190.252 L1993.43 190.252 L1993.43 194.187 Q1994.77 191.872 1996.81 190.761 Q1998.87 189.627 2001.72 189.627 Q2006.44 189.627 2009.38 193.377 Q2012.34 197.127 2012.34 203.238 Q2012.34 209.349 2009.38 213.099 Q2006.44 216.849 2001.72 216.849 Q1998.87 216.849 1996.81 215.738 Q1994.77 214.603 1993.43 212.289 M2007.92 203.238 Q2007.92 198.539 2005.98 195.877 Q2004.06 193.192 2000.68 193.192 Q1997.3 193.192 1995.35 195.877 Q1993.43 198.539 1993.43 203.238 Q1993.43 207.937 1995.35 210.622 Q1997.3 213.284 2000.68 213.284 Q2004.06 213.284 2005.98 210.622 Q2007.92 207.937 2007.92 203.238 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2029.45 193.238 Q2026.02 193.238 2024.03 195.923 Q2022.04 198.585 2022.04 203.238 Q2022.04 207.891 2024.01 210.576 Q2026 213.238 2029.45 213.238 Q2032.85 213.238 2034.84 210.553 Q2036.83 207.867 2036.83 203.238 Q2036.83 198.631 2034.84 195.946 Q2032.85 193.238 2029.45 193.238 M2029.45 189.627 Q2035 189.627 2038.18 193.238 Q2041.35 196.849 2041.35 203.238 Q2041.35 209.604 2038.18 213.238 Q2035 216.849 2029.45 216.849 Q2023.87 216.849 2020.7 213.238 Q2017.55 209.604 2017.55 203.238 Q2017.55 196.849 2020.7 193.238 Q2023.87 189.627 2029.45 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2067.06 191.247 L2067.06 195.229 Q2065.26 194.233 2063.43 193.747 Q2061.62 193.238 2059.77 193.238 Q2055.63 193.238 2053.34 195.877 Q2051.05 198.492 2051.05 203.238 Q2051.05 207.983 2053.34 210.622 Q2055.63 213.238 2059.77 213.238 Q2061.62 213.238 2063.43 212.752 Q2065.26 212.242 2067.06 211.247 L2067.06 215.182 Q2065.28 216.016 2063.36 216.432 Q2061.46 216.849 2059.31 216.849 Q2053.45 216.849 2050 213.168 Q2046.55 209.488 2046.55 203.238 Q2046.55 196.895 2050.03 193.261 Q2053.52 189.627 2059.59 189.627 Q2061.55 189.627 2063.43 190.043 Q2065.3 190.437 2067.06 191.247 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2096.02 200.529 L2096.02 216.178 L2091.76 216.178 L2091.76 200.668 Q2091.76 196.988 2090.33 195.159 Q2088.89 193.33 2086.02 193.33 Q2082.57 193.33 2080.58 195.53 Q2078.59 197.729 2078.59 201.525 L2078.59 216.178 L2074.31 216.178 L2074.31 180.159 L2078.59 180.159 L2078.59 194.28 Q2080.12 191.942 2082.18 190.784 Q2084.26 189.627 2086.97 189.627 Q2091.44 189.627 2093.73 192.405 Q2096.02 195.159 2096.02 200.529 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip780)\" d=\"M2121.05 191.016 L2121.05 195.043 Q2119.24 194.117 2117.3 193.655 Q2115.35 193.192 2113.27 193.192 Q2110.1 193.192 2108.5 194.164 Q2106.92 195.136 2106.92 197.08 Q2106.92 198.562 2108.06 199.418 Q2109.19 200.252 2112.62 201.016 L2114.08 201.34 Q2118.61 202.312 2120.51 204.094 Q2122.43 205.854 2122.43 209.025 Q2122.43 212.636 2119.56 214.742 Q2116.72 216.849 2111.72 216.849 Q2109.63 216.849 2107.36 216.432 Q2105.12 216.039 2102.62 215.228 L2102.62 210.83 Q2104.98 212.057 2107.27 212.682 Q2109.56 213.284 2111.81 213.284 Q2114.82 213.284 2116.44 212.266 Q2118.06 211.224 2118.06 209.349 Q2118.06 207.613 2116.88 206.687 Q2115.72 205.761 2111.76 204.904 L2110.28 204.557 Q2106.32 203.724 2104.56 202.011 Q2102.8 200.275 2102.8 197.266 Q2102.8 193.608 2105.4 191.618 Q2107.99 189.627 2112.76 189.627 Q2115.12 189.627 2117.2 189.974 Q2119.29 190.321 2121.05 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "#using BenchmarkTools: @btime\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "learning_rate = 0.0075\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=batch_size, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(prod((6,6,2)), 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model)\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2304da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "test_data = mnist.test_images()\n",
    "test_labels = Int64.(mnist.test_labels())\n",
    "\n",
    "test_x =  reshape(test_data, :, 28, 28, 1) / 255.0\n",
    "test_y = test_labels\n",
    "\n",
    "n = 5000\n",
    "idx = randperm(n)\n",
    "\n",
    "test_x = test_x[idx, :,:,:]\n",
    "test_y = test_y[idx]\n",
    "\n",
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "617527d7-dea6-41da-9ecd-f68b7687482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 167.38948\n",
      "2/16 avg epoch loss: 157.14773\n",
      "4/16 avg epoch loss: 154.58339\n",
      "6/16 avg epoch loss: 149.96334\n",
      "8/16 avg epoch loss: 145.67213\n",
      "10/16 avg epoch loss: 141.41149\n",
      "12/16 avg epoch loss: 137.43503\n",
      "14/16 avg epoch loss: 132.00282\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 83.3737\n",
      "2/16 avg epoch loss: 88.08394\n",
      "4/16 avg epoch loss: 83.72199\n",
      "6/16 avg epoch loss: 79.17688\n",
      "8/16 avg epoch loss: 76.06024\n",
      "10/16 avg epoch loss: 72.83292\n",
      "12/16 avg epoch loss: 71.4785\n",
      "14/16 avg epoch loss: 68.83259\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 57.0956\n",
      "2/16 avg epoch loss: 47.6644\n",
      "4/16 avg epoch loss: 47.85129\n",
      "6/16 avg epoch loss: 47.33534\n",
      "8/16 avg epoch loss: 45.51791\n",
      "10/16 avg epoch loss: 45.64605\n",
      "12/16 avg epoch loss: 45.08582\n",
      "14/16 avg epoch loss: 45.27988\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 27.86498\n",
      "2/16 avg epoch loss: 38.62776\n",
      "4/16 avg epoch loss: 36.1864\n",
      "6/16 avg epoch loss: 37.58715\n",
      "8/16 avg epoch loss: 36.28799\n",
      "10/16 avg epoch loss: 36.43501\n",
      "12/16 avg epoch loss: 37.24301\n",
      "14/16 avg epoch loss: 36.63557\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 20.46703\n",
      "2/16 avg epoch loss: 27.48728\n",
      "4/16 avg epoch loss: 30.52242\n",
      "6/16 avg epoch loss: 31.68713\n",
      "8/16 avg epoch loss: 32.07531\n",
      "10/16 avg epoch loss: 31.14398\n",
      "12/16 avg epoch loss: 31.49701\n",
      "14/16 avg epoch loss: 30.36705\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 22.39925\n",
      "2/16 avg epoch loss: 25.91528\n",
      "4/16 avg epoch loss: 25.80805\n",
      "6/16 avg epoch loss: 24.59463\n",
      "8/16 avg epoch loss: 25.83038\n",
      "10/16 avg epoch loss: 27.37195\n",
      "12/16 avg epoch loss: 27.55129\n",
      "14/16 avg epoch loss: 26.93998\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 33.81302\n",
      "2/16 avg epoch loss: 30.16226\n",
      "4/16 avg epoch loss: 28.83967\n",
      "6/16 avg epoch loss: 27.43028\n",
      "8/16 avg epoch loss: 26.53461\n",
      "10/16 avg epoch loss: 26.8593\n",
      "12/16 avg epoch loss: 26.48246\n",
      "14/16 avg epoch loss: 24.70067\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 18.2219\n",
      "2/16 avg epoch loss: 25.28664\n",
      "4/16 avg epoch loss: 24.70386\n",
      "6/16 avg epoch loss: 23.68532\n",
      "8/16 avg epoch loss: 23.45687\n",
      "10/16 avg epoch loss: 22.32628\n",
      "12/16 avg epoch loss: 22.41571\n",
      "14/16 avg epoch loss: 23.86668\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 25.29436\n",
      "2/16 avg epoch loss: 28.61672\n",
      "4/16 avg epoch loss: 26.50178\n",
      "6/16 avg epoch loss: 26.40555\n",
      "8/16 avg epoch loss: 24.43908\n",
      "10/16 avg epoch loss: 23.38896\n",
      "12/16 avg epoch loss: 22.91452\n",
      "14/16 avg epoch loss: 21.938\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 16.44113\n",
      "2/16 avg epoch loss: 19.32211\n",
      "4/16 avg epoch loss: 17.17489\n",
      "6/16 avg epoch loss: 19.61012\n",
      "8/16 avg epoch loss: 19.4023\n",
      "10/16 avg epoch loss: 19.93821\n",
      "12/16 avg epoch loss: 20.07682\n",
      "14/16 avg epoch loss: 20.35012\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 9.07562\n",
      "2/16 avg epoch loss: 16.51972\n",
      "4/16 avg epoch loss: 16.15856\n",
      "6/16 avg epoch loss: 16.33899\n",
      "8/16 avg epoch loss: 16.98021\n",
      "10/16 avg epoch loss: 18.62855\n",
      "12/16 avg epoch loss: 19.73636\n",
      "14/16 avg epoch loss: 19.25448\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 12.54359\n",
      "2/16 avg epoch loss: 18.38028\n",
      "4/16 avg epoch loss: 15.94946\n",
      "6/16 avg epoch loss: 15.37208\n",
      "8/16 avg epoch loss: 16.37421\n",
      "10/16 avg epoch loss: 17.10477\n",
      "12/16 avg epoch loss: 17.38184\n",
      "14/16 avg epoch loss: 17.56114\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 15.40526\n",
      "2/16 avg epoch loss: 15.13069\n",
      "4/16 avg epoch loss: 16.01868\n",
      "6/16 avg epoch loss: 16.01047\n",
      "8/16 avg epoch loss: 16.67363\n",
      "10/16 avg epoch loss: 17.11436\n",
      "12/16 avg epoch loss: 17.5635\n",
      "14/16 avg epoch loss: 17.00517\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 8.35729\n",
      "2/16 avg epoch loss: 12.36693\n",
      "4/16 avg epoch loss: 15.07071\n",
      "6/16 avg epoch loss: 14.92025\n",
      "8/16 avg epoch loss: 13.62095\n",
      "10/16 avg epoch loss: 14.75883\n",
      "12/16 avg epoch loss: 15.94395\n",
      "14/16 avg epoch loss: 15.84547\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 7.13714\n",
      "2/16 avg epoch loss: 12.25954\n",
      "4/16 avg epoch loss: 12.72582\n",
      "6/16 avg epoch loss: 13.66639\n",
      "8/16 avg epoch loss: 14.46434\n",
      "10/16 avg epoch loss: 14.44068\n",
      "12/16 avg epoch loss: 15.02807\n",
      "14/16 avg epoch loss: 15.04163\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 19.47628\n",
      "2/16 avg epoch loss: 16.24525\n",
      "4/16 avg epoch loss: 14.72496\n",
      "6/16 avg epoch loss: 13.58055\n",
      "8/16 avg epoch loss: 13.22106\n",
      "10/16 avg epoch loss: 13.89072\n",
      "12/16 avg epoch loss: 14.01322\n",
      "14/16 avg epoch loss: 13.80093\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 14.4423\n",
      "2/16 avg epoch loss: 17.58838\n",
      "4/16 avg epoch loss: 14.7754\n",
      "6/16 avg epoch loss: 15.16332\n",
      "8/16 avg epoch loss: 13.65342\n",
      "10/16 avg epoch loss: 14.53168\n",
      "12/16 avg epoch loss: 14.2072\n",
      "14/16 avg epoch loss: 14.21658\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 15.85032\n",
      "2/16 avg epoch loss: 15.24203\n",
      "4/16 avg epoch loss: 15.22387\n",
      "6/16 avg epoch loss: 14.08611\n",
      "8/16 avg epoch loss: 14.05149\n",
      "10/16 avg epoch loss: 13.68343\n",
      "12/16 avg epoch loss: 13.65128\n",
      "14/16 avg epoch loss: 13.75459\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 16.73546\n",
      "2/16 avg epoch loss: 14.97564\n",
      "4/16 avg epoch loss: 11.832\n",
      "6/16 avg epoch loss: 12.30011\n",
      "8/16 avg epoch loss: 12.86684\n",
      "10/16 avg epoch loss: 12.4819\n",
      "12/16 avg epoch loss: 12.99432\n",
      "14/16 avg epoch loss: 12.71067\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 9.20554\n",
      "2/16 avg epoch loss: 15.10756\n",
      "4/16 avg epoch loss: 15.08076\n",
      "6/16 avg epoch loss: 13.9614\n",
      "8/16 avg epoch loss: 13.10588\n",
      "10/16 avg epoch loss: 12.97406\n",
      "12/16 avg epoch loss: 13.52582\n",
      "14/16 avg epoch loss: 12.86778\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 10.41224\n",
      "2/16 avg epoch loss: 9.32632\n",
      "4/16 avg epoch loss: 9.48247\n",
      "6/16 avg epoch loss: 10.30669\n",
      "8/16 avg epoch loss: 10.71973\n",
      "10/16 avg epoch loss: 11.0647\n",
      "12/16 avg epoch loss: 10.74475\n",
      "14/16 avg epoch loss: 11.02086\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 9.04524\n",
      "2/16 avg epoch loss: 13.95863\n",
      "4/16 avg epoch loss: 13.17075\n",
      "6/16 avg epoch loss: 12.71024\n",
      "8/16 avg epoch loss: 12.46713\n",
      "10/16 avg epoch loss: 11.91025\n",
      "12/16 avg epoch loss: 12.13347\n",
      "14/16 avg epoch loss: 11.80061\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 16.4279\n",
      "2/16 avg epoch loss: 10.32909\n",
      "4/16 avg epoch loss: 10.52876\n",
      "6/16 avg epoch loss: 11.10883\n",
      "8/16 avg epoch loss: 10.12104\n",
      "10/16 avg epoch loss: 10.98062\n",
      "12/16 avg epoch loss: 10.48876\n",
      "14/16 avg epoch loss: 10.94078\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 9.37317\n",
      "2/16 avg epoch loss: 6.89514\n",
      "4/16 avg epoch loss: 7.34767\n",
      "6/16 avg epoch loss: 9.56025\n",
      "8/16 avg epoch loss: 9.52565\n",
      "10/16 avg epoch loss: 10.29143\n",
      "12/16 avg epoch loss: 10.05268\n",
      "14/16 avg epoch loss: 10.41319\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 7.29473\n",
      "2/16 avg epoch loss: 11.36573\n",
      "4/16 avg epoch loss: 10.58306\n",
      "6/16 avg epoch loss: 10.12668\n",
      "8/16 avg epoch loss: 9.37947\n",
      "10/16 avg epoch loss: 9.19581\n",
      "12/16 avg epoch loss: 9.72903\n",
      "14/16 avg epoch loss: 10.04675\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 14.45401\n",
      "2/16 avg epoch loss: 11.08875\n",
      "4/16 avg epoch loss: 9.87183\n",
      "6/16 avg epoch loss: 11.39335\n",
      "8/16 avg epoch loss: 10.56979\n",
      "10/16 avg epoch loss: 10.03099\n",
      "12/16 avg epoch loss: 9.74588\n",
      "14/16 avg epoch loss: 9.48486\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 10.43345\n",
      "2/16 avg epoch loss: 10.11803\n",
      "4/16 avg epoch loss: 9.00587\n",
      "6/16 avg epoch loss: 8.7018\n",
      "8/16 avg epoch loss: 9.51631\n",
      "10/16 avg epoch loss: 8.61972\n",
      "12/16 avg epoch loss: 8.94423\n",
      "14/16 avg epoch loss: 9.63781\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 11.19909\n",
      "2/16 avg epoch loss: 8.21421\n",
      "4/16 avg epoch loss: 9.72917\n",
      "6/16 avg epoch loss: 8.91405\n",
      "8/16 avg epoch loss: 9.23553\n",
      "10/16 avg epoch loss: 9.33279\n",
      "12/16 avg epoch loss: 8.69726\n",
      "14/16 avg epoch loss: 8.65978\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 5.4727\n",
      "2/16 avg epoch loss: 5.92904\n",
      "4/16 avg epoch loss: 8.37884\n",
      "6/16 avg epoch loss: 7.71875\n",
      "8/16 avg epoch loss: 7.71276\n",
      "10/16 avg epoch loss: 8.09623\n",
      "12/16 avg epoch loss: 8.24724\n",
      "14/16 avg epoch loss: 8.0525\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 9.62386\n",
      "2/16 avg epoch loss: 9.78158\n",
      "4/16 avg epoch loss: 8.71841\n",
      "6/16 avg epoch loss: 8.3624\n",
      "8/16 avg epoch loss: 7.8899\n",
      "10/16 avg epoch loss: 8.19811\n",
      "12/16 avg epoch loss: 8.55018\n",
      "14/16 avg epoch loss: 8.50332\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 8.88045\n",
      "2/16 avg epoch loss: 9.50829\n",
      "4/16 avg epoch loss: 8.96001\n",
      "6/16 avg epoch loss: 8.38288\n",
      "8/16 avg epoch loss: 8.57857\n",
      "10/16 avg epoch loss: 8.13708\n",
      "12/16 avg epoch loss: 8.2066\n",
      "14/16 avg epoch loss: 8.28876\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 10.28288\n",
      "2/16 avg epoch loss: 9.40062\n",
      "4/16 avg epoch loss: 8.50231\n",
      "6/16 avg epoch loss: 7.6368\n",
      "8/16 avg epoch loss: 7.35409\n",
      "10/16 avg epoch loss: 7.20781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16 avg epoch loss: 7.87093\n",
      "14/16 avg epoch loss: 7.76802\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 5.84873\n",
      "2/16 avg epoch loss: 6.94297\n",
      "4/16 avg epoch loss: 7.94108\n",
      "6/16 avg epoch loss: 7.37064\n",
      "8/16 avg epoch loss: 7.15515\n",
      "10/16 avg epoch loss: 7.65775\n",
      "12/16 avg epoch loss: 7.16575\n",
      "14/16 avg epoch loss: 7.06177\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 8.80341\n",
      "2/16 avg epoch loss: 7.11852\n",
      "4/16 avg epoch loss: 6.92754\n",
      "6/16 avg epoch loss: 6.60787\n",
      "8/16 avg epoch loss: 6.38252\n",
      "10/16 avg epoch loss: 6.07635\n",
      "12/16 avg epoch loss: 6.04186\n",
      "14/16 avg epoch loss: 6.75714\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 4.14553\n",
      "2/16 avg epoch loss: 3.97268\n",
      "4/16 avg epoch loss: 5.64073\n",
      "6/16 avg epoch loss: 6.90453\n",
      "8/16 avg epoch loss: 6.492\n",
      "10/16 avg epoch loss: 6.51576\n",
      "12/16 avg epoch loss: 6.82229\n",
      "14/16 avg epoch loss: 6.61012\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 4.76356\n",
      "2/16 avg epoch loss: 4.83716\n",
      "4/16 avg epoch loss: 5.54727\n",
      "6/16 avg epoch loss: 5.68131\n",
      "8/16 avg epoch loss: 5.69115\n",
      "10/16 avg epoch loss: 5.62812\n",
      "12/16 avg epoch loss: 6.17327\n",
      "14/16 avg epoch loss: 6.27351\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 6.92192\n",
      "2/16 avg epoch loss: 5.28559\n",
      "4/16 avg epoch loss: 4.87645\n",
      "6/16 avg epoch loss: 5.33476\n",
      "8/16 avg epoch loss: 5.3765\n",
      "10/16 avg epoch loss: 5.71244\n",
      "12/16 avg epoch loss: 6.03653\n",
      "14/16 avg epoch loss: 6.18872\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 5.67924\n",
      "2/16 avg epoch loss: 6.89039\n",
      "4/16 avg epoch loss: 6.32742\n",
      "6/16 avg epoch loss: 6.14152\n",
      "8/16 avg epoch loss: 5.84468\n",
      "10/16 avg epoch loss: 6.29268\n",
      "12/16 avg epoch loss: 6.39418\n",
      "14/16 avg epoch loss: 6.23032\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 5.23795\n",
      "2/16 avg epoch loss: 5.42676\n",
      "4/16 avg epoch loss: 6.11544\n",
      "6/16 avg epoch loss: 5.99324\n",
      "8/16 avg epoch loss: 5.33912\n",
      "10/16 avg epoch loss: 5.49482\n",
      "12/16 avg epoch loss: 5.94194\n",
      "14/16 avg epoch loss: 5.9447\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 5.11868\n",
      "2/16 avg epoch loss: 6.65287\n",
      "4/16 avg epoch loss: 6.15166\n",
      "6/16 avg epoch loss: 6.07506\n",
      "8/16 avg epoch loss: 5.72593\n",
      "10/16 avg epoch loss: 5.66093\n",
      "12/16 avg epoch loss: 5.63884\n",
      "14/16 avg epoch loss: 6.43308\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 6.11669\n",
      "2/16 avg epoch loss: 4.84505\n",
      "4/16 avg epoch loss: 5.94108\n",
      "6/16 avg epoch loss: 5.63693\n",
      "8/16 avg epoch loss: 5.77258\n",
      "10/16 avg epoch loss: 5.46026\n",
      "12/16 avg epoch loss: 5.88355\n",
      "14/16 avg epoch loss: 5.8673\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 6.44504\n",
      "2/16 avg epoch loss: 5.51863\n",
      "4/16 avg epoch loss: 5.31978\n",
      "6/16 avg epoch loss: 5.51044\n",
      "8/16 avg epoch loss: 5.47578\n",
      "10/16 avg epoch loss: 5.62826\n",
      "12/16 avg epoch loss: 5.48749\n",
      "14/16 avg epoch loss: 5.28232\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 2.79998\n",
      "2/16 avg epoch loss: 3.65512\n",
      "4/16 avg epoch loss: 4.40497\n",
      "6/16 avg epoch loss: 4.40814\n",
      "8/16 avg epoch loss: 4.5713\n",
      "10/16 avg epoch loss: 4.72344\n",
      "12/16 avg epoch loss: 5.09806\n",
      "14/16 avg epoch loss: 5.023\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 6.63577\n",
      "2/16 avg epoch loss: 4.61182\n",
      "4/16 avg epoch loss: 3.89358\n",
      "6/16 avg epoch loss: 3.78025\n",
      "8/16 avg epoch loss: 3.85178\n",
      "10/16 avg epoch loss: 4.0714\n",
      "12/16 avg epoch loss: 4.32891\n",
      "14/16 avg epoch loss: 4.59081\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 3.52183\n",
      "2/16 avg epoch loss: 4.21103\n",
      "4/16 avg epoch loss: 4.32455\n",
      "6/16 avg epoch loss: 4.02986\n",
      "8/16 avg epoch loss: 4.23417\n",
      "10/16 avg epoch loss: 4.3475\n",
      "12/16 avg epoch loss: 4.23808\n",
      "14/16 avg epoch loss: 4.31309\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 3.35062\n",
      "2/16 avg epoch loss: 5.12919\n",
      "4/16 avg epoch loss: 4.20763\n",
      "6/16 avg epoch loss: 4.27927\n",
      "8/16 avg epoch loss: 4.04631\n",
      "10/16 avg epoch loss: 4.21427\n",
      "12/16 avg epoch loss: 4.54311\n",
      "14/16 avg epoch loss: 4.34999\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 5.43314\n",
      "2/16 avg epoch loss: 4.89971\n",
      "4/16 avg epoch loss: 4.0603\n",
      "6/16 avg epoch loss: 3.87467\n",
      "8/16 avg epoch loss: 3.63729\n",
      "10/16 avg epoch loss: 3.45763\n",
      "12/16 avg epoch loss: 3.78454\n",
      "14/16 avg epoch loss: 3.98201\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 5.20543\n",
      "2/16 avg epoch loss: 3.4989\n",
      "4/16 avg epoch loss: 4.43094\n",
      "6/16 avg epoch loss: 3.99655\n",
      "8/16 avg epoch loss: 3.62186\n",
      "10/16 avg epoch loss: 3.4489\n",
      "12/16 avg epoch loss: 3.81709\n",
      "14/16 avg epoch loss: 4.01292\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 1.78703\n",
      "2/16 avg epoch loss: 2.4798\n",
      "4/16 avg epoch loss: 2.35034\n",
      "6/16 avg epoch loss: 2.8693\n",
      "8/16 avg epoch loss: 3.62273\n",
      "10/16 avg epoch loss: 3.77883\n",
      "12/16 avg epoch loss: 3.96323\n",
      "14/16 avg epoch loss: 4.04002\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 3.32394\n",
      "2/16 avg epoch loss: 3.86158\n",
      "4/16 avg epoch loss: 4.13936\n",
      "6/16 avg epoch loss: 4.37148\n",
      "8/16 avg epoch loss: 4.48169\n",
      "10/16 avg epoch loss: 4.32593\n",
      "12/16 avg epoch loss: 4.09592\n",
      "14/16 avg epoch loss: 4.44278\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 4.53467\n",
      "2/16 avg epoch loss: 4.15954\n",
      "4/16 avg epoch loss: 4.04076\n",
      "6/16 avg epoch loss: 3.54133\n",
      "8/16 avg epoch loss: 3.35601\n",
      "10/16 avg epoch loss: 3.72147\n",
      "12/16 avg epoch loss: 3.64908\n",
      "14/16 avg epoch loss: 3.7909\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 2.1385\n",
      "2/16 avg epoch loss: 3.74485\n",
      "4/16 avg epoch loss: 3.69309\n",
      "6/16 avg epoch loss: 3.37415\n",
      "8/16 avg epoch loss: 3.14308\n",
      "10/16 avg epoch loss: 3.23585\n",
      "12/16 avg epoch loss: 3.20293\n",
      "14/16 avg epoch loss: 3.57936\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 6.07914\n",
      "2/16 avg epoch loss: 3.50107\n",
      "4/16 avg epoch loss: 3.10063\n",
      "6/16 avg epoch loss: 2.84907\n",
      "8/16 avg epoch loss: 3.50019\n",
      "10/16 avg epoch loss: 3.45324\n",
      "12/16 avg epoch loss: 3.25438\n",
      "14/16 avg epoch loss: 3.39365\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 2.68671\n",
      "2/16 avg epoch loss: 3.18594\n",
      "4/16 avg epoch loss: 3.11149\n",
      "6/16 avg epoch loss: 3.28901\n",
      "8/16 avg epoch loss: 3.3112\n",
      "10/16 avg epoch loss: 3.40813\n",
      "12/16 avg epoch loss: 3.38332\n",
      "14/16 avg epoch loss: 3.36763\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 3.98031\n",
      "2/16 avg epoch loss: 2.92375\n",
      "4/16 avg epoch loss: 2.99137\n",
      "6/16 avg epoch loss: 2.90463\n",
      "8/16 avg epoch loss: 2.97737\n",
      "10/16 avg epoch loss: 3.30508\n",
      "12/16 avg epoch loss: 3.12211\n",
      "14/16 avg epoch loss: 3.10738\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 4.59008\n",
      "2/16 avg epoch loss: 4.06895\n",
      "4/16 avg epoch loss: 3.49725\n",
      "6/16 avg epoch loss: 3.30139\n",
      "8/16 avg epoch loss: 3.11281\n",
      "10/16 avg epoch loss: 2.85422\n",
      "12/16 avg epoch loss: 2.89108\n",
      "14/16 avg epoch loss: 3.06116\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 1.06072\n",
      "2/16 avg epoch loss: 1.86929\n",
      "4/16 avg epoch loss: 2.10276\n",
      "6/16 avg epoch loss: 2.5656\n",
      "8/16 avg epoch loss: 2.79213\n",
      "10/16 avg epoch loss: 3.02761\n",
      "12/16 avg epoch loss: 3.10946\n",
      "14/16 avg epoch loss: 2.99696\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 3.71321\n",
      "2/16 avg epoch loss: 3.60762\n",
      "4/16 avg epoch loss: 3.08502\n",
      "6/16 avg epoch loss: 2.89173\n",
      "8/16 avg epoch loss: 3.09348\n",
      "10/16 avg epoch loss: 2.82477\n",
      "12/16 avg epoch loss: 2.81045\n",
      "14/16 avg epoch loss: 2.8257\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 5.22567\n",
      "2/16 avg epoch loss: 3.05197\n",
      "4/16 avg epoch loss: 2.75927\n",
      "6/16 avg epoch loss: 3.00631\n",
      "8/16 avg epoch loss: 3.22566\n",
      "10/16 avg epoch loss: 3.05176\n",
      "12/16 avg epoch loss: 2.9754\n",
      "14/16 avg epoch loss: 3.13118\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 4.01904\n",
      "2/16 avg epoch loss: 3.45725\n",
      "4/16 avg epoch loss: 3.10977\n",
      "6/16 avg epoch loss: 2.94046\n",
      "8/16 avg epoch loss: 2.88717\n",
      "10/16 avg epoch loss: 2.77855\n",
      "12/16 avg epoch loss: 2.75768\n",
      "14/16 avg epoch loss: 2.7423\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/16 avg epoch loss: 5.3105\n",
      "2/16 avg epoch loss: 2.75318\n",
      "4/16 avg epoch loss: 3.00086\n",
      "6/16 avg epoch loss: 2.69388\n",
      "8/16 avg epoch loss: 2.62528\n",
      "10/16 avg epoch loss: 2.73176\n",
      "12/16 avg epoch loss: 2.57553\n",
      "14/16 avg epoch loss: 2.59863\n",
      "\n",
      "----EPOCH 2----\n",
      "0/16 avg epoch loss: 1.70812\n",
      "2/16 avg epoch loss: 2.23604\n",
      "4/16 avg epoch loss: 2.92469\n",
      "6/16 avg epoch loss: 2.65254\n",
      "8/16 avg epoch loss: 2.73388\n",
      "10/16 avg epoch loss: 2.63539\n",
      "12/16 avg epoch loss: 2.50176\n",
      "14/16 avg epoch loss: 2.48825\n",
      "\n",
      "----EPOCH 3----\n",
      "0/16 avg epoch loss: 3.00052\n",
      "2/16 avg epoch loss: 2.29664\n",
      "4/16 avg epoch loss: 2.44318\n",
      "6/16 avg epoch loss: 2.70034\n",
      "8/16 avg epoch loss: 2.47384\n",
      "10/16 avg epoch loss: 2.56764\n",
      "12/16 avg epoch loss: 2.66207\n",
      "14/16 avg epoch loss: 2.51946\n",
      "\n",
      "----EPOCH 4----\n",
      "0/16 avg epoch loss: 1.79616\n",
      "2/16 avg epoch loss: 2.10513\n",
      "4/16 avg epoch loss: 2.25618\n",
      "6/16 avg epoch loss: 2.22939\n",
      "8/16 avg epoch loss: 2.21828\n",
      "10/16 avg epoch loss: 2.2233\n",
      "12/16 avg epoch loss: 2.32299\n",
      "14/16 avg epoch loss: 2.35688\n",
      "\n",
      "----EPOCH 5----\n",
      "0/16 avg epoch loss: 2.99351\n",
      "2/16 avg epoch loss: 2.84554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/16 avg epoch loss: 2.69665\n",
      "6/16 avg epoch loss: 2.39245\n",
      "8/16 avg epoch loss: 2.39937\n",
      "10/16 avg epoch loss: 2.33444\n",
      "12/16 avg epoch loss: 2.19163\n",
      "14/16 avg epoch loss: 2.20405\n",
      "\n",
      "----EPOCH 6----\n",
      "0/16 avg epoch loss: 1.41819\n",
      "2/16 avg epoch loss: 1.90404\n",
      "4/16 avg epoch loss: 2.53215\n",
      "6/16 avg epoch loss: 2.44395\n",
      "8/16 avg epoch loss: 2.42938\n",
      "10/16 avg epoch loss: 2.22314\n",
      "12/16 avg epoch loss: 2.17911\n",
      "14/16 avg epoch loss: 2.22157\n",
      "\n",
      "----EPOCH 7----\n",
      "0/16 avg epoch loss: 1.69329\n",
      "2/16 avg epoch loss: 2.47988\n",
      "4/16 avg epoch loss: 2.19031\n",
      "6/16 avg epoch loss: 2.22817\n",
      "8/16 avg epoch loss: 2.28681\n",
      "10/16 avg epoch loss: 2.10902\n",
      "12/16 avg epoch loss: 1.96417\n",
      "14/16 avg epoch loss: 2.02813\n",
      "\n",
      "----EPOCH 8----\n",
      "0/16 avg epoch loss: 0.91606\n",
      "2/16 avg epoch loss: 1.17986\n",
      "4/16 avg epoch loss: 1.67277\n",
      "6/16 avg epoch loss: 2.05598\n",
      "8/16 avg epoch loss: 1.8561\n",
      "10/16 avg epoch loss: 1.87498\n",
      "12/16 avg epoch loss: 1.94502\n",
      "14/16 avg epoch loss: 1.91709\n",
      "\n",
      "----EPOCH 9----\n",
      "0/16 avg epoch loss: 2.54403\n",
      "2/16 avg epoch loss: 2.30399\n",
      "4/16 avg epoch loss: 1.75495\n",
      "6/16 avg epoch loss: 1.91124\n",
      "8/16 avg epoch loss: 1.67243\n",
      "10/16 avg epoch loss: 1.66549\n",
      "12/16 avg epoch loss: 1.84966\n",
      "14/16 avg epoch loss: 1.95957\n",
      "\n",
      "----EPOCH 10----\n",
      "0/16 avg epoch loss: 1.67855\n",
      "2/16 avg epoch loss: 1.34589\n",
      "4/16 avg epoch loss: 1.43425\n",
      "6/16 avg epoch loss: 1.52944\n",
      "8/16 avg epoch loss: 1.45959\n",
      "10/16 avg epoch loss: 1.66658\n",
      "12/16 avg epoch loss: 1.71706\n",
      "14/16 avg epoch loss: 1.94038\n",
      "\n",
      "Training finished!\n",
      "  4.890 s (68464929 allocations: 8.94 GiB)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip320\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip321\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M169.121 1486.45 L2352.76 1486.45 L2352.76 47.2441 L169.121 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip322\">\n",
       "    <rect x=\"169\" y=\"47\" width=\"2185\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"204.846,1486.45 204.846,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"726.373,1486.45 726.373,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1247.9,1486.45 1247.9,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1769.43,1486.45 1769.43,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2290.95,1486.45 2290.95,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"204.846,1486.45 204.846,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"726.373,1486.45 726.373,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1247.9,1486.45 1247.9,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1769.43,1486.45 1769.43,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2290.95,1486.45 2290.95,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M204.846 1517.37 Q201.235 1517.37 199.406 1520.93 Q197.601 1524.47 197.601 1531.6 Q197.601 1538.71 199.406 1542.27 Q201.235 1545.82 204.846 1545.82 Q208.48 1545.82 210.286 1542.27 Q212.114 1538.71 212.114 1531.6 Q212.114 1524.47 210.286 1520.93 Q208.48 1517.37 204.846 1517.37 M204.846 1513.66 Q210.656 1513.66 213.712 1518.27 Q216.79 1522.85 216.79 1531.6 Q216.79 1540.33 213.712 1544.94 Q210.656 1549.52 204.846 1549.52 Q199.036 1549.52 195.957 1544.94 Q192.902 1540.33 192.902 1531.6 Q192.902 1522.85 195.957 1518.27 Q199.036 1513.66 204.846 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M705.146 1544.91 L721.466 1544.91 L721.466 1548.85 L699.521 1548.85 L699.521 1544.91 Q702.183 1542.16 706.767 1537.53 Q711.373 1532.88 712.554 1531.53 Q714.799 1529.01 715.679 1527.27 Q716.582 1525.51 716.582 1523.82 Q716.582 1521.07 714.637 1519.33 Q712.716 1517.6 709.614 1517.6 Q707.415 1517.6 704.961 1518.36 Q702.531 1519.13 699.753 1520.68 L699.753 1515.95 Q702.577 1514.82 705.031 1514.24 Q707.484 1513.66 709.521 1513.66 Q714.892 1513.66 718.086 1516.35 Q721.281 1519.03 721.281 1523.52 Q721.281 1525.65 720.47 1527.57 Q719.683 1529.47 717.577 1532.07 Q716.998 1532.74 713.896 1535.95 Q710.795 1539.15 705.146 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M741.281 1517.37 Q737.669 1517.37 735.841 1520.93 Q734.035 1524.47 734.035 1531.6 Q734.035 1538.71 735.841 1542.27 Q737.669 1545.82 741.281 1545.82 Q744.915 1545.82 746.72 1542.27 Q748.549 1538.71 748.549 1531.6 Q748.549 1524.47 746.72 1520.93 Q744.915 1517.37 741.281 1517.37 M741.281 1513.66 Q747.091 1513.66 750.146 1518.27 Q753.225 1522.85 753.225 1531.6 Q753.225 1540.33 750.146 1544.94 Q747.091 1549.52 741.281 1549.52 Q735.47 1549.52 732.392 1544.94 Q729.336 1540.33 729.336 1531.6 Q729.336 1522.85 732.392 1518.27 Q735.47 1513.66 741.281 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1236.07 1518.36 L1224.27 1536.81 L1236.07 1536.81 L1236.07 1518.36 M1234.84 1514.29 L1240.72 1514.29 L1240.72 1536.81 L1245.66 1536.81 L1245.66 1540.7 L1240.72 1540.7 L1240.72 1548.85 L1236.07 1548.85 L1236.07 1540.7 L1220.47 1540.7 L1220.47 1536.19 L1234.84 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1263.39 1517.37 Q1259.78 1517.37 1257.95 1520.93 Q1256.14 1524.47 1256.14 1531.6 Q1256.14 1538.71 1257.95 1542.27 Q1259.78 1545.82 1263.39 1545.82 Q1267.02 1545.82 1268.83 1542.27 Q1270.65 1538.71 1270.65 1531.6 Q1270.65 1524.47 1268.83 1520.93 Q1267.02 1517.37 1263.39 1517.37 M1263.39 1513.66 Q1269.2 1513.66 1272.25 1518.27 Q1275.33 1522.85 1275.33 1531.6 Q1275.33 1540.33 1272.25 1544.94 Q1269.2 1549.52 1263.39 1549.52 Q1257.58 1549.52 1254.5 1544.94 Q1251.44 1540.33 1251.44 1531.6 Q1251.44 1522.85 1254.5 1518.27 Q1257.58 1513.66 1263.39 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1754.83 1529.7 Q1751.68 1529.7 1749.83 1531.86 Q1748 1534.01 1748 1537.76 Q1748 1541.49 1749.83 1543.66 Q1751.68 1545.82 1754.83 1545.82 Q1757.98 1545.82 1759.81 1543.66 Q1761.66 1541.49 1761.66 1537.76 Q1761.66 1534.01 1759.81 1531.86 Q1757.98 1529.7 1754.83 1529.7 M1764.12 1515.05 L1764.12 1519.31 Q1762.36 1518.48 1760.55 1518.04 Q1758.77 1517.6 1757.01 1517.6 Q1752.38 1517.6 1749.93 1520.72 Q1747.49 1523.85 1747.15 1530.17 Q1748.51 1528.15 1750.57 1527.09 Q1752.63 1526 1755.11 1526 Q1760.32 1526 1763.33 1529.17 Q1766.36 1532.32 1766.36 1537.76 Q1766.36 1543.08 1763.21 1546.3 Q1760.06 1549.52 1754.83 1549.52 Q1748.84 1549.52 1745.67 1544.94 Q1742.49 1540.33 1742.49 1531.6 Q1742.49 1523.41 1746.38 1518.55 Q1750.27 1513.66 1756.82 1513.66 Q1758.58 1513.66 1760.37 1514.01 Q1762.17 1514.36 1764.12 1515.05 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1784.42 1517.37 Q1780.8 1517.37 1778.98 1520.93 Q1777.17 1524.47 1777.17 1531.6 Q1777.17 1538.71 1778.98 1542.27 Q1780.8 1545.82 1784.42 1545.82 Q1788.05 1545.82 1789.86 1542.27 Q1791.68 1538.71 1791.68 1531.6 Q1791.68 1524.47 1789.86 1520.93 Q1788.05 1517.37 1784.42 1517.37 M1784.42 1513.66 Q1790.23 1513.66 1793.28 1518.27 Q1796.36 1522.85 1796.36 1531.6 Q1796.36 1540.33 1793.28 1544.94 Q1790.23 1549.52 1784.42 1549.52 Q1778.61 1549.52 1775.53 1544.94 Q1772.47 1540.33 1772.47 1531.6 Q1772.47 1522.85 1775.53 1518.27 Q1778.61 1513.66 1784.42 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2275.83 1532.44 Q2272.49 1532.44 2270.57 1534.22 Q2268.67 1536 2268.67 1539.13 Q2268.67 1542.25 2270.57 1544.03 Q2272.49 1545.82 2275.83 1545.82 Q2279.16 1545.82 2281.08 1544.03 Q2283 1542.23 2283 1539.13 Q2283 1536 2281.08 1534.22 Q2279.18 1532.44 2275.83 1532.44 M2271.15 1530.45 Q2268.14 1529.7 2266.45 1527.64 Q2264.79 1525.58 2264.79 1522.62 Q2264.79 1518.48 2267.73 1516.07 Q2270.69 1513.66 2275.83 1513.66 Q2280.99 1513.66 2283.93 1516.07 Q2286.87 1518.48 2286.87 1522.62 Q2286.87 1525.58 2285.18 1527.64 Q2283.51 1529.7 2280.53 1530.45 Q2283.91 1531.23 2285.78 1533.52 Q2287.68 1535.82 2287.68 1539.13 Q2287.68 1544.15 2284.6 1546.83 Q2281.55 1549.52 2275.83 1549.52 Q2270.11 1549.52 2267.03 1546.83 Q2263.98 1544.15 2263.98 1539.13 Q2263.98 1535.82 2265.87 1533.52 Q2267.77 1531.23 2271.15 1530.45 M2269.44 1523.06 Q2269.44 1525.75 2271.11 1527.25 Q2272.8 1528.76 2275.83 1528.76 Q2278.84 1528.76 2280.53 1527.25 Q2282.24 1525.75 2282.24 1523.06 Q2282.24 1520.38 2280.53 1518.87 Q2278.84 1517.37 2275.83 1517.37 Q2272.8 1517.37 2271.11 1518.87 Q2269.44 1520.38 2269.44 1523.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2305.99 1517.37 Q2302.38 1517.37 2300.55 1520.93 Q2298.74 1524.47 2298.74 1531.6 Q2298.74 1538.71 2300.55 1542.27 Q2302.38 1545.82 2305.99 1545.82 Q2309.62 1545.82 2311.43 1542.27 Q2313.26 1538.71 2313.26 1531.6 Q2313.26 1524.47 2311.43 1520.93 Q2309.62 1517.37 2305.99 1517.37 M2305.99 1513.66 Q2311.8 1513.66 2314.86 1518.27 Q2317.93 1522.85 2317.93 1531.6 Q2317.93 1540.33 2314.86 1544.94 Q2311.8 1549.52 2305.99 1549.52 Q2300.18 1549.52 2297.1 1544.94 Q2294.05 1540.33 2294.05 1531.6 Q2294.05 1522.85 2297.1 1518.27 Q2300.18 1513.66 2305.99 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,1292.99 2352.76,1292.99 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,1010.56 2352.76,1010.56 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,728.13 2352.76,728.13 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,445.701 2352.76,445.701 \"/>\n",
       "<polyline clip-path=\"url(#clip322)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,163.271 2352.76,163.271 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1486.45 169.121,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1292.99 188.019,1292.99 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1010.56 188.019,1010.56 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,728.13 188.019,728.13 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,445.701 188.019,445.701 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,163.271 188.019,163.271 \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M95.1817 1291.63 Q98.5382 1292.35 100.413 1294.62 Q102.311 1296.89 102.311 1300.22 Q102.311 1305.34 98.7928 1308.14 Q95.2743 1310.94 88.7928 1310.94 Q86.6169 1310.94 84.3021 1310.5 Q82.0105 1310.08 79.5568 1309.23 L79.5568 1304.71 Q81.5012 1305.85 83.816 1306.43 Q86.1308 1307 88.654 1307 Q93.0521 1307 95.3437 1305.27 Q97.6585 1303.53 97.6585 1300.22 Q97.6585 1297.17 95.5058 1295.45 Q93.3762 1293.72 89.5567 1293.72 L85.529 1293.72 L85.529 1289.87 L89.7419 1289.87 Q93.191 1289.87 95.0197 1288.51 Q96.8484 1287.12 96.8484 1284.53 Q96.8484 1281.87 94.9502 1280.45 Q93.0752 1279.02 89.5567 1279.02 Q87.6354 1279.02 85.4364 1279.43 Q83.2373 1279.85 80.5984 1280.73 L80.5984 1276.56 Q83.2605 1275.82 85.5753 1275.45 Q87.9132 1275.08 89.9734 1275.08 Q95.2974 1275.08 98.3993 1277.51 Q101.501 1279.92 101.501 1284.04 Q101.501 1286.91 99.8576 1288.9 Q98.2141 1290.87 95.1817 1291.63 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 1278.79 Q117.566 1278.79 115.737 1282.35 Q113.932 1285.89 113.932 1293.02 Q113.932 1300.13 115.737 1303.69 Q117.566 1307.24 121.177 1307.24 Q124.811 1307.24 126.617 1303.69 Q128.445 1300.13 128.445 1293.02 Q128.445 1285.89 126.617 1282.35 Q124.811 1278.79 121.177 1278.79 M121.177 1275.08 Q126.987 1275.08 130.043 1279.69 Q133.121 1284.27 133.121 1293.02 Q133.121 1301.75 130.043 1306.36 Q126.987 1310.94 121.177 1310.94 Q115.367 1310.94 112.288 1306.36 Q109.233 1301.75 109.233 1293.02 Q109.233 1284.27 112.288 1279.69 Q115.367 1275.08 121.177 1275.08 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M91.5938 1008.7 Q88.4456 1008.7 86.5938 1010.85 Q84.7651 1013 84.7651 1016.75 Q84.7651 1020.48 86.5938 1022.65 Q88.4456 1024.81 91.5938 1024.81 Q94.7419 1024.81 96.5706 1022.65 Q98.4224 1020.48 98.4224 1016.75 Q98.4224 1013 96.5706 1010.85 Q94.7419 1008.7 91.5938 1008.7 M100.876 994.043 L100.876 998.302 Q99.1169 997.469 97.3113 997.029 Q95.5289 996.589 93.7697 996.589 Q89.1401 996.589 86.6864 999.714 Q84.2558 1002.84 83.9086 1009.16 Q85.2743 1007.14 87.3345 1006.08 Q89.3947 1004.99 91.8715 1004.99 Q97.0798 1004.99 100.089 1008.16 Q103.121 1011.31 103.121 1016.75 Q103.121 1022.07 99.9733 1025.29 Q96.8252 1028.51 91.5938 1028.51 Q85.5984 1028.51 82.4271 1023.93 Q79.2559 1019.32 79.2559 1010.59 Q79.2559 1002.4 83.1447 997.538 Q87.0336 992.654 93.5845 992.654 Q95.3437 992.654 97.1261 993.001 Q98.9317 993.348 100.876 994.043 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 996.357 Q117.566 996.357 115.737 999.922 Q113.932 1003.46 113.932 1010.59 Q113.932 1017.7 115.737 1021.26 Q117.566 1024.81 121.177 1024.81 Q124.811 1024.81 126.617 1021.26 Q128.445 1017.7 128.445 1010.59 Q128.445 1003.46 126.617 999.922 Q124.811 996.357 121.177 996.357 M121.177 992.654 Q126.987 992.654 130.043 997.26 Q133.121 1001.84 133.121 1010.59 Q133.121 1019.32 130.043 1023.93 Q126.987 1028.51 121.177 1028.51 Q115.367 1028.51 112.288 1023.93 Q109.233 1019.32 109.233 1010.59 Q109.233 1001.84 112.288 997.26 Q115.367 992.654 121.177 992.654 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M81.154 744.692 L81.154 740.433 Q82.9133 741.266 84.7188 741.706 Q86.5243 742.146 88.2604 742.146 Q92.89 742.146 95.3206 739.044 Q97.7743 735.919 98.1215 729.576 Q96.7789 731.567 94.7187 732.632 Q92.6586 733.697 90.1586 733.697 Q84.9734 733.697 81.941 730.572 Q78.9318 727.424 78.9318 721.984 Q78.9318 716.66 82.0799 713.442 Q85.2281 710.225 90.4595 710.225 Q96.4548 710.225 99.603 714.831 Q102.774 719.414 102.774 728.164 Q102.774 736.336 98.8854 741.22 Q95.0197 746.081 88.4688 746.081 Q86.7095 746.081 84.904 745.734 Q83.0984 745.386 81.154 744.692 M90.4595 730.039 Q93.6076 730.039 95.4363 727.887 Q97.2882 725.734 97.2882 721.984 Q97.2882 718.257 95.4363 716.104 Q93.6076 713.928 90.4595 713.928 Q87.3114 713.928 85.4595 716.104 Q83.6308 718.257 83.6308 721.984 Q83.6308 725.734 85.4595 727.887 Q87.3114 730.039 90.4595 730.039 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 713.928 Q117.566 713.928 115.737 717.493 Q113.932 721.035 113.932 728.164 Q113.932 735.271 115.737 738.836 Q117.566 742.377 121.177 742.377 Q124.811 742.377 126.617 738.836 Q128.445 735.271 128.445 728.164 Q128.445 721.035 126.617 717.493 Q124.811 713.928 121.177 713.928 M121.177 710.225 Q126.987 710.225 130.043 714.831 Q133.121 719.414 133.121 728.164 Q133.121 736.891 130.043 741.498 Q126.987 746.081 121.177 746.081 Q115.367 746.081 112.288 741.498 Q109.233 736.891 109.233 728.164 Q109.233 719.414 112.288 714.831 Q115.367 710.225 121.177 710.225 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M51.6634 459.045 L59.3023 459.045 L59.3023 432.68 L50.9921 434.346 L50.9921 430.087 L59.256 428.421 L63.9319 428.421 L63.9319 459.045 L71.5707 459.045 L71.5707 462.981 L51.6634 462.981 L51.6634 459.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M85.0429 459.045 L101.362 459.045 L101.362 462.981 L79.4179 462.981 L79.4179 459.045 Q82.0799 456.291 86.6632 451.661 Q91.2697 447.008 92.4502 445.666 Q94.6956 443.143 95.5752 441.407 Q96.478 439.647 96.478 437.957 Q96.478 435.203 94.5336 433.467 Q92.6123 431.731 89.5104 431.731 Q87.3114 431.731 84.8577 432.495 Q82.4271 433.258 79.6494 434.809 L79.6494 430.087 Q82.4734 428.953 84.9271 428.374 Q87.3808 427.796 89.4178 427.796 Q94.7882 427.796 97.9826 430.481 Q101.177 433.166 101.177 437.657 Q101.177 439.786 100.367 441.707 Q99.5798 443.606 97.4734 446.198 Q96.8947 446.869 93.7928 450.087 Q90.691 453.281 85.0429 459.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 431.499 Q117.566 431.499 115.737 435.064 Q113.932 438.606 113.932 445.735 Q113.932 452.842 115.737 456.406 Q117.566 459.948 121.177 459.948 Q124.811 459.948 126.617 456.406 Q128.445 452.842 128.445 445.735 Q128.445 438.606 126.617 435.064 Q124.811 431.499 121.177 431.499 M121.177 427.796 Q126.987 427.796 130.043 432.402 Q133.121 436.985 133.121 445.735 Q133.121 454.462 130.043 459.068 Q126.987 463.652 121.177 463.652 Q115.367 463.652 112.288 459.068 Q109.233 454.462 109.233 445.735 Q109.233 436.985 112.288 432.402 Q115.367 427.796 121.177 427.796 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M51.6634 176.616 L59.3023 176.616 L59.3023 150.251 L50.9921 151.917 L50.9921 147.658 L59.256 145.991 L63.9319 145.991 L63.9319 176.616 L71.5707 176.616 L71.5707 180.551 L51.6634 180.551 L51.6634 176.616 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M81.0614 145.991 L99.4178 145.991 L99.4178 149.927 L85.3438 149.927 L85.3438 158.399 Q86.3623 158.052 87.3808 157.889 Q88.3993 157.704 89.4178 157.704 Q95.2049 157.704 98.5845 160.876 Q101.964 164.047 101.964 169.463 Q101.964 175.042 98.4919 178.144 Q95.0197 181.223 88.7003 181.223 Q86.5243 181.223 84.2558 180.852 Q82.0105 180.482 79.6031 179.741 L79.6031 175.042 Q81.6864 176.176 83.9086 176.732 Q86.1308 177.288 88.6077 177.288 Q92.6123 177.288 94.9502 175.181 Q97.2882 173.075 97.2882 169.463 Q97.2882 165.852 94.9502 163.746 Q92.6123 161.639 88.6077 161.639 Q86.7327 161.639 84.8577 162.056 Q83.0058 162.473 81.0614 163.352 L81.0614 145.991 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M121.177 149.07 Q117.566 149.07 115.737 152.635 Q113.932 156.177 113.932 163.306 Q113.932 170.413 115.737 173.977 Q117.566 177.519 121.177 177.519 Q124.811 177.519 126.617 173.977 Q128.445 170.413 128.445 163.306 Q128.445 156.177 126.617 152.635 Q124.811 149.07 121.177 149.07 M121.177 145.366 Q126.987 145.366 130.043 149.973 Q133.121 154.556 133.121 163.306 Q133.121 172.033 130.043 176.639 Q126.987 181.223 121.177 181.223 Q115.367 181.223 112.288 176.639 Q109.233 172.033 109.233 163.306 Q109.233 154.556 112.288 149.973 Q115.367 145.366 121.177 145.366 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip322)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"230.922,87.9763 256.999,107.006 283.075,147.7 309.151,184.098 335.228,227.837 361.304,268.984 387.38,317.203 413.457,380.234 439.533,859.216 465.61,896.404 491.686,915.151 517.762,951.486 543.839,969.882 569.915,995.593 595.991,1017.45 622.068,1036.29 648.144,1160.46 674.22,1216.6 700.297,1206.26 726.373,1239.98 752.45,1237.97 778.526,1217.18 804.602,1221.16 830.679,1214.64 856.755,1124.57 882.831,1219.07 908.908,1241.39 934.984,1269.36 961.06,1281.73 987.137,1277.49 1013.21,1279.49 1039.29,1278.16 1065.37,1325.29 1091.44,1327.52 1117.52,1341.94 1143.59,1339.91 1169.67,1332.26 1195.75,1324.24 1221.82,1317.05 1247.9,1323.98 1273.98,1387.35 1300.05,1377.73 1326.13,1397.01 1352.21,1351.24 1378.28,1353.82 1404.36,1361.94 1430.43,1362 1456.51,1350.3 1482.59,1374.53 1508.66,1385.21 1534.74,1386.24 1560.82,1379.5 1586.89,1380.39 1612.97,1384.75 1639.05,1387.68 1665.12,1385.05 1691.2,1403.53 1717.27,1368.46 1743.35,1384.79 1769.43,1383.18 1795.5,1384.05 1821.58,1382.74 1847.66,1393.46 1873.73,1389.89 1899.81,1445.72 1925.89,1399.94 1951.96,1375.46 1978.04,1384.12 2004.11,1390.62 2030.19,1391.66 2056.27,1402.4 2082.34,1407.59 2108.42,1371.16 2134.5,1437.36 2160.57,1435.97 2186.65,1417.23 2212.73,1411.78 2238.8,1417.98 2264.88,1420.87 2290.95,1422.6 \"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"413.457\" cy=\"380.234\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"622.068\" cy=\"1036.29\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"830.679\" cy=\"1214.64\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1039.29\" cy=\"1278.16\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1247.9\" cy=\"1323.98\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1456.51\" cy=\"1350.3\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1665.12\" cy=\"1385.05\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"1873.73\" cy=\"1389.89\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"2082.34\" cy=\"1407.59\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip322)\" cx=\"2290.95\" cy=\"1422.6\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1765.4 250.738 L2279.97 250.738 L2279.97 95.2176 L1765.4 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1765.4,250.738 2279.97,250.738 2279.97,95.2176 1765.4,95.2176 1765.4,250.738 \"/>\n",
       "<polyline clip-path=\"url(#clip320)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.67,147.058 1935.24,147.058 \"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1959.5 128.319 L1963.76 128.319 L1963.76 164.338 L1959.5 164.338 L1959.5 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1982.72 141.398 Q1979.29 141.398 1977.3 144.083 Q1975.31 146.745 1975.31 151.398 Q1975.31 156.051 1977.28 158.736 Q1979.27 161.398 1982.72 161.398 Q1986.12 161.398 1988.11 158.713 Q1990.11 156.027 1990.11 151.398 Q1990.11 146.791 1988.11 144.106 Q1986.12 141.398 1982.72 141.398 M1982.72 137.787 Q1988.28 137.787 1991.45 141.398 Q1994.62 145.009 1994.62 151.398 Q1994.62 157.764 1991.45 161.398 Q1988.28 165.009 1982.72 165.009 Q1977.14 165.009 1973.97 161.398 Q1970.82 157.764 1970.82 151.398 Q1970.82 145.009 1973.97 141.398 Q1977.14 137.787 1982.72 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2018.21 139.176 L2018.21 143.203 Q2016.4 142.277 2014.46 141.815 Q2012.51 141.352 2010.43 141.352 Q2007.26 141.352 2005.66 142.324 Q2004.09 143.296 2004.09 145.24 Q2004.09 146.722 2005.22 147.578 Q2006.35 148.412 2009.78 149.176 L2011.24 149.5 Q2015.78 150.472 2017.67 152.254 Q2019.6 154.014 2019.6 157.185 Q2019.6 160.796 2016.73 162.902 Q2013.88 165.009 2008.88 165.009 Q2006.79 165.009 2004.53 164.592 Q2002.28 164.199 1999.78 163.388 L1999.78 158.99 Q2002.14 160.217 2004.43 160.842 Q2006.73 161.444 2008.97 161.444 Q2011.98 161.444 2013.6 160.426 Q2015.22 159.384 2015.22 157.509 Q2015.22 155.773 2014.04 154.847 Q2012.88 153.921 2008.92 153.064 L2007.44 152.717 Q2003.48 151.884 2001.73 150.171 Q1999.97 148.435 1999.97 145.426 Q1999.97 141.768 2002.56 139.778 Q2005.15 137.787 2009.92 137.787 Q2012.28 137.787 2014.36 138.134 Q2016.45 138.481 2018.21 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2042.91 139.176 L2042.91 143.203 Q2041.1 142.277 2039.16 141.815 Q2037.21 141.352 2035.13 141.352 Q2031.96 141.352 2030.36 142.324 Q2028.79 143.296 2028.79 145.24 Q2028.79 146.722 2029.92 147.578 Q2031.05 148.412 2034.48 149.176 L2035.94 149.5 Q2040.48 150.472 2042.37 152.254 Q2044.29 154.014 2044.29 157.185 Q2044.29 160.796 2041.42 162.902 Q2038.58 165.009 2033.58 165.009 Q2031.49 165.009 2029.23 164.592 Q2026.98 164.199 2024.48 163.388 L2024.48 158.99 Q2026.84 160.217 2029.13 160.842 Q2031.42 161.444 2033.67 161.444 Q2036.68 161.444 2038.3 160.426 Q2039.92 159.384 2039.92 157.509 Q2039.92 155.773 2038.74 154.847 Q2037.58 153.921 2033.62 153.064 L2032.14 152.717 Q2028.18 151.884 2026.42 150.171 Q2024.67 148.435 2024.67 145.426 Q2024.67 141.768 2027.26 139.778 Q2029.85 137.787 2034.62 137.787 Q2036.98 137.787 2039.06 138.134 Q2041.15 138.481 2042.91 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2070.78 172.208 L2070.78 175.518 L2046.15 175.518 L2046.15 172.208 L2070.78 172.208 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2093.39 151.398 Q2093.39 146.699 2091.45 144.037 Q2089.53 141.352 2086.15 141.352 Q2082.77 141.352 2080.82 144.037 Q2078.9 146.699 2078.9 151.398 Q2078.9 156.097 2080.82 158.782 Q2082.77 161.444 2086.15 161.444 Q2089.53 161.444 2091.45 158.782 Q2093.39 156.097 2093.39 151.398 M2078.9 142.347 Q2080.24 140.032 2082.28 138.921 Q2084.34 137.787 2087.19 137.787 Q2091.91 137.787 2094.85 141.537 Q2097.81 145.287 2097.81 151.398 Q2097.81 157.509 2094.85 161.259 Q2091.91 165.009 2087.19 165.009 Q2084.34 165.009 2082.28 163.898 Q2080.24 162.763 2078.9 160.449 L2078.9 164.338 L2074.62 164.338 L2074.62 128.319 L2078.9 128.319 L2078.9 142.347 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2116.66 151.305 Q2111.49 151.305 2109.5 152.486 Q2107.51 153.666 2107.51 156.514 Q2107.51 158.782 2108.99 160.125 Q2110.5 161.444 2113.07 161.444 Q2116.61 161.444 2118.74 158.944 Q2120.89 156.421 2120.89 152.254 L2120.89 151.305 L2116.66 151.305 M2125.15 149.546 L2125.15 164.338 L2120.89 164.338 L2120.89 160.402 Q2119.43 162.763 2117.26 163.898 Q2115.08 165.009 2111.93 165.009 Q2107.95 165.009 2105.59 162.787 Q2103.25 160.541 2103.25 156.791 Q2103.25 152.416 2106.17 150.194 Q2109.11 147.972 2114.92 147.972 L2120.89 147.972 L2120.89 147.555 Q2120.89 144.615 2118.95 143.018 Q2117.03 141.398 2113.53 141.398 Q2111.31 141.398 2109.2 141.93 Q2107.1 142.463 2105.15 143.527 L2105.15 139.592 Q2107.49 138.69 2109.69 138.25 Q2111.89 137.787 2113.97 137.787 Q2119.6 137.787 2122.37 140.703 Q2125.15 143.62 2125.15 149.546 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2138.14 131.051 L2138.14 138.412 L2146.91 138.412 L2146.91 141.722 L2138.14 141.722 L2138.14 155.796 Q2138.14 158.967 2138.99 159.87 Q2139.87 160.773 2142.53 160.773 L2146.91 160.773 L2146.91 164.338 L2142.53 164.338 Q2137.6 164.338 2135.73 162.509 Q2133.85 160.657 2133.85 155.796 L2133.85 141.722 L2130.73 141.722 L2130.73 138.412 L2133.85 138.412 L2133.85 131.051 L2138.14 131.051 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2171.17 139.407 L2171.17 143.389 Q2169.36 142.393 2167.53 141.907 Q2165.73 141.398 2163.88 141.398 Q2159.73 141.398 2157.44 144.037 Q2155.15 146.652 2155.15 151.398 Q2155.15 156.143 2157.44 158.782 Q2159.73 161.398 2163.88 161.398 Q2165.73 161.398 2167.53 160.912 Q2169.36 160.402 2171.17 159.407 L2171.17 163.342 Q2169.39 164.176 2167.47 164.592 Q2165.57 165.009 2163.41 165.009 Q2157.56 165.009 2154.11 161.328 Q2150.66 157.648 2150.66 151.398 Q2150.66 145.055 2154.13 141.421 Q2157.63 137.787 2163.69 137.787 Q2165.66 137.787 2167.53 138.203 Q2169.41 138.597 2171.17 139.407 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2200.13 148.689 L2200.13 164.338 L2195.87 164.338 L2195.87 148.828 Q2195.87 145.148 2194.43 143.319 Q2193 141.49 2190.13 141.49 Q2186.68 141.49 2184.69 143.69 Q2182.7 145.889 2182.7 149.685 L2182.7 164.338 L2178.41 164.338 L2178.41 128.319 L2182.7 128.319 L2182.7 142.44 Q2184.22 140.102 2186.28 138.944 Q2188.37 137.787 2191.08 137.787 Q2195.54 137.787 2197.84 140.565 Q2200.13 143.319 2200.13 148.689 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2230.8 150.31 L2230.8 152.393 L2211.21 152.393 Q2211.49 156.791 2213.85 159.106 Q2216.24 161.398 2220.47 161.398 Q2222.93 161.398 2225.22 160.796 Q2227.53 160.194 2229.8 158.99 L2229.8 163.018 Q2227.51 163.99 2225.1 164.5 Q2222.7 165.009 2220.22 165.009 Q2214.02 165.009 2210.38 161.398 Q2206.77 157.787 2206.77 151.629 Q2206.77 145.264 2210.2 141.537 Q2213.65 137.787 2219.48 137.787 Q2224.71 137.787 2227.74 141.166 Q2230.8 144.523 2230.8 150.31 M2226.54 149.06 Q2226.49 145.565 2224.57 143.481 Q2222.67 141.398 2219.53 141.398 Q2215.96 141.398 2213.81 143.412 Q2211.68 145.426 2211.35 149.083 L2226.54 149.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2254.32 139.176 L2254.32 143.203 Q2252.51 142.277 2250.57 141.815 Q2248.62 141.352 2246.54 141.352 Q2243.37 141.352 2241.77 142.324 Q2240.2 143.296 2240.2 145.24 Q2240.2 146.722 2241.33 147.578 Q2242.46 148.412 2245.89 149.176 L2247.35 149.5 Q2251.89 150.472 2253.78 152.254 Q2255.71 154.014 2255.71 157.185 Q2255.71 160.796 2252.84 162.902 Q2249.99 165.009 2244.99 165.009 Q2242.9 165.009 2240.64 164.592 Q2238.39 164.199 2235.89 163.388 L2235.89 158.99 Q2238.25 160.217 2240.54 160.842 Q2242.84 161.444 2245.08 161.444 Q2248.09 161.444 2249.71 160.426 Q2251.33 159.384 2251.33 157.509 Q2251.33 155.773 2250.15 154.847 Q2248.99 153.921 2245.03 153.064 L2243.55 152.717 Q2239.59 151.884 2237.84 150.171 Q2236.08 148.435 2236.08 145.426 Q2236.08 141.768 2238.67 139.778 Q2241.26 137.787 2246.03 137.787 Q2248.39 137.787 2250.47 138.134 Q2252.56 138.481 2254.32 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip320)\" cx=\"1862.45\" cy=\"198.898\" r=\"20.48\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"4.55111\"/>\n",
       "<path clip-path=\"url(#clip320)\" d=\"M1983.53 202.15 L1983.53 204.233 L1963.95 204.233 Q1964.23 208.631 1966.59 210.946 Q1968.97 213.238 1973.21 213.238 Q1975.66 213.238 1977.95 212.636 Q1980.27 212.034 1982.54 210.83 L1982.54 214.858 Q1980.24 215.83 1977.84 216.34 Q1975.43 216.849 1972.95 216.849 Q1966.75 216.849 1963.11 213.238 Q1959.5 209.627 1959.5 203.469 Q1959.5 197.104 1962.93 193.377 Q1966.38 189.627 1972.21 189.627 Q1977.44 189.627 1980.48 193.006 Q1983.53 196.363 1983.53 202.15 M1979.27 200.9 Q1979.23 197.405 1977.3 195.321 Q1975.41 193.238 1972.26 193.238 Q1968.69 193.238 1966.54 195.252 Q1964.41 197.266 1964.09 200.923 L1979.27 200.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M1994.64 212.289 L1994.64 226.039 L1990.36 226.039 L1990.36 190.252 L1994.64 190.252 L1994.64 194.187 Q1995.98 191.872 1998.02 190.761 Q2000.08 189.627 2002.93 189.627 Q2007.65 189.627 2010.59 193.377 Q2013.55 197.127 2013.55 203.238 Q2013.55 209.349 2010.59 213.099 Q2007.65 216.849 2002.93 216.849 Q2000.08 216.849 1998.02 215.738 Q1995.98 214.603 1994.64 212.289 M2009.13 203.238 Q2009.13 198.539 2007.19 195.877 Q2005.27 193.192 2001.89 193.192 Q1998.51 193.192 1996.56 195.877 Q1994.64 198.539 1994.64 203.238 Q1994.64 207.937 1996.56 210.622 Q1998.51 213.284 2001.89 213.284 Q2005.27 213.284 2007.19 210.622 Q2009.13 207.937 2009.13 203.238 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2030.66 193.238 Q2027.23 193.238 2025.24 195.923 Q2023.25 198.585 2023.25 203.238 Q2023.25 207.891 2025.22 210.576 Q2027.21 213.238 2030.66 213.238 Q2034.06 213.238 2036.05 210.553 Q2038.04 207.867 2038.04 203.238 Q2038.04 198.631 2036.05 195.946 Q2034.06 193.238 2030.66 193.238 M2030.66 189.627 Q2036.22 189.627 2039.39 193.238 Q2042.56 196.849 2042.56 203.238 Q2042.56 209.604 2039.39 213.238 Q2036.22 216.849 2030.66 216.849 Q2025.08 216.849 2021.91 213.238 Q2018.76 209.604 2018.76 203.238 Q2018.76 196.849 2021.91 193.238 Q2025.08 189.627 2030.66 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2068.28 191.247 L2068.28 195.229 Q2066.47 194.233 2064.64 193.747 Q2062.84 193.238 2060.98 193.238 Q2056.84 193.238 2054.55 195.877 Q2052.26 198.492 2052.26 203.238 Q2052.26 207.983 2054.55 210.622 Q2056.84 213.238 2060.98 213.238 Q2062.84 213.238 2064.64 212.752 Q2066.47 212.242 2068.28 211.247 L2068.28 215.182 Q2066.49 216.016 2064.57 216.432 Q2062.67 216.849 2060.52 216.849 Q2054.66 216.849 2051.22 213.168 Q2047.77 209.488 2047.77 203.238 Q2047.77 196.895 2051.24 193.261 Q2054.73 189.627 2060.8 189.627 Q2062.77 189.627 2064.64 190.043 Q2066.52 190.437 2068.28 191.247 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2097.23 200.529 L2097.23 216.178 L2092.97 216.178 L2092.97 200.668 Q2092.97 196.988 2091.54 195.159 Q2090.1 193.33 2087.23 193.33 Q2083.79 193.33 2081.79 195.53 Q2079.8 197.729 2079.8 201.525 L2079.8 216.178 L2075.52 216.178 L2075.52 180.159 L2079.8 180.159 L2079.8 194.28 Q2081.33 191.942 2083.39 190.784 Q2085.47 189.627 2088.18 189.627 Q2092.65 189.627 2094.94 192.405 Q2097.23 195.159 2097.23 200.529 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip320)\" d=\"M2122.26 191.016 L2122.26 195.043 Q2120.45 194.117 2118.51 193.655 Q2116.56 193.192 2114.48 193.192 Q2111.31 193.192 2109.71 194.164 Q2108.14 195.136 2108.14 197.08 Q2108.14 198.562 2109.27 199.418 Q2110.41 200.252 2113.83 201.016 L2115.29 201.34 Q2119.83 202.312 2121.72 204.094 Q2123.65 205.854 2123.65 209.025 Q2123.65 212.636 2120.78 214.742 Q2117.93 216.849 2112.93 216.849 Q2110.85 216.849 2108.58 216.432 Q2106.33 216.039 2103.83 215.228 L2103.83 210.83 Q2106.19 212.057 2108.48 212.682 Q2110.78 213.284 2113.02 213.284 Q2116.03 213.284 2117.65 212.266 Q2119.27 211.224 2119.27 209.349 Q2119.27 207.613 2118.09 206.687 Q2116.93 205.761 2112.97 204.904 L2111.49 204.557 Q2107.53 203.724 2105.78 202.011 Q2104.02 200.275 2104.02 197.266 Q2104.02 193.608 2106.61 191.618 Q2109.2 189.627 2113.97 189.627 Q2116.33 189.627 2118.41 189.974 Q2120.5 190.321 2122.26 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.0025\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=32, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(72, 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model)\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2522c34c-babf-4c8b-85b1-24a2268f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
