{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9beed57c-7db7-4e59-9990-a470390d4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "#@code_warntype, @simd, @inbounds, statyczne typowenie (brak niestabilności typów, brak Any), @benchmark_tools, @btime, brak alokacji, \n",
    "#kolumny czy wiersze w for\n",
    "#test co szybsz, Matrix czy Array{Float64, 2}\n",
    "#softmax backward\n",
    "\n",
    "mutable struct Conv2DNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    W::Array{Float64, 4}\n",
    "    b::Vector{Float64}\n",
    "    batch_size::Int\n",
    "    kernel_size::Tuple{Int, Int}\n",
    "    filters::Int\n",
    "    output::Array{Float64, 4}\n",
    "    gradient_W::Array{Float64, 4}\n",
    "    gradient_b::Vector{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct MaxPoolNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    pool_size::Tuple{Int, Int}\n",
    "    output::Array{Float64, 4}\n",
    "end\n",
    "\n",
    "mutable struct DenseNode <: Node\n",
    "    x::Matrix{Float64}\n",
    "    W::Matrix{Float64} \n",
    "    b::Vector{Float64}\n",
    "    neurons::Int\n",
    "    output::Matrix{Float64}\n",
    "    gradient_W::Matrix{Float64}\n",
    "    gradient_b::Matrix{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct FlattenNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    output::Matrix{Float64}\n",
    "    input_shape::Tuple{Int, Int, Int}\n",
    "end\n",
    "\n",
    "mutable struct ReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct SoftmaxNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    loss_func::Union{Function, Nothing}\n",
    "end\n",
    "\n",
    "mutable struct SigmoidNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    loss_func::Union{Function, Nothing}\n",
    "end\n",
    "\n",
    "mutable struct TanhNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct LeakyReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    alpha::Float64\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8eb86b4-d598-4d00-ab75-f71b4ab219a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 9 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward!(node::Conv2DNode, x::Array{Float64, 4}) \n",
    "    @views node.x = x\n",
    "    W = node.W\n",
    "    b = node.b\n",
    "\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width,  num_chanels, num_filters = size(W)\n",
    "\n",
    "    output_height = 1 + (input_height - filter_height)\n",
    "    output_width = 1 + (input_width - filter_width)\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, num_filters)\n",
    "    for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:output_width, i in 1:output_height\n",
    "                @inbounds output[n, i, j, f] = sum(view(x, n, i:i+filter_height-1, j:j+filter_width-1, :) .* W[:,:,:,f]) + b[f]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    @views node.output = output\n",
    "end\n",
    "\n",
    "function forward!(node::MaxPoolNode, x::Array{Float64, 4})\n",
    "    @views node.x = x\n",
    "    pool_size = node.pool_size\n",
    "    \n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "\n",
    "    output_height = 1 + (input_height - pool_size[1]) ÷ pool_size[1]\n",
    "    output_width = 1 + (input_width - pool_size[2]) ÷ pool_size[2]\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, input_channels)\n",
    "\n",
    "    for n in 1:batch_size\n",
    "        for c in 1:input_channels\n",
    "            @inbounds for j in 1:pool_size[2]:input_width-pool_size[2]+1, i in 1:pool_size[1]:input_height-pool_size[1]+1\n",
    "                @inbounds output[n, 1+div(i-1, pool_size[1]), 1+div(j-1, pool_size[2]), c] = maximum(view(x, n, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @views node.output = output\n",
    "end\n",
    "\n",
    "function forward!(node::FlattenNode, x::Array{Float64, 4})\n",
    "    node.x = x\n",
    "    node.output = reshape(x, size(x, 1), size(x, 2) * size(x, 3) * size(x, 4))\n",
    "end\n",
    "\n",
    "function forward!(node::DenseNode, x::Matrix{Float64})\n",
    "    node.x = x\n",
    "    @views node.output = x * node.W .+ node.b'\n",
    "end\n",
    "\n",
    "function forward!(node::ReLUNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    @views node.output = max.(0, x)\n",
    "end\n",
    "\n",
    "function forward!(node::SoftmaxNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    exp_x = exp.(x .- maximum(x, dims=2))\n",
    "    node.output = exp_x ./ sum(exp_x, dims=2)\n",
    "end\n",
    "\n",
    "function forward!(node::SigmoidNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    node.output = 1.0 ./ (1.0 .+ exp.(-x))\n",
    "end\n",
    "\n",
    "function forward!(node::TanhNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    node.output = tanh.(x)\n",
    "end\n",
    "\n",
    "function forward!(node::LeakyReLUNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    @views node.output = ifelse.(x .> 0, x, node.alpha .* x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602fd274-f243-48fd-874c-9567ce11b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 9 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(node::Conv2DNode, dy::Array{Float64, 4}, x::Array{Float64, 4})\n",
    "    W = node.W\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width, num_channels, num_filters = size(W)\n",
    "\n",
    "    dx = zeros(size(x))\n",
    "    dW = zeros(size(W))\n",
    "    db = zeros(num_filters)\n",
    "\n",
    "    for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:input_width - filter_width + 1, i in 1:input_height - filter_height + 1\n",
    "                @inbounds dx[n, i:i + filter_height - 1, j:j + filter_width - 1, :] .+= W[:, :, :, f] .* dy[n, i, j, f]\n",
    "                @inbounds dW[:, :, :, f] .+= view(x, n, i:i + filter_height - 1, j:j + filter_width - 1, :) .* dy[n, i, j, f]\n",
    "            end\n",
    "            @inbounds db[f] += sum(dy[n, :, :, f])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::MaxPoolNode, dy::Array{Float64, 4}, x::Array{Float64, 4})\n",
    "    pool_size = node.pool_size\n",
    "    new_size = size(node.output)\n",
    "    dy = reshape(dy, new_size)\n",
    "    \n",
    "    batch_size, height, width, channels = size(x)\n",
    "    dx = zeros(size(x))\n",
    "    for b in 1:batch_size\n",
    "        for c in 1:channels\n",
    "            @inbounds for j in 1:pool_size[2]:width-pool_size[2]+1, i in 1:pool_size[1]:height-pool_size[1]+1\n",
    "                @inbounds window = view(x, b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c)\n",
    "                @inbounds dx[b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c] .+= dy[b, i ÷ pool_size[1]+1, j ÷ pool_size[2]+1, c] .* (window .== maximum(window))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::DenseNode, dy::Matrix{Float64}, x::Matrix{Float64})\n",
    "    dx, dW, db = dy * node.W', x' * dy, reshape(sum(dy, dims=1), :, 1)\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "function backward!(node::FlattenNode, dy::Matrix{Float64}, x::Array{Float64, 4})\n",
    "    dx = reshape(dy, size(x, 1), node.input_shape...)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::ReLUNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* (x .> 0)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::SigmoidNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* (node.output .* (1.0 .- node.output))\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::TanhNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* (1.0 .- node.output .^ 2)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::LeakyReLUNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* ifelse.(x .> 0, 1, node.alpha)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::SoftmaxNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    softmax_output = node.output\n",
    "    batch_size, num_classes = size(softmax_output)\n",
    "\n",
    "    dx = zeros(batch_size, num_classes)\n",
    "\n",
    "    for i in 1:batch_size\n",
    "        vec = softmax_output[i,:]\n",
    "        J = diagm(vec) .- vec * vec'\n",
    "        dx[i, :] = J' * dy[i, :]\n",
    "    end\n",
    "    return dx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d202a415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hasproperty (generic function with 4 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Conv2DNode(; batch_size::Int, filters::Int, kernel_size::Tuple{Int, Int}, input_shape::Tuple{Int, Int, Int}, activation::String)\n",
    "    W = randn(kernel_size[1], kernel_size[2], input_shape[3], filters) * sqrt(1 / (kernel_size[1] * kernel_size[2] * input_shape[3]))\n",
    "    b = zeros(filters)\n",
    "\n",
    "    return Conv2DNode(zeros(1,1,1,1), W, b, batch_size, kernel_size, filters, zeros(1,1,1,1), zeros(1, 1, 1, 1), zeros(1), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function MaxPoolNode(; pool_size::Tuple{Int, Int})\n",
    "    return MaxPoolNode(zeros((1, 1, 1, 1)), pool_size, zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function DenseNode(; neurons::Int, activation::String, input_shape::Union{Tuple{Int, Int}, Tuple{Int, Int, Int}})\n",
    "    W = randn(prod(input_shape), neurons) * sqrt(1 / (prod(input_shape)))\n",
    "    b = zeros(neurons)\n",
    "\n",
    "    return DenseNode(zeros(1,1), W, b, neurons, zeros((1,1)), zeros((1, 1)), zeros((1, 1)), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function FlattenNode(; input_shape::Tuple{Int, Int, Int})\n",
    "    return FlattenNode(zeros((input_shape..., 1)), zeros((1, 1)), input_shape)\n",
    "end\n",
    "\n",
    "function ReLUNode()\n",
    "    return ReLUNode(zeros((1, 1, 1, 1)), zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function SoftmaxNode()\n",
    "    return SoftmaxNode(zeros((1, 1)), zeros((1, 1)), nothing)\n",
    "end\n",
    "\n",
    "function TanhNode()\n",
    "    return TanhNode(zeros((1, 1)), zeros((1, 1)))\n",
    "end\n",
    "\n",
    "function SigmoidNode()\n",
    "    return SigmoidNode(zeros((1, 1)), zeros((1, 1)), nothing)\n",
    "end\n",
    "\n",
    "function LeakyReLUNode(alpha::Float64)\n",
    "    return LeakyReLUNode(zeros((1, 1)), zeros((1, 1)), alpha)\n",
    "end\n",
    "\n",
    "function forward_pass!(graph::Vector{Any}, x::Array{Float64, 4})\n",
    "    input = x\n",
    "    for node in graph\n",
    "        @inbounds forward!(node, input)\n",
    "        input = @views node.output\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function backward_pass!(graph::Vector{Any}, y_true::Vector{Int64})\n",
    "    preds = graph[end].output\n",
    "    dout  = graph[end].loss_func(preds, y_true)\n",
    "\n",
    "    for i in reverse(1:length(graph))\n",
    "        layer = graph[i]\n",
    "        prev_layer_output = i > 1 ? graph[i - 1].output : layer.x\n",
    "        if hasproperty(layer, :gradient_W)\n",
    "            @inbounds dout,dW,db = backward!(layer, dout, prev_layer_output)\n",
    "            layer.gradient_W = dW\n",
    "            layer.gradient_b = db\n",
    "\n",
    "        else \n",
    "            @inbounds dout = backward!(layer, dout, prev_layer_output)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function onehotbatch(labels::Vector{Int64}, classes::UnitRange{Int64})\n",
    "    Y = zeros(Int, length(labels), length(classes))\n",
    "    for (i, label) in enumerate(labels)\n",
    "        Y[i, label .== classes] .= 1\n",
    "    end\n",
    "    return Y\n",
    "end\n",
    "\n",
    "function graph_build(layers::Vector{Node}, loss::String)\n",
    "    output = []\n",
    "    for layer in layers\n",
    "        push!(output, layer)\n",
    "        if hasproperty(layer, :activation)\n",
    "            push!(output, getActivation(layer.activation))\n",
    "        end\n",
    "    end\n",
    "    if loss == \"cross_entropy_loss\"\n",
    "        output[end].loss_func = cross_entropy_backward\n",
    "    else\n",
    "        error(\"Invalid activation function for output layer\")\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "function getActivation(name::String)\n",
    "    if name == \"relu\"\n",
    "        return ReLUNode()\n",
    "    elseif name == \"softmax\"\n",
    "        return SoftmaxNode()\n",
    "    elseif name == \"sigmoid\"\n",
    "        return SigmoidNode()\n",
    "    elseif name == \"tanh\"\n",
    "        return TanhNode()\n",
    "    elseif name == \"leakyrelu\"\n",
    "        return LeakyReLUNode(0.01)\n",
    "    else \n",
    "        error(\"no such activation function\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function cross_entropy_backward(y_pred::Matrix{Float64}, y_true::Vector{Int64})\n",
    "    return -(onehotbatch(y_true, 0:9) ./ y_pred) ./ size(y_true, 1)\n",
    "end\n",
    "\n",
    "import Base.hasproperty\n",
    "hasproperty(x, s::Symbol) = s in fieldnames(typeof(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433dfab2-7d04-4fef-a952-6546082e4fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_adam!(layer::Node, learning_rate::Float64, t::Int, beta1::Float64 = 0.9, beta2::Float64 = 0.999, epsilon::Float64 = 1e-8)\n",
    "    layer.m_W = beta1 * layer.m_W + (1 - beta1) * layer.gradient_W\n",
    "    layer.v_W = beta2 * layer.v_W + (1 - beta2) * layer.gradient_W .^ 2\n",
    "    mhat_W = layer.m_W / (1 - beta1 ^ t)\n",
    "    vhat_W = layer.v_W / (1 - beta2 ^ t)\n",
    "    layer.W .-= learning_rate .* mhat_W ./ (sqrt.(vhat_W) .+ epsilon)\n",
    "\n",
    "\n",
    "    layer.m_b = beta1 * layer.m_b + (1 - beta1) * layer.gradient_b\n",
    "    layer.v_b = beta2 * layer.v_b + (1 - beta2) * layer.gradient_b .^ 2\n",
    "    mhat_b = layer.m_b / (1 - beta1 ^ t)\n",
    "    vhat_b = layer.v_b / (1 - beta2 ^ t)\n",
    "    layer.b .-= learning_rate .* mhat_b ./ (sqrt.(vhat_b) .+ epsilon)\n",
    "end\n",
    "\n",
    "function update_sgd!(layer::Node, learning_rate::Float64)\n",
    "    layer.W .-= learning_rate .* layer.gradient_W\n",
    "    layer.b .-= learning_rate .* layer.gradient_b\n",
    "end\n",
    "\n",
    "function train!(graph::Vector{Any}, train_x::Array{Float64, 4}, train_y::Vector{Int64}, epochs::Int=5, batch_size::Int=32, learning_rate::Float64=1e-3, optimizer::String=\"adam\")\n",
    "    total_batches = ceil(Int, size(train_x, 1) / batch_size)\n",
    "    each = ceil(Int, total_batches/10)\n",
    " \n",
    "    loss_history = zeros(ceil(Int, total_batches*epochs/each))\n",
    "    counter = 1\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        println(\"----EPOCH $epoch----\")\n",
    "\n",
    "        idx = randperm(size(train_x, 1))\n",
    "        train_x = train_x[idx, :, :, :]\n",
    "        train_y = train_y[idx]\n",
    "        loss = 0\n",
    "        current_epoch_loss = zeros(total_batches)\n",
    "        \n",
    "\n",
    "        for i in 1:batch_size:size(train_x, 1)\n",
    "            x_batch = train_x[i:min(i+batch_size-1, end), :, :, :]\n",
    "            y_batch = train_y[i:min(i+batch_size-1, end)]\n",
    "            tmp  = (i ÷ batch_size)\n",
    "\n",
    "            forward_pass!(graph, x_batch)\n",
    "            \n",
    "            current_epoch_loss[tmp+1] = -sum(log.(graph[end].output) .* onehotbatch(y_batch, 0:9))#/size(y_batch, 1)\n",
    "            \n",
    "            backward_pass!(graph, y_batch)\n",
    "\n",
    "            for layer in graph\n",
    "                if hasproperty(layer, :gradient_W)\n",
    "                    if optimizer == \"adam\"\n",
    "                        update_adam!(layer, learning_rate, i)\n",
    "                    elseif optimizer == \"sgd\"\n",
    "                        update_sgd!(layer, learning_rate)\n",
    "                    else\n",
    "                        error(\"Invalid optimizer selected.\")\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            \n",
    "            if i == 1  ||  tmp % each == 0\n",
    "                avg_loss = sum(current_epoch_loss)/(tmp+1)\n",
    "\n",
    "                loss_history[counter] = avg_loss\n",
    "                counter += 1\n",
    "                print(i ÷ batch_size, \"/\", total_batches)\n",
    "                println(\" avg epoch loss: \", round(avg_loss, digits=5))\n",
    "                \n",
    "            end\n",
    "        end\n",
    "        println()\n",
    "        \n",
    "    end\n",
    "    println(\"Training finished!\")\n",
    "\n",
    "    return loss_history\n",
    "end\n",
    "\n",
    "function evaluate(graph::Vector{Any}, x_data::Array{Float64, 4}, y_data::Vector{Int64}, batch_size::Int, pool_size::Tuple{Int, Int})\n",
    "    num_correct = 0\n",
    "    num_samples = size(x_data, 1)\n",
    "\n",
    "    for i in 1:batch_size:num_samples\n",
    "        x_batch = x_data[i:min(i+batch_size-1, end), :, :, :]\n",
    "        y_batch = y_data[i:min(i+batch_size-1, end)]\n",
    "        \n",
    "        forward_pass!(graph, x_batch)\n",
    "        pred = graph[end].output\n",
    "        \n",
    "        predictions = argmax.(eachrow(pred)) .- 1\n",
    "        num_correct += sum(predictions .== y_batch)\n",
    "    end\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "    return accuracy\n",
    "end\n",
    "\n",
    "function plot_loss(loss::Vector{Float64}, epochs::Int)\n",
    "    plot(1:size(loss,1), loss, label=\"loss_batches\")\n",
    "\n",
    "    each = Int64(size(loss,1)/epochs)\n",
    "    pts = loss[each:each:end]\n",
    "    plot!(each:each:each*size(pts,1), pts, seriestype=:scatter, label=\"epochs\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5e96f3-f2cb-47d1-a14d-8c9d84c3bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "mnist = pyimport(\"mnist\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_data = mnist.train_images()\n",
    "train_labels = Int64.(mnist.train_labels())\n",
    "train_x =  reshape(train_data, :, 28, 28, 1) / 255.0\n",
    "train_y = train_labels\n",
    "\n",
    "\n",
    "n = 10000\n",
    "idx = randperm(n)\n",
    "train_x = train_x[idx, :,:,:]\n",
    "train_y = train_y[idx]\n",
    "\n",
    "train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b287c46-20e9-435c-95e2-23d699c4eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "0/157 avg epoch loss: 170.48247\n",
      "16/157 avg epoch loss: 112.24467\n",
      "32/157 avg epoch loss: 84.87817\n",
      "48/157 avg epoch loss: 73.01851\n",
      "64/157 avg epoch loss: 65.61729\n",
      "80/157 avg epoch loss: 59.05118\n",
      "96/157 avg epoch loss: 54.8643\n",
      "112/157 avg epoch loss: 52.44928\n",
      "128/157 avg epoch loss: 49.85713\n",
      "144/157 avg epoch loss: 47.56724\n",
      "\n",
      "----EPOCH 2----\n",
      "0/157 avg epoch loss: 36.24274\n",
      "16/157 avg epoch loss: 26.57344\n",
      "32/157 avg epoch loss: 27.00391\n",
      "48/157 avg epoch loss: 27.7879\n",
      "64/157 avg epoch loss: 27.6629\n",
      "80/157 avg epoch loss: 27.10073\n",
      "96/157 avg epoch loss: 27.24066\n",
      "112/157 avg epoch loss: 26.75936\n",
      "128/157 avg epoch loss: 26.08822\n",
      "144/157 avg epoch loss: 25.61335\n",
      "\n",
      "----EPOCH 3----\n",
      "0/157 avg epoch loss: 9.34676\n",
      "16/157 avg epoch loss: 20.17001\n",
      "32/157 avg epoch loss: 20.77845\n",
      "48/157 avg epoch loss: 20.68021\n",
      "64/157 avg epoch loss: 21.52027\n",
      "80/157 avg epoch loss: 21.22869\n",
      "96/157 avg epoch loss: 21.49109\n",
      "112/157 avg epoch loss: 21.4078\n",
      "128/157 avg epoch loss: 21.38065\n",
      "144/157 avg epoch loss: 21.53665\n",
      "\n",
      "Training finished!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip560\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip561\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M169.121 1486.45 L2352.76 1486.45 L2352.76 47.2441 L169.121 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip562\">\n",
       "    <rect x=\"169\" y=\"47\" width=\"2185\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"515.065,1486.45 515.065,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"870.243,1486.45 870.243,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1225.42,1486.45 1225.42,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1580.6,1486.45 1580.6,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1935.78,1486.45 1935.78,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2290.95,1486.45 2290.95,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"515.065,1486.45 515.065,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"870.243,1486.45 870.243,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1225.42,1486.45 1225.42,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1580.6,1486.45 1580.6,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1935.78,1486.45 1935.78,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2290.95,1486.45 2290.95,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M505.343 1514.29 L523.699 1514.29 L523.699 1518.22 L509.625 1518.22 L509.625 1526.7 Q510.643 1526.35 511.662 1526.19 Q512.68 1526 513.699 1526 Q519.486 1526 522.866 1529.17 Q526.245 1532.34 526.245 1537.76 Q526.245 1543.34 522.773 1546.44 Q519.301 1549.52 512.981 1549.52 Q510.805 1549.52 508.537 1549.15 Q506.292 1548.78 503.884 1548.04 L503.884 1543.34 Q505.968 1544.47 508.19 1545.03 Q510.412 1545.58 512.889 1545.58 Q516.893 1545.58 519.231 1543.48 Q521.569 1541.37 521.569 1537.76 Q521.569 1534.15 519.231 1532.04 Q516.893 1529.94 512.889 1529.94 Q511.014 1529.94 509.139 1530.35 Q507.287 1530.77 505.343 1531.65 L505.343 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M844.93 1544.91 L852.569 1544.91 L852.569 1518.55 L844.259 1520.21 L844.259 1515.95 L852.523 1514.29 L857.199 1514.29 L857.199 1544.91 L864.838 1544.91 L864.838 1548.85 L844.93 1548.85 L844.93 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M884.282 1517.37 Q880.671 1517.37 878.842 1520.93 Q877.037 1524.47 877.037 1531.6 Q877.037 1538.71 878.842 1542.27 Q880.671 1545.82 884.282 1545.82 Q887.916 1545.82 889.722 1542.27 Q891.551 1538.71 891.551 1531.6 Q891.551 1524.47 889.722 1520.93 Q887.916 1517.37 884.282 1517.37 M884.282 1513.66 Q890.092 1513.66 893.148 1518.27 Q896.226 1522.85 896.226 1531.6 Q896.226 1540.33 893.148 1544.94 Q890.092 1549.52 884.282 1549.52 Q878.472 1549.52 875.393 1544.94 Q872.338 1540.33 872.338 1531.6 Q872.338 1522.85 875.393 1518.27 Q878.472 1513.66 884.282 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1200.61 1544.91 L1208.24 1544.91 L1208.24 1518.55 L1199.93 1520.21 L1199.93 1515.95 L1208.2 1514.29 L1212.87 1514.29 L1212.87 1544.91 L1220.51 1544.91 L1220.51 1548.85 L1200.61 1548.85 L1200.61 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1230 1514.29 L1248.36 1514.29 L1248.36 1518.22 L1234.29 1518.22 L1234.29 1526.7 Q1235.31 1526.35 1236.32 1526.19 Q1237.34 1526 1238.36 1526 Q1244.15 1526 1247.53 1529.17 Q1250.91 1532.34 1250.91 1537.76 Q1250.91 1543.34 1247.43 1546.44 Q1243.96 1549.52 1237.64 1549.52 Q1235.47 1549.52 1233.2 1549.15 Q1230.95 1548.78 1228.55 1548.04 L1228.55 1543.34 Q1230.63 1544.47 1232.85 1545.03 Q1235.07 1545.58 1237.55 1545.58 Q1241.55 1545.58 1243.89 1543.48 Q1246.23 1541.37 1246.23 1537.76 Q1246.23 1534.15 1243.89 1532.04 Q1241.55 1529.94 1237.55 1529.94 Q1235.68 1529.94 1233.8 1530.35 Q1231.95 1530.77 1230 1531.65 L1230 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1559.37 1544.91 L1575.69 1544.91 L1575.69 1548.85 L1553.75 1548.85 L1553.75 1544.91 Q1556.41 1542.16 1560.99 1537.53 Q1565.6 1532.88 1566.78 1531.53 Q1569.02 1529.01 1569.9 1527.27 Q1570.81 1525.51 1570.81 1523.82 Q1570.81 1521.07 1568.86 1519.33 Q1566.94 1517.6 1563.84 1517.6 Q1561.64 1517.6 1559.19 1518.36 Q1556.76 1519.13 1553.98 1520.68 L1553.98 1515.95 Q1556.8 1514.82 1559.26 1514.24 Q1561.71 1513.66 1563.75 1513.66 Q1569.12 1513.66 1572.31 1516.35 Q1575.51 1519.03 1575.51 1523.52 Q1575.51 1525.65 1574.7 1527.57 Q1573.91 1529.47 1571.8 1532.07 Q1571.22 1532.74 1568.12 1535.95 Q1565.02 1539.15 1559.37 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1595.51 1517.37 Q1591.9 1517.37 1590.07 1520.93 Q1588.26 1524.47 1588.26 1531.6 Q1588.26 1538.71 1590.07 1542.27 Q1591.9 1545.82 1595.51 1545.82 Q1599.14 1545.82 1600.95 1542.27 Q1602.77 1538.71 1602.77 1531.6 Q1602.77 1524.47 1600.95 1520.93 Q1599.14 1517.37 1595.51 1517.37 M1595.51 1513.66 Q1601.32 1513.66 1604.37 1518.27 Q1607.45 1522.85 1607.45 1531.6 Q1607.45 1540.33 1604.37 1544.94 Q1601.32 1549.52 1595.51 1549.52 Q1589.7 1549.52 1586.62 1544.94 Q1583.56 1540.33 1583.56 1531.6 Q1583.56 1522.85 1586.62 1518.27 Q1589.7 1513.66 1595.51 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1915.05 1544.91 L1931.37 1544.91 L1931.37 1548.85 L1909.42 1548.85 L1909.42 1544.91 Q1912.08 1542.16 1916.67 1537.53 Q1921.27 1532.88 1922.46 1531.53 Q1924.7 1529.01 1925.58 1527.27 Q1926.48 1525.51 1926.48 1523.82 Q1926.48 1521.07 1924.54 1519.33 Q1922.62 1517.6 1919.52 1517.6 Q1917.32 1517.6 1914.86 1518.36 Q1912.43 1519.13 1909.65 1520.68 L1909.65 1515.95 Q1912.48 1514.82 1914.93 1514.24 Q1917.39 1513.66 1919.42 1513.66 Q1924.79 1513.66 1927.99 1516.35 Q1931.18 1519.03 1931.18 1523.52 Q1931.18 1525.65 1930.37 1527.57 Q1929.58 1529.47 1927.48 1532.07 Q1926.9 1532.74 1923.8 1535.95 Q1920.7 1539.15 1915.05 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1941.23 1514.29 L1959.58 1514.29 L1959.58 1518.22 L1945.51 1518.22 L1945.51 1526.7 Q1946.53 1526.35 1947.55 1526.19 Q1948.57 1526 1949.58 1526 Q1955.37 1526 1958.75 1529.17 Q1962.13 1532.34 1962.13 1537.76 Q1962.13 1543.34 1958.66 1546.44 Q1955.19 1549.52 1948.87 1549.52 Q1946.69 1549.52 1944.42 1549.15 Q1942.18 1548.78 1939.77 1548.04 L1939.77 1543.34 Q1941.85 1544.47 1944.08 1545.03 Q1946.3 1545.58 1948.77 1545.58 Q1952.78 1545.58 1955.12 1543.48 Q1957.46 1541.37 1957.46 1537.76 Q1957.46 1534.15 1955.12 1532.04 Q1952.78 1529.94 1948.77 1529.94 Q1946.9 1529.94 1945.02 1530.35 Q1943.17 1530.77 1941.23 1531.65 L1941.23 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2279.8 1530.21 Q2283.15 1530.93 2285.03 1533.2 Q2286.93 1535.47 2286.93 1538.8 Q2286.93 1543.92 2283.41 1546.72 Q2279.89 1549.52 2273.41 1549.52 Q2271.23 1549.52 2268.92 1549.08 Q2266.63 1548.66 2264.17 1547.81 L2264.17 1543.29 Q2266.12 1544.43 2268.43 1545.01 Q2270.75 1545.58 2273.27 1545.58 Q2277.67 1545.58 2279.96 1543.85 Q2282.27 1542.11 2282.27 1538.8 Q2282.27 1535.75 2280.12 1534.03 Q2277.99 1532.3 2274.17 1532.3 L2270.14 1532.3 L2270.14 1528.45 L2274.36 1528.45 Q2277.81 1528.45 2279.64 1527.09 Q2281.46 1525.7 2281.46 1523.11 Q2281.46 1520.45 2279.57 1519.03 Q2277.69 1517.6 2274.17 1517.6 Q2272.25 1517.6 2270.05 1518.01 Q2267.85 1518.43 2265.21 1519.31 L2265.21 1515.14 Q2267.88 1514.4 2270.19 1514.03 Q2272.53 1513.66 2274.59 1513.66 Q2279.91 1513.66 2283.02 1516.09 Q2286.12 1518.5 2286.12 1522.62 Q2286.12 1525.49 2284.47 1527.48 Q2282.83 1529.45 2279.8 1530.21 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2305.79 1517.37 Q2302.18 1517.37 2300.35 1520.93 Q2298.55 1524.47 2298.55 1531.6 Q2298.55 1538.71 2300.35 1542.27 Q2302.18 1545.82 2305.79 1545.82 Q2309.43 1545.82 2311.23 1542.27 Q2313.06 1538.71 2313.06 1531.6 Q2313.06 1524.47 2311.23 1520.93 Q2309.43 1517.37 2305.79 1517.37 M2305.79 1513.66 Q2311.6 1513.66 2314.66 1518.27 Q2317.74 1522.85 2317.74 1531.6 Q2317.74 1540.33 2314.66 1544.94 Q2311.6 1549.52 2305.79 1549.52 Q2299.98 1549.52 2296.9 1544.94 Q2293.85 1540.33 2293.85 1531.6 Q2293.85 1522.85 2296.9 1518.27 Q2299.98 1513.66 2305.79 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,1271.69 2352.76,1271.69 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,1018.91 2352.76,1018.91 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,766.127 2352.76,766.127 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,513.345 2352.76,513.345 \"/>\n",
       "<polyline clip-path=\"url(#clip562)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"169.121,260.563 2352.76,260.563 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1486.45 169.121,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1271.69 188.019,1271.69 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,1018.91 188.019,1018.91 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,766.127 188.019,766.127 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,513.345 188.019,513.345 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"169.121,260.563 188.019,260.563 \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M95.1817 1270.34 Q98.5382 1271.05 100.413 1273.32 Q102.311 1275.59 102.311 1278.92 Q102.311 1284.04 98.7928 1286.84 Q95.2743 1289.64 88.7928 1289.64 Q86.6169 1289.64 84.3021 1289.2 Q82.0105 1288.79 79.5568 1287.93 L79.5568 1283.41 Q81.5012 1284.55 83.816 1285.13 Q86.1308 1285.71 88.654 1285.71 Q93.0521 1285.71 95.3437 1283.97 Q97.6585 1282.23 97.6585 1278.92 Q97.6585 1275.87 95.5058 1274.16 Q93.3762 1272.42 89.5567 1272.42 L85.529 1272.42 L85.529 1268.58 L89.7419 1268.58 Q93.191 1268.58 95.0197 1267.21 Q96.8484 1265.82 96.8484 1263.23 Q96.8484 1260.57 94.9502 1259.16 Q93.0752 1257.72 89.5567 1257.72 Q87.6354 1257.72 85.4364 1258.14 Q83.2373 1258.55 80.5984 1259.43 L80.5984 1255.27 Q83.2605 1254.53 85.5753 1254.16 Q87.9132 1253.79 89.9734 1253.79 Q95.2974 1253.79 98.3993 1256.22 Q101.501 1258.62 101.501 1262.74 Q101.501 1265.61 99.8576 1267.6 Q98.2141 1269.57 95.1817 1270.34 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M121.177 1257.49 Q117.566 1257.49 115.737 1261.05 Q113.932 1264.6 113.932 1271.72 Q113.932 1278.83 115.737 1282.4 Q117.566 1285.94 121.177 1285.94 Q124.811 1285.94 126.617 1282.4 Q128.445 1278.83 128.445 1271.72 Q128.445 1264.6 126.617 1261.05 Q124.811 1257.49 121.177 1257.49 M121.177 1253.79 Q126.987 1253.79 130.043 1258.39 Q133.121 1262.97 133.121 1271.72 Q133.121 1280.45 130.043 1285.06 Q126.987 1289.64 121.177 1289.64 Q115.367 1289.64 112.288 1285.06 Q109.233 1280.45 109.233 1271.72 Q109.233 1262.97 112.288 1258.39 Q115.367 1253.79 121.177 1253.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M91.5938 1017.04 Q88.4456 1017.04 86.5938 1019.2 Q84.7651 1021.35 84.7651 1025.1 Q84.7651 1028.83 86.5938 1031 Q88.4456 1033.16 91.5938 1033.16 Q94.7419 1033.16 96.5706 1031 Q98.4224 1028.83 98.4224 1025.1 Q98.4224 1021.35 96.5706 1019.2 Q94.7419 1017.04 91.5938 1017.04 M100.876 1002.39 L100.876 1006.65 Q99.1169 1005.82 97.3113 1005.38 Q95.5289 1004.94 93.7697 1004.94 Q89.1401 1004.94 86.6864 1008.06 Q84.2558 1011.19 83.9086 1017.51 Q85.2743 1015.49 87.3345 1014.43 Q89.3947 1013.34 91.8715 1013.34 Q97.0798 1013.34 100.089 1016.51 Q103.121 1019.66 103.121 1025.1 Q103.121 1030.42 99.9733 1033.64 Q96.8252 1036.86 91.5938 1036.86 Q85.5984 1036.86 82.4271 1032.28 Q79.2559 1027.67 79.2559 1018.94 Q79.2559 1010.75 83.1447 1005.89 Q87.0336 1001 93.5845 1001 Q95.3437 1001 97.1261 1001.35 Q98.9317 1001.7 100.876 1002.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M121.177 1004.71 Q117.566 1004.71 115.737 1008.27 Q113.932 1011.81 113.932 1018.94 Q113.932 1026.05 115.737 1029.61 Q117.566 1033.16 121.177 1033.16 Q124.811 1033.16 126.617 1029.61 Q128.445 1026.05 128.445 1018.94 Q128.445 1011.81 126.617 1008.27 Q124.811 1004.71 121.177 1004.71 M121.177 1001 Q126.987 1001 130.043 1005.61 Q133.121 1010.19 133.121 1018.94 Q133.121 1027.67 130.043 1032.28 Q126.987 1036.86 121.177 1036.86 Q115.367 1036.86 112.288 1032.28 Q109.233 1027.67 109.233 1018.94 Q109.233 1010.19 112.288 1005.61 Q115.367 1001 121.177 1001 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M81.154 782.689 L81.154 778.43 Q82.9133 779.263 84.7188 779.703 Q86.5243 780.143 88.2604 780.143 Q92.89 780.143 95.3206 777.041 Q97.7743 773.916 98.1215 767.573 Q96.7789 769.564 94.7187 770.629 Q92.6586 771.694 90.1586 771.694 Q84.9734 771.694 81.941 768.569 Q78.9318 765.42 78.9318 759.981 Q78.9318 754.657 82.0799 751.439 Q85.2281 748.222 90.4595 748.222 Q96.4548 748.222 99.603 752.828 Q102.774 757.411 102.774 766.161 Q102.774 774.332 98.8854 779.217 Q95.0197 784.078 88.4688 784.078 Q86.7095 784.078 84.904 783.731 Q83.0984 783.383 81.154 782.689 M90.4595 768.036 Q93.6076 768.036 95.4363 765.883 Q97.2882 763.731 97.2882 759.981 Q97.2882 756.254 95.4363 754.101 Q93.6076 751.925 90.4595 751.925 Q87.3114 751.925 85.4595 754.101 Q83.6308 756.254 83.6308 759.981 Q83.6308 763.731 85.4595 765.883 Q87.3114 768.036 90.4595 768.036 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M121.177 751.925 Q117.566 751.925 115.737 755.49 Q113.932 759.032 113.932 766.161 Q113.932 773.268 115.737 776.832 Q117.566 780.374 121.177 780.374 Q124.811 780.374 126.617 776.832 Q128.445 773.268 128.445 766.161 Q128.445 759.032 126.617 755.49 Q124.811 751.925 121.177 751.925 M121.177 748.222 Q126.987 748.222 130.043 752.828 Q133.121 757.411 133.121 766.161 Q133.121 774.888 130.043 779.494 Q126.987 784.078 121.177 784.078 Q115.367 784.078 112.288 779.494 Q109.233 774.888 109.233 766.161 Q109.233 757.411 112.288 752.828 Q115.367 748.222 121.177 748.222 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M51.6634 526.689 L59.3023 526.689 L59.3023 500.324 L50.9921 501.991 L50.9921 497.731 L59.256 496.065 L63.9319 496.065 L63.9319 526.689 L71.5707 526.689 L71.5707 530.625 L51.6634 530.625 L51.6634 526.689 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M85.0429 526.689 L101.362 526.689 L101.362 530.625 L79.4179 530.625 L79.4179 526.689 Q82.0799 523.935 86.6632 519.305 Q91.2697 514.653 92.4502 513.31 Q94.6956 510.787 95.5752 509.051 Q96.478 507.291 96.478 505.602 Q96.478 502.847 94.5336 501.111 Q92.6123 499.375 89.5104 499.375 Q87.3114 499.375 84.8577 500.139 Q82.4271 500.903 79.6494 502.454 L79.6494 497.731 Q82.4734 496.597 84.9271 496.018 Q87.3808 495.44 89.4178 495.44 Q94.7882 495.44 97.9826 498.125 Q101.177 500.81 101.177 505.301 Q101.177 507.43 100.367 509.352 Q99.5798 511.25 97.4734 513.842 Q96.8947 514.514 93.7928 517.731 Q90.691 520.926 85.0429 526.689 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M121.177 499.143 Q117.566 499.143 115.737 502.708 Q113.932 506.25 113.932 513.379 Q113.932 520.486 115.737 524.051 Q117.566 527.592 121.177 527.592 Q124.811 527.592 126.617 524.051 Q128.445 520.486 128.445 513.379 Q128.445 506.25 126.617 502.708 Q124.811 499.143 121.177 499.143 M121.177 495.44 Q126.987 495.44 130.043 500.046 Q133.121 504.629 133.121 513.379 Q133.121 522.106 130.043 526.713 Q126.987 531.296 121.177 531.296 Q115.367 531.296 112.288 526.713 Q109.233 522.106 109.233 513.379 Q109.233 504.629 112.288 500.046 Q115.367 495.44 121.177 495.44 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M51.6634 273.908 L59.3023 273.908 L59.3023 247.542 L50.9921 249.209 L50.9921 244.949 L59.256 243.283 L63.9319 243.283 L63.9319 273.908 L71.5707 273.908 L71.5707 277.843 L51.6634 277.843 L51.6634 273.908 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M81.0614 243.283 L99.4178 243.283 L99.4178 247.218 L85.3438 247.218 L85.3438 255.69 Q86.3623 255.343 87.3808 255.181 Q88.3993 254.996 89.4178 254.996 Q95.2049 254.996 98.5845 258.167 Q101.964 261.338 101.964 266.755 Q101.964 272.334 98.4919 275.435 Q95.0197 278.514 88.7003 278.514 Q86.5243 278.514 84.2558 278.144 Q82.0105 277.773 79.6031 277.033 L79.6031 272.334 Q81.6864 273.468 83.9086 274.023 Q86.1308 274.579 88.6077 274.579 Q92.6123 274.579 94.9502 272.472 Q97.2882 270.366 97.2882 266.755 Q97.2882 263.144 94.9502 261.037 Q92.6123 258.931 88.6077 258.931 Q86.7327 258.931 84.8577 259.348 Q83.0058 259.764 81.0614 260.644 L81.0614 243.283 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M121.177 246.362 Q117.566 246.362 115.737 249.926 Q113.932 253.468 113.932 260.598 Q113.932 267.704 115.737 271.269 Q117.566 274.81 121.177 274.81 Q124.811 274.81 126.617 271.269 Q128.445 267.704 128.445 260.598 Q128.445 253.468 126.617 249.926 Q124.811 246.362 121.177 246.362 M121.177 242.658 Q126.987 242.658 130.043 247.264 Q133.121 251.848 133.121 260.598 Q133.121 269.324 130.043 273.931 Q126.987 278.514 121.177 278.514 Q115.367 278.514 112.288 273.931 Q109.233 269.324 109.233 260.598 Q109.233 251.848 112.288 247.264 Q115.367 242.658 121.177 242.658 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip562)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"230.922,87.9763 301.958,578.692 372.994,809.283 444.029,909.214 515.065,971.577 586.1,1026.9 657.136,1062.18 728.172,1082.53 799.207,1104.37 870.243,1123.67 941.278,1219.09 1012.31,1300.56 1083.35,1296.94 1154.39,1290.33 1225.42,1291.38 1296.46,1296.12 1367.49,1294.94 1438.53,1299 1509.56,1304.65 1580.6,1308.65 1651.63,1445.72 1722.67,1354.52 1793.71,1349.39 1864.74,1350.22 1935.78,1343.14 2006.81,1345.6 2077.85,1343.39 2148.88,1344.09 2219.92,1344.32 2290.95,1343 \"/>\n",
       "<circle clip-path=\"url(#clip562)\" cx=\"870.243\" cy=\"1123.67\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip562)\" cx=\"1580.6\" cy=\"1308.65\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip562)\" cx=\"2290.95\" cy=\"1343\" r=\"14.4\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M1765.4 250.738 L2279.97 250.738 L2279.97 95.2176 L1765.4 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1765.4,250.738 2279.97,250.738 2279.97,95.2176 1765.4,95.2176 1765.4,250.738 \"/>\n",
       "<polyline clip-path=\"url(#clip560)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.67,147.058 1935.24,147.058 \"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M1959.5 128.319 L1963.76 128.319 L1963.76 164.338 L1959.5 164.338 L1959.5 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1982.72 141.398 Q1979.29 141.398 1977.3 144.083 Q1975.31 146.745 1975.31 151.398 Q1975.31 156.051 1977.28 158.736 Q1979.27 161.398 1982.72 161.398 Q1986.12 161.398 1988.11 158.713 Q1990.11 156.027 1990.11 151.398 Q1990.11 146.791 1988.11 144.106 Q1986.12 141.398 1982.72 141.398 M1982.72 137.787 Q1988.28 137.787 1991.45 141.398 Q1994.62 145.009 1994.62 151.398 Q1994.62 157.764 1991.45 161.398 Q1988.28 165.009 1982.72 165.009 Q1977.14 165.009 1973.97 161.398 Q1970.82 157.764 1970.82 151.398 Q1970.82 145.009 1973.97 141.398 Q1977.14 137.787 1982.72 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2018.21 139.176 L2018.21 143.203 Q2016.4 142.277 2014.46 141.815 Q2012.51 141.352 2010.43 141.352 Q2007.26 141.352 2005.66 142.324 Q2004.09 143.296 2004.09 145.24 Q2004.09 146.722 2005.22 147.578 Q2006.35 148.412 2009.78 149.176 L2011.24 149.5 Q2015.78 150.472 2017.67 152.254 Q2019.6 154.014 2019.6 157.185 Q2019.6 160.796 2016.73 162.902 Q2013.88 165.009 2008.88 165.009 Q2006.79 165.009 2004.53 164.592 Q2002.28 164.199 1999.78 163.388 L1999.78 158.99 Q2002.14 160.217 2004.43 160.842 Q2006.73 161.444 2008.97 161.444 Q2011.98 161.444 2013.6 160.426 Q2015.22 159.384 2015.22 157.509 Q2015.22 155.773 2014.04 154.847 Q2012.88 153.921 2008.92 153.064 L2007.44 152.717 Q2003.48 151.884 2001.73 150.171 Q1999.97 148.435 1999.97 145.426 Q1999.97 141.768 2002.56 139.778 Q2005.15 137.787 2009.92 137.787 Q2012.28 137.787 2014.36 138.134 Q2016.45 138.481 2018.21 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2042.91 139.176 L2042.91 143.203 Q2041.1 142.277 2039.16 141.815 Q2037.21 141.352 2035.13 141.352 Q2031.96 141.352 2030.36 142.324 Q2028.79 143.296 2028.79 145.24 Q2028.79 146.722 2029.92 147.578 Q2031.05 148.412 2034.48 149.176 L2035.94 149.5 Q2040.48 150.472 2042.37 152.254 Q2044.29 154.014 2044.29 157.185 Q2044.29 160.796 2041.42 162.902 Q2038.58 165.009 2033.58 165.009 Q2031.49 165.009 2029.23 164.592 Q2026.98 164.199 2024.48 163.388 L2024.48 158.99 Q2026.84 160.217 2029.13 160.842 Q2031.42 161.444 2033.67 161.444 Q2036.68 161.444 2038.3 160.426 Q2039.92 159.384 2039.92 157.509 Q2039.92 155.773 2038.74 154.847 Q2037.58 153.921 2033.62 153.064 L2032.14 152.717 Q2028.18 151.884 2026.42 150.171 Q2024.67 148.435 2024.67 145.426 Q2024.67 141.768 2027.26 139.778 Q2029.85 137.787 2034.62 137.787 Q2036.98 137.787 2039.06 138.134 Q2041.15 138.481 2042.91 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2070.78 172.208 L2070.78 175.518 L2046.15 175.518 L2046.15 172.208 L2070.78 172.208 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2093.39 151.398 Q2093.39 146.699 2091.45 144.037 Q2089.53 141.352 2086.15 141.352 Q2082.77 141.352 2080.82 144.037 Q2078.9 146.699 2078.9 151.398 Q2078.9 156.097 2080.82 158.782 Q2082.77 161.444 2086.15 161.444 Q2089.53 161.444 2091.45 158.782 Q2093.39 156.097 2093.39 151.398 M2078.9 142.347 Q2080.24 140.032 2082.28 138.921 Q2084.34 137.787 2087.19 137.787 Q2091.91 137.787 2094.85 141.537 Q2097.81 145.287 2097.81 151.398 Q2097.81 157.509 2094.85 161.259 Q2091.91 165.009 2087.19 165.009 Q2084.34 165.009 2082.28 163.898 Q2080.24 162.763 2078.9 160.449 L2078.9 164.338 L2074.62 164.338 L2074.62 128.319 L2078.9 128.319 L2078.9 142.347 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2116.66 151.305 Q2111.49 151.305 2109.5 152.486 Q2107.51 153.666 2107.51 156.514 Q2107.51 158.782 2108.99 160.125 Q2110.5 161.444 2113.07 161.444 Q2116.61 161.444 2118.74 158.944 Q2120.89 156.421 2120.89 152.254 L2120.89 151.305 L2116.66 151.305 M2125.15 149.546 L2125.15 164.338 L2120.89 164.338 L2120.89 160.402 Q2119.43 162.763 2117.26 163.898 Q2115.08 165.009 2111.93 165.009 Q2107.95 165.009 2105.59 162.787 Q2103.25 160.541 2103.25 156.791 Q2103.25 152.416 2106.17 150.194 Q2109.11 147.972 2114.92 147.972 L2120.89 147.972 L2120.89 147.555 Q2120.89 144.615 2118.95 143.018 Q2117.03 141.398 2113.53 141.398 Q2111.31 141.398 2109.2 141.93 Q2107.1 142.463 2105.15 143.527 L2105.15 139.592 Q2107.49 138.69 2109.69 138.25 Q2111.89 137.787 2113.97 137.787 Q2119.6 137.787 2122.37 140.703 Q2125.15 143.62 2125.15 149.546 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2138.14 131.051 L2138.14 138.412 L2146.91 138.412 L2146.91 141.722 L2138.14 141.722 L2138.14 155.796 Q2138.14 158.967 2138.99 159.87 Q2139.87 160.773 2142.53 160.773 L2146.91 160.773 L2146.91 164.338 L2142.53 164.338 Q2137.6 164.338 2135.73 162.509 Q2133.85 160.657 2133.85 155.796 L2133.85 141.722 L2130.73 141.722 L2130.73 138.412 L2133.85 138.412 L2133.85 131.051 L2138.14 131.051 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2171.17 139.407 L2171.17 143.389 Q2169.36 142.393 2167.53 141.907 Q2165.73 141.398 2163.88 141.398 Q2159.73 141.398 2157.44 144.037 Q2155.15 146.652 2155.15 151.398 Q2155.15 156.143 2157.44 158.782 Q2159.73 161.398 2163.88 161.398 Q2165.73 161.398 2167.53 160.912 Q2169.36 160.402 2171.17 159.407 L2171.17 163.342 Q2169.39 164.176 2167.47 164.592 Q2165.57 165.009 2163.41 165.009 Q2157.56 165.009 2154.11 161.328 Q2150.66 157.648 2150.66 151.398 Q2150.66 145.055 2154.13 141.421 Q2157.63 137.787 2163.69 137.787 Q2165.66 137.787 2167.53 138.203 Q2169.41 138.597 2171.17 139.407 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2200.13 148.689 L2200.13 164.338 L2195.87 164.338 L2195.87 148.828 Q2195.87 145.148 2194.43 143.319 Q2193 141.49 2190.13 141.49 Q2186.68 141.49 2184.69 143.69 Q2182.7 145.889 2182.7 149.685 L2182.7 164.338 L2178.41 164.338 L2178.41 128.319 L2182.7 128.319 L2182.7 142.44 Q2184.22 140.102 2186.28 138.944 Q2188.37 137.787 2191.08 137.787 Q2195.54 137.787 2197.84 140.565 Q2200.13 143.319 2200.13 148.689 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2230.8 150.31 L2230.8 152.393 L2211.21 152.393 Q2211.49 156.791 2213.85 159.106 Q2216.24 161.398 2220.47 161.398 Q2222.93 161.398 2225.22 160.796 Q2227.53 160.194 2229.8 158.99 L2229.8 163.018 Q2227.51 163.99 2225.1 164.5 Q2222.7 165.009 2220.22 165.009 Q2214.02 165.009 2210.38 161.398 Q2206.77 157.787 2206.77 151.629 Q2206.77 145.264 2210.2 141.537 Q2213.65 137.787 2219.48 137.787 Q2224.71 137.787 2227.74 141.166 Q2230.8 144.523 2230.8 150.31 M2226.54 149.06 Q2226.49 145.565 2224.57 143.481 Q2222.67 141.398 2219.53 141.398 Q2215.96 141.398 2213.81 143.412 Q2211.68 145.426 2211.35 149.083 L2226.54 149.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2254.32 139.176 L2254.32 143.203 Q2252.51 142.277 2250.57 141.815 Q2248.62 141.352 2246.54 141.352 Q2243.37 141.352 2241.77 142.324 Q2240.2 143.296 2240.2 145.24 Q2240.2 146.722 2241.33 147.578 Q2242.46 148.412 2245.89 149.176 L2247.35 149.5 Q2251.89 150.472 2253.78 152.254 Q2255.71 154.014 2255.71 157.185 Q2255.71 160.796 2252.84 162.902 Q2249.99 165.009 2244.99 165.009 Q2242.9 165.009 2240.64 164.592 Q2238.39 164.199 2235.89 163.388 L2235.89 158.99 Q2238.25 160.217 2240.54 160.842 Q2242.84 161.444 2245.08 161.444 Q2248.09 161.444 2249.71 160.426 Q2251.33 159.384 2251.33 157.509 Q2251.33 155.773 2250.15 154.847 Q2248.99 153.921 2245.03 153.064 L2243.55 152.717 Q2239.59 151.884 2237.84 150.171 Q2236.08 148.435 2236.08 145.426 Q2236.08 141.768 2238.67 139.778 Q2241.26 137.787 2246.03 137.787 Q2248.39 137.787 2250.47 138.134 Q2252.56 138.481 2254.32 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip560)\" cx=\"1862.45\" cy=\"198.898\" r=\"20.48\" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"4.55111\"/>\n",
       "<path clip-path=\"url(#clip560)\" d=\"M1983.53 202.15 L1983.53 204.233 L1963.95 204.233 Q1964.23 208.631 1966.59 210.946 Q1968.97 213.238 1973.21 213.238 Q1975.66 213.238 1977.95 212.636 Q1980.27 212.034 1982.54 210.83 L1982.54 214.858 Q1980.24 215.83 1977.84 216.34 Q1975.43 216.849 1972.95 216.849 Q1966.75 216.849 1963.11 213.238 Q1959.5 209.627 1959.5 203.469 Q1959.5 197.104 1962.93 193.377 Q1966.38 189.627 1972.21 189.627 Q1977.44 189.627 1980.48 193.006 Q1983.53 196.363 1983.53 202.15 M1979.27 200.9 Q1979.23 197.405 1977.3 195.321 Q1975.41 193.238 1972.26 193.238 Q1968.69 193.238 1966.54 195.252 Q1964.41 197.266 1964.09 200.923 L1979.27 200.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M1994.64 212.289 L1994.64 226.039 L1990.36 226.039 L1990.36 190.252 L1994.64 190.252 L1994.64 194.187 Q1995.98 191.872 1998.02 190.761 Q2000.08 189.627 2002.93 189.627 Q2007.65 189.627 2010.59 193.377 Q2013.55 197.127 2013.55 203.238 Q2013.55 209.349 2010.59 213.099 Q2007.65 216.849 2002.93 216.849 Q2000.08 216.849 1998.02 215.738 Q1995.98 214.603 1994.64 212.289 M2009.13 203.238 Q2009.13 198.539 2007.19 195.877 Q2005.27 193.192 2001.89 193.192 Q1998.51 193.192 1996.56 195.877 Q1994.64 198.539 1994.64 203.238 Q1994.64 207.937 1996.56 210.622 Q1998.51 213.284 2001.89 213.284 Q2005.27 213.284 2007.19 210.622 Q2009.13 207.937 2009.13 203.238 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2030.66 193.238 Q2027.23 193.238 2025.24 195.923 Q2023.25 198.585 2023.25 203.238 Q2023.25 207.891 2025.22 210.576 Q2027.21 213.238 2030.66 213.238 Q2034.06 213.238 2036.05 210.553 Q2038.04 207.867 2038.04 203.238 Q2038.04 198.631 2036.05 195.946 Q2034.06 193.238 2030.66 193.238 M2030.66 189.627 Q2036.22 189.627 2039.39 193.238 Q2042.56 196.849 2042.56 203.238 Q2042.56 209.604 2039.39 213.238 Q2036.22 216.849 2030.66 216.849 Q2025.08 216.849 2021.91 213.238 Q2018.76 209.604 2018.76 203.238 Q2018.76 196.849 2021.91 193.238 Q2025.08 189.627 2030.66 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2068.28 191.247 L2068.28 195.229 Q2066.47 194.233 2064.64 193.747 Q2062.84 193.238 2060.98 193.238 Q2056.84 193.238 2054.55 195.877 Q2052.26 198.492 2052.26 203.238 Q2052.26 207.983 2054.55 210.622 Q2056.84 213.238 2060.98 213.238 Q2062.84 213.238 2064.64 212.752 Q2066.47 212.242 2068.28 211.247 L2068.28 215.182 Q2066.49 216.016 2064.57 216.432 Q2062.67 216.849 2060.52 216.849 Q2054.66 216.849 2051.22 213.168 Q2047.77 209.488 2047.77 203.238 Q2047.77 196.895 2051.24 193.261 Q2054.73 189.627 2060.8 189.627 Q2062.77 189.627 2064.64 190.043 Q2066.52 190.437 2068.28 191.247 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2097.23 200.529 L2097.23 216.178 L2092.97 216.178 L2092.97 200.668 Q2092.97 196.988 2091.54 195.159 Q2090.1 193.33 2087.23 193.33 Q2083.79 193.33 2081.79 195.53 Q2079.8 197.729 2079.8 201.525 L2079.8 216.178 L2075.52 216.178 L2075.52 180.159 L2079.8 180.159 L2079.8 194.28 Q2081.33 191.942 2083.39 190.784 Q2085.47 189.627 2088.18 189.627 Q2092.65 189.627 2094.94 192.405 Q2097.23 195.159 2097.23 200.529 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip560)\" d=\"M2122.26 191.016 L2122.26 195.043 Q2120.45 194.117 2118.51 193.655 Q2116.56 193.192 2114.48 193.192 Q2111.31 193.192 2109.71 194.164 Q2108.14 195.136 2108.14 197.08 Q2108.14 198.562 2109.27 199.418 Q2110.41 200.252 2113.83 201.016 L2115.29 201.34 Q2119.83 202.312 2121.72 204.094 Q2123.65 205.854 2123.65 209.025 Q2123.65 212.636 2120.78 214.742 Q2117.93 216.849 2112.93 216.849 Q2110.85 216.849 2108.58 216.432 Q2106.33 216.039 2103.83 215.228 L2103.83 210.83 Q2106.19 212.057 2108.48 212.682 Q2110.78 213.284 2113.02 213.284 Q2116.03 213.284 2117.65 212.266 Q2119.27 211.224 2119.27 209.349 Q2119.27 207.613 2118.09 206.687 Q2116.93 205.761 2112.97 204.904 L2111.49 204.557 Q2107.53 203.724 2105.78 202.011 Q2104.02 200.275 2104.02 197.266 Q2104.02 193.608 2106.61 191.618 Q2109.2 189.627 2113.97 189.627 Q2116.33 189.627 2118.41 189.974 Q2120.5 190.321 2122.26 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "using BenchmarkTools: @btime\n",
    "using ProfileSVG\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 0.0075\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=batch_size, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(prod((6,6,2)), 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model, \"cross_entropy_loss\")\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "test_data = mnist.test_images()\n",
    "test_labels = Int64.(mnist.test_labels())\n",
    "\n",
    "test_x =  reshape(test_data, :, 28, 28, 1) / 255.0\n",
    "test_y = test_labels\n",
    "\n",
    "n = 5000\n",
    "idx = randperm(n)\n",
    "\n",
    "test_x = test_x[idx, :,:,:]\n",
    "test_y = test_y[idx]\n",
    "\n",
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617527d7-dea6-41da-9ecd-f68b7687482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 147.52457\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 143.14738\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 130.03402\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 108.22491\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 92.05772\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 81.53964\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 74.02918\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 67.91751\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 62.92345\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 59.13204\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 2----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 23.84804\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 25.96631\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 24.2109\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 22.44795\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 21.14644\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 21.19724\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 20.95331\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 20.15431\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 19.4129\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 19.24319\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 3----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 20.80796\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 17.18696\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 16.91766\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 17.43083\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 16.97105\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 16.86665\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 16.37721\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 16.16714\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 16.07794\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 16.27895\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 4----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 19.74534\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 14.98824\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 13.83305\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 14.3076\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 14.49048\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 14.0232\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 14.20584\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 14.04307\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 13.48206\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 13.72761\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 5----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 4.02729\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 13.00317\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 13.11219\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 12.28982\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 12.59413\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 12.23376\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 12.70905\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 12.85076\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 12.75333\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 12.80681\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 6----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 13.9074\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 6.52107\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 10.35325\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 10.76944\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 10.32042\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 10.14798\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 10.52738\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 11.0494\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 11.31\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 11.36027\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 7----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 13.93132\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 10.82788\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 11.05804\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 10.59213\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 11.1829\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 10.84811\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 10.39011\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 10.17436\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 10.77139\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 10.94904\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 8----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 8.06207\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 9.71434\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 10.0326\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 8.96111\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 10.24528\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 11.1615\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 11.04382\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 10.57957\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 10.41807\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 10.4773\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 9----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 9.28385\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 8.37212\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 9.54223\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 9.2492\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 9.10241\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 9.44434\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 9.71991\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 9.44675\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 9.95292\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 10.09845\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 10----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 10.90326\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 10.01607\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 8.9067\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 8.46707\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 8.24124\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 9.27634\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 9.21268\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 9.16409\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 9.01562\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 99-element Vector{Float64} at index [100]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 99-element Vector{Float64} at index [100]",
      "",
      "Stacktrace:",
      " [1] setindex!",
      "   @ .\\array.jl:966 [inlined]",
      " [2] train!(graph::Vector{Any}, train_x::Array{Float64, 4}, train_y::Vector{Int64}, epochs::Int64, batch_size::Int64, learning_rate::Float64, optimizer::String)",
      "   @ Main .\\In[6]:66",
      " [3] top-level scope",
      "   @ In[18]:18"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.0025\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=32, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(72, 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model)\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522c34c-babf-4c8b-85b1-24a2268f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
