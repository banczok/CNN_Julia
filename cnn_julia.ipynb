{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9beed57c-7db7-4e59-9990-a470390d4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "#@code_warntype, @simd, @inbounds, statyczne typowenie (brak niestabilności typów, brak Any), @benchmark_tools, @btime, brak alokacji, \n",
    "#kolumny czy wiersze w for\n",
    "#test co szybsz, Matrix czy Array{Float64, 2}\n",
    "#softmax backward\n",
    "\n",
    "mutable struct Conv2DNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    W::Array{Float64, 4}\n",
    "    b::Vector{Float64}\n",
    "    batch_size::Int\n",
    "    kernel_size::Tuple{Int, Int}\n",
    "    filters::Int\n",
    "    output::Array{Float64, 4}\n",
    "    gradient_W::Array{Float64, 4}\n",
    "    gradient_b::Vector{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct MaxPoolNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    pool_size::Tuple{Int, Int}\n",
    "    output::Array{Float64, 4}\n",
    "end\n",
    "\n",
    "mutable struct DenseNode <: Node\n",
    "    x::Matrix{Float64}\n",
    "    W::Matrix{Float64} \n",
    "    b::Vector{Float64}\n",
    "    neurons::Int\n",
    "    output::Matrix{Float64}\n",
    "    gradient_W::Matrix{Float64}\n",
    "    gradient_b::Matrix{Float64}\n",
    "    activation::String\n",
    "    m_W::Array{Float64}\n",
    "    v_W::Array{Float64}\n",
    "    m_b::Array{Float64}\n",
    "    v_b::Array{Float64}\n",
    "end\n",
    "\n",
    "mutable struct FlattenNode <: Node\n",
    "    x::Array{Float64, 4}\n",
    "    output::Matrix{Float64}\n",
    "    input_shape::Tuple{Int, Int, Int}\n",
    "end\n",
    "\n",
    "mutable struct ReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct SoftmaxNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    loss_func::Union{Function, Nothing}\n",
    "end\n",
    "\n",
    "mutable struct SigmoidNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    loss_func::Union{Function, Nothing}\n",
    "end\n",
    "\n",
    "mutable struct TanhNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "end\n",
    "\n",
    "mutable struct LeakyReLUNode <: Node\n",
    "    x::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    output::Union{Array{Float64, 4}, Matrix{Float64}}\n",
    "    alpha::Float64\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8eb86b4-d598-4d00-ab75-f71b4ab219a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 9 methods)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward!(node::Conv2DNode, x::Array{Float64, 4}) \n",
    "    @views node.x = x\n",
    "    W = node.W\n",
    "    b = node.b\n",
    "\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width,  num_chanels, num_filters = size(W)\n",
    "\n",
    "    output_height = 1 + (input_height - filter_height)\n",
    "    output_width = 1 + (input_width - filter_width)\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, num_filters)\n",
    "    @simd for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:output_width, i in 1:output_height\n",
    "                @inbounds output[n, i, j, f] = sum(view(x, n, i:i+filter_height-1, j:j+filter_width-1, :) .* W[:,:,:,f]) + b[f]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    @views node.output = output\n",
    "end\n",
    "\n",
    "function forward!(node::MaxPoolNode, x::Array{Float64, 4})\n",
    "    @views node.x = x\n",
    "    pool_size = node.pool_size\n",
    "    \n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "\n",
    "    output_height = 1 + (input_height - pool_size[1]) ÷ pool_size[1]\n",
    "    output_width = 1 + (input_width - pool_size[2]) ÷ pool_size[2]\n",
    "\n",
    "    output = zeros(batch_size, output_height, output_width, input_channels)\n",
    "\n",
    "    @simd for n in 1:batch_size\n",
    "        for c in 1:input_channels\n",
    "            @inbounds for j in 1:pool_size[2]:input_width-pool_size[2]+1, i in 1:pool_size[1]:input_height-pool_size[1]+1\n",
    "                @inbounds output[n, 1+div(i-1, pool_size[1]), 1+div(j-1, pool_size[2]), c] = maximum(view(x, n, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @views node.output = output\n",
    "end\n",
    "\n",
    "function forward!(node::FlattenNode, x::Array{Float64, 4})\n",
    "    node.x = x\n",
    "    node.output = reshape(x, size(x, 1), size(x, 2) * size(x, 3) * size(x, 4))\n",
    "end\n",
    "\n",
    "function forward!(node::DenseNode, x::Matrix{Float64})\n",
    "    node.x = x\n",
    "    @views node.output = x * node.W .+ node.b'\n",
    "end\n",
    "\n",
    "function forward!(node::ReLUNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    @views node.output = max.(0, x)\n",
    "end\n",
    "\n",
    "function forward!(node::SoftmaxNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    exp_x = exp.(x .- maximum(x, dims=2))\n",
    "    node.output = exp_x ./ sum(exp_x, dims=2)\n",
    "end\n",
    "\n",
    "function forward!(node::SigmoidNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    node.output = 1.0 ./ (1.0 .+ exp.(-x))\n",
    "end\n",
    "\n",
    "function forward!(node::TanhNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    node.output = tanh.(x)\n",
    "end\n",
    "\n",
    "function forward!(node::LeakyReLUNode, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    node.x = x\n",
    "    @views node.output = ifelse.(x .> 0, x, node.alpha .* x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "602fd274-f243-48fd-874c-9567ce11b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 9 methods)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(node::Conv2DNode, dy::Array{Float64, 4}, x::Array{Float64, 4})\n",
    "    W = node.W\n",
    "    batch_size, input_height, input_width, input_channels = size(x)\n",
    "    filter_height, filter_width, num_channels, num_filters = size(W)\n",
    "\n",
    "    dx = zeros(size(x))\n",
    "    dW = zeros(size(W))\n",
    "    db = zeros(num_filters)\n",
    "\n",
    "    @simd for n in 1:batch_size\n",
    "        for f in 1:num_filters\n",
    "            for j in 1:input_width - filter_width + 1, i in 1:input_height - filter_height + 1\n",
    "                @inbounds dx[n, i:i + filter_height - 1, j:j + filter_width - 1, :] .+= W[:, :, :, f] .* dy[n, i, j, f]\n",
    "                @inbounds dW[:, :, :, f] .+= view(x, n, i:i + filter_height - 1, j:j + filter_width - 1, :) .* dy[n, i, j, f]\n",
    "            end\n",
    "            @inbounds db[f] += sum(dy[n, :, :, f])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::MaxPoolNode, dy::Array{Float64, 4}, x::Array{Float64, 4})\n",
    "    pool_size = node.pool_size\n",
    "    new_size = size(node.output)\n",
    "    dy = reshape(dy, new_size)\n",
    "    \n",
    "    batch_size, height, width, channels = size(x)\n",
    "    dx = zeros(size(x))\n",
    "    @simd for b in 1:batch_size\n",
    "        for c in 1:channels\n",
    "            @inbounds for j in 1:pool_size[2]:width-pool_size[2]+1, i in 1:pool_size[1]:height-pool_size[1]+1\n",
    "                @inbounds window = view(x, b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c)\n",
    "                @inbounds dx[b, i:i+pool_size[1]-1, j:j+pool_size[2]-1, c] .+= dy[b, i ÷ pool_size[1]+1, j ÷ pool_size[2]+1, c] .* (window .== maximum(window))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::DenseNode, dy::Matrix{Float64}, x::Matrix{Float64})\n",
    "    dx, dW, db = dy * node.W', x' * dy, reshape(sum(dy, dims=1), :, 1)\n",
    "    return (dx, dW, db)\n",
    "end\n",
    "\n",
    "function backward!(node::FlattenNode, dy::Matrix{Float64}, x::Array{Float64, 4})\n",
    "    dx = reshape(dy, size(x, 1), node.input_shape...)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(node::ReLUNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* (x .> 0)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::SigmoidNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* (node.output .* (1.0 .- node.output))\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::TanhNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* (1.0 .- node.output .^ 2)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::LeakyReLUNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    dx = dy .* ifelse.(x .> 0, 1, node.alpha)\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function backward!(node::SoftmaxNode, dy::Union{Array{Float64, 4}, Matrix{Float64}}, x::Union{Array{Float64, 4}, Matrix{Float64}})\n",
    "    softmax_output = node.output\n",
    "    batch_size, num_classes = size(softmax_output)\n",
    "\n",
    "    dx = similar(dy)\n",
    "\n",
    "    @simd for i in 1:batch_size\n",
    "        vec = view(softmax_output, i, :)\n",
    "        J = diagm(vec) .- vec * vec'\n",
    "        view(dx, i, :) .= J' * view(dy, i, :)\n",
    "    end\n",
    "\n",
    "    return dx\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d202a415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hasproperty (generic function with 4 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Conv2DNode(; batch_size::Int, filters::Int, kernel_size::Tuple{Int, Int}, input_shape::Tuple{Int, Int, Int}, activation::String)\n",
    "    W = randn(kernel_size[1], kernel_size[2], input_shape[3], filters) * sqrt(1 / (kernel_size[1] * kernel_size[2] * input_shape[3]))\n",
    "    b = zeros(filters)\n",
    "\n",
    "    return Conv2DNode(zeros(1,1,1,1), W, b, batch_size, kernel_size, filters, zeros(1,1,1,1), zeros(1, 1, 1, 1), zeros(1), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function MaxPoolNode(; pool_size::Tuple{Int, Int})\n",
    "    return MaxPoolNode(zeros((1, 1, 1, 1)), pool_size, zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function DenseNode(; neurons::Int, activation::String, input_shape::Union{Tuple{Int, Int}, Tuple{Int, Int, Int}})\n",
    "    W = randn(prod(input_shape), neurons) * sqrt(1 / (prod(input_shape)))\n",
    "    b = zeros(neurons)\n",
    "\n",
    "    return DenseNode(zeros(1,1), W, b, neurons, zeros((1,1)), zeros((1, 1)), zeros((1, 1)), activation, zeros(size(W)), zeros(size(W)), zeros(size(b)), zeros(size(b)))\n",
    "end\n",
    "\n",
    "function FlattenNode(; input_shape::Tuple{Int, Int, Int})\n",
    "    return FlattenNode(zeros((input_shape..., 1)), zeros((1, 1)), input_shape)\n",
    "end\n",
    "\n",
    "function ReLUNode()\n",
    "    return ReLUNode(zeros((1, 1, 1, 1)), zeros((1, 1, 1, 1)))\n",
    "end\n",
    "\n",
    "function SoftmaxNode()\n",
    "    return SoftmaxNode(zeros((1, 1)), zeros((1, 1)), nothing)\n",
    "end\n",
    "\n",
    "function TanhNode()\n",
    "    return TanhNode(zeros((1, 1)), zeros((1, 1)))\n",
    "end\n",
    "\n",
    "function SigmoidNode()\n",
    "    return SigmoidNode(zeros((1, 1)), zeros((1, 1)), nothing)\n",
    "end\n",
    "\n",
    "function LeakyReLUNode(alpha::Float64)\n",
    "    return LeakyReLUNode(zeros((1, 1)), zeros((1, 1)), alpha)\n",
    "end\n",
    "\n",
    "function forward_pass!(graph::Vector{Any}, x::Array{Float64, 4})\n",
    "    input = x\n",
    "    for node in graph\n",
    "        @inbounds forward!(node, input)\n",
    "        input = @views node.output\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function backward_pass!(graph::Vector{Any}, y_true::Vector{Int64})\n",
    "    preds = graph[end].output\n",
    "    dout  = graph[end].loss_func(preds, y_true) #loss func backpropagation\n",
    "\n",
    "    for i in reverse(1:length(graph))\n",
    "        layer = graph[i]\n",
    "        prev_layer_output = i > 1 ? graph[i - 1].output : layer.x\n",
    "        if hasproperty(layer, :gradient_W)\n",
    "            @inbounds dout,dW,db = backward!(layer, dout, prev_layer_output)\n",
    "            layer.gradient_W = dW\n",
    "            layer.gradient_b = db\n",
    "\n",
    "        else \n",
    "            @inbounds dout = backward!(layer, dout, prev_layer_output)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function onehotbatch(labels::Vector{Int64}, classes::UnitRange{Int64})\n",
    "    Y = zeros(Int, length(labels), length(classes))\n",
    "    for (i, label) in enumerate(labels)\n",
    "        Y[i, label .== classes] .= 1\n",
    "    end\n",
    "    return Y\n",
    "end\n",
    "\n",
    "function graph_build(layers::Vector{Node}, loss::String)\n",
    "    output = []\n",
    "    for layer in layers\n",
    "        push!(output, layer)\n",
    "        if hasproperty(layer, :activation)\n",
    "            push!(output, getActivation(layer.activation))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if loss == \"cross_entropy_loss\"\n",
    "        output[end].loss_func = cross_entropy_backward\n",
    "    else\n",
    "        error(\"Invalid activation function for output layer\")\n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "function getActivation(name::String)\n",
    "    if name == \"relu\"\n",
    "        return ReLUNode()\n",
    "    elseif name == \"softmax\"\n",
    "        return SoftmaxNode()\n",
    "    elseif name == \"sigmoid\"\n",
    "        return SigmoidNode()\n",
    "    elseif name == \"tanh\"\n",
    "        return TanhNode()\n",
    "    elseif name == \"leakyrelu\"\n",
    "        return LeakyReLUNode(0.01)\n",
    "    else \n",
    "        error(\"no such activation function\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function cross_entropy_backward(y_pred::Matrix{Float64}, y_true::Vector{Int64})\n",
    "    return -(onehotbatch(y_true, 0:9) ./ y_pred) ./ size(y_true, 1)\n",
    "end\n",
    "\n",
    "import Base.hasproperty\n",
    "hasproperty(x, s::Symbol) = s in fieldnames(typeof(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433dfab2-7d04-4fef-a952-6546082e4fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_adam!(layer::Node, learning_rate::Float64, t::Int, beta1::Float64 = 0.9, beta2::Float64 = 0.999, epsilon::Float64 = 1e-8)\n",
    "    layer.m_W = beta1 * layer.m_W + (1 - beta1) * layer.gradient_W\n",
    "    layer.v_W = beta2 * layer.v_W + (1 - beta2) * layer.gradient_W .^ 2\n",
    "    mhat_W = layer.m_W / (1 - beta1 ^ t)\n",
    "    vhat_W = layer.v_W / (1 - beta2 ^ t)\n",
    "    layer.W .-= learning_rate .* mhat_W ./ (sqrt.(vhat_W) .+ epsilon)\n",
    "\n",
    "\n",
    "    layer.m_b = beta1 * layer.m_b + (1 - beta1) * layer.gradient_b\n",
    "    layer.v_b = beta2 * layer.v_b + (1 - beta2) * layer.gradient_b .^ 2\n",
    "    mhat_b = layer.m_b / (1 - beta1 ^ t)\n",
    "    vhat_b = layer.v_b / (1 - beta2 ^ t)\n",
    "    layer.b .-= learning_rate .* mhat_b ./ (sqrt.(vhat_b) .+ epsilon)\n",
    "end\n",
    "\n",
    "function update_sgd!(layer::Node, learning_rate::Float64)\n",
    "    layer.W .-= learning_rate .* layer.gradient_W\n",
    "    layer.b .-= learning_rate .* layer.gradient_b\n",
    "end\n",
    "\n",
    "function train!(graph::Vector{Any}, train_x::Array{Float64, 4}, train_y::Vector{Int64}, epochs::Int=5, batch_size::Int=32, learning_rate::Float64=1e-3, optimizer::String=\"adam\")\n",
    "    total_batches = ceil(Int, size(train_x, 1) / batch_size)\n",
    "    each = ceil(Int, total_batches/10)\n",
    " \n",
    "    loss_history = zeros(ceil(Int, total_batches*epochs/each))\n",
    "    counter = 1\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        println(\"----EPOCH $epoch----\")\n",
    "\n",
    "        idx = randperm(size(train_x, 1))\n",
    "        train_x = train_x[idx, :, :, :]\n",
    "        train_y = train_y[idx]\n",
    "        loss = 0\n",
    "        current_epoch_loss = zeros(total_batches)\n",
    "        \n",
    "\n",
    "        for i in 1:batch_size:size(train_x, 1)\n",
    "            x_batch = train_x[i:min(i+batch_size-1, end), :, :, :]\n",
    "            y_batch = train_y[i:min(i+batch_size-1, end)]\n",
    "            tmp  = (i ÷ batch_size)\n",
    "\n",
    "            forward_pass!(graph, x_batch)\n",
    "            \n",
    "            current_epoch_loss[tmp+1] = -sum(log.(graph[end].output) .* onehotbatch(y_batch, 0:9))#/size(y_batch, 1)\n",
    "            \n",
    "            backward_pass!(graph, y_batch)\n",
    "\n",
    "            for layer in graph\n",
    "                if hasproperty(layer, :gradient_W)\n",
    "                    if optimizer == \"adam\"\n",
    "                        update_adam!(layer, learning_rate, i)\n",
    "                    elseif optimizer == \"sgd\"\n",
    "                        update_sgd!(layer, learning_rate)\n",
    "                    else\n",
    "                        error(\"Invalid optimizer selected.\")\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            \n",
    "            if i == 1  ||  tmp % each == 0\n",
    "                avg_loss = sum(current_epoch_loss)/(tmp+1)\n",
    "\n",
    "                loss_history[counter] = avg_loss\n",
    "                counter += 1\n",
    "                print(i ÷ batch_size, \"/\", total_batches)\n",
    "                println(\" avg epoch loss: \", round(avg_loss, digits=5))\n",
    "                \n",
    "            end\n",
    "        end\n",
    "        println()\n",
    "        \n",
    "    end\n",
    "    println(\"Training finished!\")\n",
    "\n",
    "    return loss_history\n",
    "end\n",
    "\n",
    "function evaluate(graph::Vector{Any}, x_data::Array{Float64, 4}, y_data::Vector{Int64}, batch_size::Int, pool_size::Tuple{Int, Int})\n",
    "    num_correct = 0\n",
    "    num_samples = size(x_data, 1)\n",
    "\n",
    "    for i in 1:batch_size:num_samples\n",
    "        x_batch = x_data[i:min(i+batch_size-1, end), :, :, :]\n",
    "        y_batch = y_data[i:min(i+batch_size-1, end)]\n",
    "        \n",
    "        forward_pass!(graph, x_batch)\n",
    "        pred = graph[end].output\n",
    "        \n",
    "        predictions = argmax.(eachrow(pred)) .- 1\n",
    "        num_correct += sum(predictions .== y_batch)\n",
    "    end\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "    return accuracy\n",
    "end\n",
    "\n",
    "function plot_loss(loss::Vector{Float64}, epochs::Int)\n",
    "    plot(1:size(loss,1), loss, label=\"loss_batches\")\n",
    "\n",
    "    each = Int64(size(loss,1)/epochs)\n",
    "    pts = loss[each:each:end]\n",
    "    plot!(each:each:each*size(pts,1), pts, seriestype=:scatter, label=\"epochs\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d5e96f3-f2cb-47d1-a14d-8c9d84c3bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "mnist = pyimport(\"mnist\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_data = mnist.train_images()\n",
    "train_labels = Int64.(mnist.train_labels())\n",
    "train_x =  reshape(train_data, :, 28, 28, 1) / 255.0\n",
    "train_y = train_labels\n",
    "\n",
    "\n",
    "n = 5000\n",
    "idx = randperm(n)\n",
    "train_x = train_x[idx, :,:,:]\n",
    "train_y = train_y[idx]\n",
    "\n",
    "train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b287c46-20e9-435c-95e2-23d699c4eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "0/79 avg epoch loss: 151.36405\n",
      "8/79 avg epoch loss: 131.90864\n",
      "16/79 avg epoch loss: 106.08491\n",
      "24/79 avg epoch loss: 91.16142\n",
      "32/79 avg epoch loss: 79.43459\n",
      "40/79 avg epoch loss: 71.41735\n",
      "48/79 avg epoch loss: 65.43096\n",
      "56/79 avg epoch loss: 61.03241\n",
      "64/79 avg epoch loss: 57.7152\n",
      "72/79 avg epoch loss: 53.96387\n",
      "\n",
      "----EPOCH 2----\n",
      "0/79 avg epoch loss: 17.36296\n",
      "8/79 avg epoch loss: 28.34626\n",
      "16/79 avg epoch loss: 23.19522\n",
      "24/79 avg epoch loss: 21.66317\n",
      "32/79 avg epoch loss: 21.88746\n",
      "40/79 avg epoch loss: 22.36063\n",
      "48/79 avg epoch loss: 22.34696\n",
      "56/79 avg epoch loss: 22.38005\n",
      "64/79 avg epoch loss: 21.95824\n",
      "72/79 avg epoch loss: 21.7001\n",
      "\n",
      "----EPOCH 3----\n",
      "0/79 avg epoch loss: 45.09632\n",
      "8/79 avg epoch loss: 21.65811\n",
      "16/79 avg epoch loss: 18.71987\n",
      "24/79 avg epoch loss: 18.71891\n",
      "32/79 avg epoch loss: 19.00524\n",
      "40/79 avg epoch loss: 20.13165\n",
      "48/79 avg epoch loss: 20.32602\n",
      "56/79 avg epoch loss: 20.04909\n",
      "64/79 avg epoch loss: 19.97503\n",
      "72/79 avg epoch loss: 19.40072\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/79 avg epoch loss: 10.02351\n",
      "8/79 avg epoch loss: 16.81128\n",
      "16/79 avg epoch loss: 15.55733\n",
      "24/79 avg epoch loss: 15.37217\n",
      "32/79 avg epoch loss: 14.50596\n",
      "40/79 avg epoch loss: 15.24397\n",
      "48/79 avg epoch loss: 15.92383\n",
      "56/79 avg epoch loss: 15.13363\n",
      "64/79 avg epoch loss: 14.80831\n",
      "72/79 avg epoch loss: 14.81051\n",
      "\n",
      "----EPOCH 2----\n",
      "0/79 avg epoch loss: 17.23927\n",
      "8/79 avg epoch loss: 12.9615\n",
      "16/79 avg epoch loss: 12.94743\n",
      "24/79 avg epoch loss: 13.74343\n",
      "32/79 avg epoch loss: 13.63039\n",
      "40/79 avg epoch loss: 13.46598\n",
      "48/79 avg epoch loss: 13.78565\n",
      "56/79 avg epoch loss: 14.63504\n",
      "64/79 avg epoch loss: 14.82736\n",
      "72/79 avg epoch loss: 14.59973\n",
      "\n",
      "----EPOCH 3----\n",
      "0/79 avg epoch loss: 11.56484\n",
      "8/79 avg epoch loss: 10.77983\n",
      "16/79 avg epoch loss: 11.94988\n",
      "24/79 avg epoch loss: 11.39883\n",
      "32/79 avg epoch loss: 11.73316\n",
      "40/79 avg epoch loss: 11.8979\n",
      "48/79 avg epoch loss: 12.07246\n",
      "56/79 avg epoch loss: 12.97675\n",
      "64/79 avg epoch loss: 13.23014\n",
      "72/79 avg epoch loss: 13.56993\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/79 avg epoch loss: 4.2001\n",
      "8/79 avg epoch loss: 10.50654\n",
      "16/79 avg epoch loss: 10.86823\n",
      "24/79 avg epoch loss: 11.23018\n",
      "32/79 avg epoch loss: 11.30211\n",
      "40/79 avg epoch loss: 12.15863\n",
      "48/79 avg epoch loss: 12.59704\n",
      "56/79 avg epoch loss: 12.35625\n",
      "64/79 avg epoch loss: 12.36917\n",
      "72/79 avg epoch loss: 12.82348\n",
      "\n",
      "----EPOCH 2----\n",
      "0/79 avg epoch loss: 20.88019\n",
      "8/79 avg epoch loss: 14.66847\n",
      "16/79 avg epoch loss: 13.64795\n",
      "24/79 avg epoch loss: 12.02729\n",
      "32/79 avg epoch loss: 11.52245\n",
      "40/79 avg epoch loss: 12.08671\n",
      "48/79 avg epoch loss: 11.47199\n",
      "56/79 avg epoch loss: 12.01714\n",
      "64/79 avg epoch loss: 11.78064\n",
      "72/79 avg epoch loss: 12.10492\n",
      "\n",
      "----EPOCH 3----\n",
      "0/79 avg epoch loss: 17.21374\n",
      "8/79 avg epoch loss: 13.79555\n",
      "16/79 avg epoch loss: 12.32695\n",
      "24/79 avg epoch loss: 10.56596\n",
      "32/79 avg epoch loss: 10.96175\n",
      "40/79 avg epoch loss: 10.5295\n",
      "48/79 avg epoch loss: 10.41424\n",
      "56/79 avg epoch loss: 10.89977\n",
      "64/79 avg epoch loss: 11.76383\n",
      "72/79 avg epoch loss: 11.55598\n",
      "\n",
      "Training finished!\n",
      "----EPOCH 1----\n",
      "0/79 avg epoch loss: 21.31993\n",
      "8/79 avg epoch loss: 9.8905\n",
      "16/79 avg epoch loss: 10.08346\n",
      "24/79 avg epoch loss: 9.98368\n",
      "32/79 avg epoch loss: 10.14309\n",
      "40/79 avg epoch loss: 9.89165\n",
      "48/79 avg epoch loss: 10.46364\n",
      "56/79 avg epoch loss: 10.78379\n",
      "64/79 avg epoch loss: 11.14014\n",
      "72/79 avg epoch loss: 11.18333\n",
      "\n",
      "----EPOCH 2----\n",
      "0/79 avg epoch loss: 17.61111\n",
      "8/79 avg epoch loss: 11.39315\n",
      "16/79 avg epoch loss: 10.99746\n",
      "24/79 avg epoch loss: 10.87382\n",
      "32/79 avg epoch loss: 10.52025\n",
      "40/79 avg epoch loss: 10.21729\n",
      "48/79 avg epoch loss: 10.26218\n",
      "56/79 avg epoch loss: 10.49739\n",
      "64/79 avg epoch loss: 10.48266\n",
      "72/79 avg epoch loss: 10.67157\n",
      "\n",
      "----EPOCH 3----\n",
      "0/79 avg epoch loss: 4.72542\n",
      "8/79 avg epoch loss: 8.86924\n",
      "16/79 avg epoch loss: 8.25249\n",
      "24/79 avg epoch loss: 7.28428\n",
      "32/79 avg epoch loss: 8.42136\n",
      "40/79 avg epoch loss: 8.67769\n",
      "48/79 avg epoch loss: 9.27737\n",
      "56/79 avg epoch loss: 9.13715\n",
      "64/79 avg epoch loss: 9.73116\n",
      "72/79 avg epoch loss: 9.61342\n",
      "\n",
      "Training finished!\n",
      "  16.195 s (102746574 allocations: 13.45 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30-element Vector{Float64}:\n",
       " 21.31992953002347\n",
       "  9.890501883854839\n",
       " 10.083463353794983\n",
       "  9.983679361374932\n",
       " 10.143085696044437\n",
       "  9.891646789211341\n",
       " 10.46363802523003\n",
       " 10.783792957287769\n",
       " 11.14013649318302\n",
       " 11.18333259798597\n",
       " 17.611114210671257\n",
       " 11.393154808924015\n",
       " 10.997455263987577\n",
       "  ⋮\n",
       " 10.482656755304655\n",
       " 10.67157209265703\n",
       "  4.72541760758218\n",
       "  8.869242601712646\n",
       "  8.252489244303534\n",
       "  7.284283217554573\n",
       "  8.421355048748865\n",
       "  8.677693030802601\n",
       "  9.277365716319999\n",
       "  9.137148359935564\n",
       "  9.73116279678036\n",
       "  9.613417033393727"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "using BenchmarkTools: @btime\n",
    "using ProfileSVG\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 0.0075\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=batch_size, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(prod((6,6,2)), 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model, \"cross_entropy_loss\")\n",
    "\n",
    "@btime loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "#plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "test_data = mnist.test_images()\n",
    "test_labels = Int64.(mnist.test_labels())\n",
    "\n",
    "test_x =  reshape(test_data, :, 28, 28, 1) / 255.0\n",
    "test_y = test_labels\n",
    "\n",
    "n = 5000\n",
    "idx = randperm(n)\n",
    "\n",
    "test_x = test_x[idx, :,:,:]\n",
    "test_y = test_y[idx]\n",
    "\n",
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617527d7-dea6-41da-9ecd-f68b7687482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----EPOCH 1----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 147.52457\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 143.14738\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 130.03402\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 108.22491\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 92.05772\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 81.53964\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 74.02918\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 67.91751\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 62.92345\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 59.13204\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 2----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 23.84804\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 25.96631\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 24.2109\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 22.44795\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 21.14644\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 21.19724\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 20.95331\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 20.15431\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 19.4129\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 19.24319\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 3----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 20.80796\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 17.18696\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 16.91766\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 17.43083\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 16.97105\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 16.86665\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 16.37721\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 16.16714\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 16.07794\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 16.27895\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 4----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 19.74534\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 14.98824\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 13.83305\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 14.3076\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 14.49048\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 14.0232\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 14.20584\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 14.04307\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 13.48206\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 13.72761\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 5----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 4.02729\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 13.00317\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 13.11219\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 12.28982\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 12.59413\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 12.23376\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 12.70905\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 12.85076\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 12.75333\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 12.80681\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 6----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 13.9074\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 6.52107\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 10.35325\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 10.76944\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 10.32042\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 10.14798\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 10.52738\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 11.0494\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 11.31\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 11.36027\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 7----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 13.93132\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 10.82788\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 11.05804\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 10.59213\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 11.1829\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 10.84811\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 10.39011\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 10.17436\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 10.77139\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 10.94904\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 8----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 8.06207\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 9.71434\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 10.0326\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 8.96111\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 10.24528\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 11.1615\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 11.04382\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 10.57957\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 10.41807\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 10.4773\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 9----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 9.28385\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 8.37212\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 9.54223\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 9.2492\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 9.10241\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 9.44434\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 9.71991\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 9.44675\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 9.95292\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "72/79 avg epoch loss: 10.09845\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(8, 26, 26, 2)\n",
      "\n",
      "----EPOCH 10----\n",
      "(64, 26, 26, 2)\n",
      "0/79 avg epoch loss: 10.90326\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "8/79 avg epoch loss: 10.01607\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "16/79 avg epoch loss: 8.9067\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "24/79 avg epoch loss: 8.46707\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "32/79 avg epoch loss: 8.24124\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "40/79 avg epoch loss: 9.27634\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "48/79 avg epoch loss: 9.21268\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "56/79 avg epoch loss: 9.16409\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "64/79 avg epoch loss: 9.01562\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n",
      "(64, 26, 26, 2)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 99-element Vector{Float64} at index [100]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 99-element Vector{Float64} at index [100]",
      "",
      "Stacktrace:",
      " [1] setindex!",
      "   @ .\\array.jl:966 [inlined]",
      " [2] train!(graph::Vector{Any}, train_x::Array{Float64, 4}, train_y::Vector{Int64}, epochs::Int64, batch_size::Int64, learning_rate::Float64, optimizer::String)",
      "   @ Main .\\In[6]:66",
      " [3] top-level scope",
      "   @ In[18]:18"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.0025\n",
    "pool_size = (4,4)\n",
    "\n",
    "model = [\n",
    "    Conv2DNode(batch_size=32, filters=2, kernel_size=(3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    MaxPoolNode(pool_size=(4,4)),\n",
    "    FlattenNode(input_shape=(6,6,2)),\n",
    "    DenseNode(neurons=32, activation=\"relu\", input_shape=(72, 1)),\n",
    "    DenseNode(neurons=10, activation=\"softmax\", input_shape=(32,1))\n",
    "]\n",
    "\n",
    "graph = graph_build(model)\n",
    "\n",
    "loss = train!(graph, train_x, train_y, epochs, batch_size, learning_rate, \"adam\")\n",
    "plot_loss(loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522c34c-babf-4c8b-85b1-24a2268f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(graph, test_x, test_y, batch_size, pool_size)\n",
    "println(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
